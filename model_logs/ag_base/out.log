2022-03-19 18:52:15,445 - INFO - allennlp.common.params - random_seed = 4040931187
2022-03-19 18:52:15,453 - INFO - allennlp.common.params - numpy_seed = 4040931187
2022-03-19 18:52:15,455 - INFO - allennlp.common.params - pytorch_seed = 4040931187
2022-03-19 18:52:15,459 - INFO - allennlp.common.checks - Pytorch version: 1.8.0+cu111
2022-03-19 18:52:15,461 - INFO - allennlp.common.params - type = default
2022-03-19 18:52:15,464 - INFO - allennlp.common.params - dataset_reader.type = text_classification_json_with_sampling
2022-03-19 18:52:15,466 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer
2022-03-19 18:52:15,468 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2022-03-19 18:52:15,470 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = roberta-base
2022-03-19 18:52:15,472 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags
2022-03-19 18:52:15,474 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512
2022-03-19 18:52:15,476 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None
2022-03-19 18:52:27,663 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer
2022-03-19 18:52:27,670 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = roberta-base
2022-03-19 18:52:27,672 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True
2022-03-19 18:52:27,674 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = 512
2022-03-19 18:52:27,676 - INFO - allennlp.common.params - dataset_reader.tokenizer.stride = 0
2022-03-19 18:52:27,678 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None
2022-03-19 18:52:27,680 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = 512
2022-03-19 18:52:27,682 - INFO - allennlp.common.params - dataset_reader.sample = None
2022-03-19 18:52:27,684 - INFO - allennlp.common.params - dataset_reader.skip_label_indexing = False
2022-03-19 18:52:27,686 - INFO - allennlp.common.params - train_data_path = datasets/ag/train.jsonl
2022-03-19 18:52:27,689 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7fae8765d210>
2022-03-19 18:52:27,691 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-03-19 18:52:27,693 - INFO - allennlp.common.params - validation_dataset_reader.type = text_classification_json_with_sampling
2022-03-19 18:52:27,696 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer
2022-03-19 18:52:27,698 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2022-03-19 18:52:27,700 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = roberta-base
2022-03-19 18:52:27,702 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags
2022-03-19 18:52:27,703 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512
2022-03-19 18:52:27,705 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None
2022-03-19 18:52:27,708 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.type = pretrained_transformer
2022-03-19 18:52:27,711 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.model_name = roberta-base
2022-03-19 18:52:27,713 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.add_special_tokens = True
2022-03-19 18:52:27,715 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.max_length = 512
2022-03-19 18:52:27,716 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.stride = 0
2022-03-19 18:52:27,718 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.tokenizer_kwargs = None
2022-03-19 18:52:27,720 - INFO - allennlp.common.params - validation_dataset_reader.max_sequence_length = 512
2022-03-19 18:52:27,723 - INFO - allennlp.common.params - validation_dataset_reader.sample = None
2022-03-19 18:52:27,725 - INFO - allennlp.common.params - validation_dataset_reader.skip_label_indexing = False
2022-03-19 18:52:27,727 - INFO - allennlp.common.params - validation_data_path = datasets/ag/dev.jsonl
2022-03-19 18:52:27,729 - INFO - allennlp.common.params - validation_data_loader = None
2022-03-19 18:52:27,731 - INFO - allennlp.common.params - test_data_path = datasets/ag/test.jsonl
2022-03-19 18:52:27,732 - INFO - allennlp.common.params - evaluate_on_test = True
2022-03-19 18:52:27,734 - INFO - allennlp.common.params - batch_weight_key = 
2022-03-19 18:52:27,736 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-19 18:52:27,738 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-19 18:52:27,740 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-19 18:52:27,742 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-19 18:52:27,743 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-19 18:52:27,745 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-19 18:52:27,747 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-19 18:52:27,749 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-19 18:52:27,750 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-19 18:52:27,752 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-19 18:52:27,754 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-19 18:52:27,755 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-19 18:52:27,757 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-19 18:52:27,759 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-19 18:52:27,761 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-19 18:52:37,808 - INFO - tqdm - loading instances: 31173it [00:10, 3614.56it/s]
2022-03-19 18:52:47,812 - INFO - tqdm - loading instances: 62256it [00:20, 3727.61it/s]
2022-03-19 18:52:57,858 - INFO - tqdm - loading instances: 92699it [00:30, 3168.58it/s]
2022-03-19 18:53:05,132 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-19 18:53:05,139 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-19 18:53:05,141 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-19 18:53:05,143 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-19 18:53:05,145 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-19 18:53:05,147 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-19 18:53:05,150 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-19 18:53:05,152 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-19 18:53:05,154 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-19 18:53:05,155 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-19 18:53:05,157 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-19 18:53:05,159 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-19 18:53:05,161 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-19 18:53:05,163 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-19 18:53:05,165 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-19 18:53:06,521 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-19 18:53:06,528 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-19 18:53:06,530 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-19 18:53:06,532 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-19 18:53:06,535 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-19 18:53:06,537 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-19 18:53:06,539 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-19 18:53:06,541 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-19 18:53:06,542 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-19 18:53:06,544 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-19 18:53:06,546 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-19 18:53:06,548 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-19 18:53:06,550 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-19 18:53:06,552 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-19 18:53:06,554 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-19 18:53:08,602 - INFO - allennlp.common.params - type = from_instances
2022-03-19 18:53:08,609 - INFO - allennlp.common.params - min_count = None
2022-03-19 18:53:08,611 - INFO - allennlp.common.params - max_vocab_size = None
2022-03-19 18:53:08,613 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-03-19 18:53:08,616 - INFO - allennlp.common.params - pretrained_files = None
2022-03-19 18:53:08,618 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-03-19 18:53:08,622 - INFO - allennlp.common.params - tokens_to_add = None
2022-03-19 18:53:08,624 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-03-19 18:53:08,626 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-03-19 18:53:08,627 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-03-19 18:53:08,629 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-03-19 18:53:08,631 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-03-19 18:53:09,343 - INFO - allennlp.common.params - model.type = basic_classifier_with_f1
2022-03-19 18:53:09,350 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2022-03-19 18:53:09,352 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer
2022-03-19 18:53:09,354 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = roberta-base
2022-03-19 18:53:09,356 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512
2022-03-19 18:53:09,357 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_module = None
2022-03-19 18:53:09,359 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True
2022-03-19 18:53:09,361 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True
2022-03-19 18:53:09,362 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None
2022-03-19 18:53:09,364 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None
2022-03-19 18:53:09,366 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None
2022-03-19 18:53:09,367 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None
2022-03-19 18:53:09,369 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None
2022-03-19 18:53:15,028 - INFO - allennlp.common.params - model.seq2vec_encoder.type = cls_pooler
2022-03-19 18:53:15,035 - INFO - allennlp.common.params - model.seq2vec_encoder.embedding_dim = 768
2022-03-19 18:53:15,037 - INFO - allennlp.common.params - model.seq2vec_encoder.cls_is_last_token = False
2022-03-19 18:53:15,040 - INFO - allennlp.common.params - model.feedforward_layer.input_dim = 768
2022-03-19 18:53:15,042 - INFO - allennlp.common.params - model.feedforward_layer.num_layers = 1
2022-03-19 18:53:15,043 - INFO - allennlp.common.params - model.feedforward_layer.hidden_dims = 768
2022-03-19 18:53:15,045 - INFO - allennlp.common.params - model.feedforward_layer.activations = tanh
2022-03-19 18:53:15,047 - INFO - allennlp.common.params - type = tanh
2022-03-19 18:53:15,049 - INFO - allennlp.common.params - model.feedforward_layer.dropout = 0.0
2022-03-19 18:53:15,056 - INFO - allennlp.common.params - model.seq2seq_encoder = None
2022-03-19 18:53:15,058 - INFO - allennlp.common.params - model.dropout = 0.1
2022-03-19 18:53:15,060 - INFO - allennlp.common.params - model.num_labels = None
2022-03-19 18:53:15,061 - INFO - allennlp.common.params - model.label_namespace = labels
2022-03-19 18:53:15,063 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7faff9145650>
2022-03-19 18:53:15,065 - INFO - allennlp.common.params - model.regularizer = None
2022-03-19 18:53:15,067 - INFO - allennlp.common.params - model.track_weights = False
2022-03-19 18:53:15,069 - INFO - allennlp.common.params - model.disable_layers = []
2022-03-19 18:53:15,071 - INFO - allennlp.nn.initializers - Initializing parameters
2022-03-19 18:53:15,074 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-03-19 18:53:15,076 - INFO - allennlp.nn.initializers -    _classification_layer.bias
2022-03-19 18:53:15,078 - INFO - allennlp.nn.initializers -    _classification_layer.weight
2022-03-19 18:53:15,080 - INFO - allennlp.nn.initializers -    _feedforward_layer._linear_layers.0.bias
2022-03-19 18:53:15,082 - INFO - allennlp.nn.initializers -    _feedforward_layer._linear_layers.0.weight
2022-03-19 18:53:15,083 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias
2022-03-19 18:53:15,085 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight
2022-03-19 18:53:15,086 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight
2022-03-19 18:53:15,088 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight
2022-03-19 18:53:15,089 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight
2022-03-19 18:53:15,091 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2022-03-19 18:53:15,093 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2022-03-19 18:53:15,094 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias
2022-03-19 18:53:15,096 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight
2022-03-19 18:53:15,098 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias
2022-03-19 18:53:15,099 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight
2022-03-19 18:53:15,101 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias
2022-03-19 18:53:15,102 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight
2022-03-19 18:53:15,104 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias
2022-03-19 18:53:15,106 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight
2022-03-19 18:53:15,107 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias
2022-03-19 18:53:15,109 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight
2022-03-19 18:53:15,110 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias
2022-03-19 18:53:15,111 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight
2022-03-19 18:53:15,113 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias
2022-03-19 18:53:15,114 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight
2022-03-19 18:53:15,115 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2022-03-19 18:53:15,117 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2022-03-19 18:53:15,118 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias
2022-03-19 18:53:15,119 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight
2022-03-19 18:53:15,121 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias
2022-03-19 18:53:15,122 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight
2022-03-19 18:53:15,124 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias
2022-03-19 18:53:15,126 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight
2022-03-19 18:53:15,127 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias
2022-03-19 18:53:15,129 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight
2022-03-19 18:53:15,130 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias
2022-03-19 18:53:15,131 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight
2022-03-19 18:53:15,133 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias
2022-03-19 18:53:15,134 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight
2022-03-19 18:53:15,136 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias
2022-03-19 18:53:15,137 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight
2022-03-19 18:53:15,139 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2022-03-19 18:53:15,141 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2022-03-19 18:53:15,143 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias
2022-03-19 18:53:15,144 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight
2022-03-19 18:53:15,146 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias
2022-03-19 18:53:15,147 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight
2022-03-19 18:53:15,148 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias
2022-03-19 18:53:15,150 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight
2022-03-19 18:53:15,151 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias
2022-03-19 18:53:15,153 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight
2022-03-19 18:53:15,154 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias
2022-03-19 18:53:15,156 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight
2022-03-19 18:53:15,157 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias
2022-03-19 18:53:15,159 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight
2022-03-19 18:53:15,160 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias
2022-03-19 18:53:15,161 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight
2022-03-19 18:53:15,163 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2022-03-19 18:53:15,164 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2022-03-19 18:53:15,166 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias
2022-03-19 18:53:15,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight
2022-03-19 18:53:15,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias
2022-03-19 18:53:15,171 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight
2022-03-19 18:53:15,173 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias
2022-03-19 18:53:15,174 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight
2022-03-19 18:53:15,176 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias
2022-03-19 18:53:15,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight
2022-03-19 18:53:15,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias
2022-03-19 18:53:15,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight
2022-03-19 18:53:15,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias
2022-03-19 18:53:15,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight
2022-03-19 18:53:15,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias
2022-03-19 18:53:15,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight
2022-03-19 18:53:15,189 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2022-03-19 18:53:15,190 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2022-03-19 18:53:15,191 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias
2022-03-19 18:53:15,193 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight
2022-03-19 18:53:15,195 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias
2022-03-19 18:53:15,196 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight
2022-03-19 18:53:15,197 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias
2022-03-19 18:53:15,199 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight
2022-03-19 18:53:15,200 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias
2022-03-19 18:53:15,202 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight
2022-03-19 18:53:15,203 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias
2022-03-19 18:53:15,204 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight
2022-03-19 18:53:15,206 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias
2022-03-19 18:53:15,207 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight
2022-03-19 18:53:15,209 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias
2022-03-19 18:53:15,210 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight
2022-03-19 18:53:15,212 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2022-03-19 18:53:15,213 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2022-03-19 18:53:15,215 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias
2022-03-19 18:53:15,217 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight
2022-03-19 18:53:15,219 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias
2022-03-19 18:53:15,220 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight
2022-03-19 18:53:15,222 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias
2022-03-19 18:53:15,223 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight
2022-03-19 18:53:15,225 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias
2022-03-19 18:53:15,226 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight
2022-03-19 18:53:15,228 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias
2022-03-19 18:53:15,229 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight
2022-03-19 18:53:15,231 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias
2022-03-19 18:53:15,232 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight
2022-03-19 18:53:15,234 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias
2022-03-19 18:53:15,235 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight
2022-03-19 18:53:15,237 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2022-03-19 18:53:15,238 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2022-03-19 18:53:15,240 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias
2022-03-19 18:53:15,241 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight
2022-03-19 18:53:15,243 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias
2022-03-19 18:53:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight
2022-03-19 18:53:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias
2022-03-19 18:53:15,247 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight
2022-03-19 18:53:15,249 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias
2022-03-19 18:53:15,250 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight
2022-03-19 18:53:15,252 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias
2022-03-19 18:53:15,254 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight
2022-03-19 18:53:15,256 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias
2022-03-19 18:53:15,257 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight
2022-03-19 18:53:15,259 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias
2022-03-19 18:53:15,260 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight
2022-03-19 18:53:15,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2022-03-19 18:53:15,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2022-03-19 18:53:15,265 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias
2022-03-19 18:53:15,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight
2022-03-19 18:53:15,268 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias
2022-03-19 18:53:15,269 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight
2022-03-19 18:53:15,270 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias
2022-03-19 18:53:15,272 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight
2022-03-19 18:53:15,273 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias
2022-03-19 18:53:15,274 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight
2022-03-19 18:53:15,276 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias
2022-03-19 18:53:15,277 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight
2022-03-19 18:53:15,279 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias
2022-03-19 18:53:15,280 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight
2022-03-19 18:53:15,282 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias
2022-03-19 18:53:15,283 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight
2022-03-19 18:53:15,285 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2022-03-19 18:53:15,286 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2022-03-19 18:53:15,287 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias
2022-03-19 18:53:15,289 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight
2022-03-19 18:53:15,291 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias
2022-03-19 18:53:15,293 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight
2022-03-19 18:53:15,294 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias
2022-03-19 18:53:15,295 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight
2022-03-19 18:53:15,297 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias
2022-03-19 18:53:15,298 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight
2022-03-19 18:53:15,300 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias
2022-03-19 18:53:15,301 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight
2022-03-19 18:53:15,302 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias
2022-03-19 18:53:15,304 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight
2022-03-19 18:53:15,305 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias
2022-03-19 18:53:15,307 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight
2022-03-19 18:53:15,308 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2022-03-19 18:53:15,309 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2022-03-19 18:53:15,311 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias
2022-03-19 18:53:15,312 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight
2022-03-19 18:53:15,314 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias
2022-03-19 18:53:15,315 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight
2022-03-19 18:53:15,316 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias
2022-03-19 18:53:15,318 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight
2022-03-19 18:53:15,319 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias
2022-03-19 18:53:15,322 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight
2022-03-19 18:53:15,323 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias
2022-03-19 18:53:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight
2022-03-19 18:53:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias
2022-03-19 18:53:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight
2022-03-19 18:53:15,330 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias
2022-03-19 18:53:15,331 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight
2022-03-19 18:53:15,333 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2022-03-19 18:53:15,334 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2022-03-19 18:53:15,335 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias
2022-03-19 18:53:15,337 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight
2022-03-19 18:53:15,338 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias
2022-03-19 18:53:15,339 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight
2022-03-19 18:53:15,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias
2022-03-19 18:53:15,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight
2022-03-19 18:53:15,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias
2022-03-19 18:53:15,349 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight
2022-03-19 18:53:15,350 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias
2022-03-19 18:53:15,351 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight
2022-03-19 18:53:15,353 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias
2022-03-19 18:53:15,354 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight
2022-03-19 18:53:15,355 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias
2022-03-19 18:53:15,357 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight
2022-03-19 18:53:15,358 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2022-03-19 18:53:15,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2022-03-19 18:53:15,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias
2022-03-19 18:53:15,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight
2022-03-19 18:53:15,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias
2022-03-19 18:53:15,367 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight
2022-03-19 18:53:15,369 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias
2022-03-19 18:53:15,370 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight
2022-03-19 18:53:15,372 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias
2022-03-19 18:53:15,373 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight
2022-03-19 18:53:15,374 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias
2022-03-19 18:53:15,376 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight
2022-03-19 18:53:15,377 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias
2022-03-19 18:53:15,378 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight
2022-03-19 18:53:15,380 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias
2022-03-19 18:53:15,381 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight
2022-03-19 18:53:15,383 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias
2022-03-19 18:53:15,384 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight
2022-03-19 18:53:21,253 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-03-19 18:53:21,260 - INFO - allennlp.common.params - trainer.patience = 3
2022-03-19 18:53:21,262 - INFO - allennlp.common.params - trainer.validation_metric = +f1
2022-03-19 18:53:21,264 - INFO - allennlp.common.params - trainer.num_epochs = 10
2022-03-19 18:53:21,266 - INFO - allennlp.common.params - trainer.cuda_device = 2
2022-03-19 18:53:21,267 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-03-19 18:53:21,269 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-03-19 18:53:21,271 - INFO - allennlp.common.params - trainer.distributed = False
2022-03-19 18:53:21,273 - INFO - allennlp.common.params - trainer.world_size = 1
2022-03-19 18:53:21,275 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1
2022-03-19 18:53:21,277 - INFO - allennlp.common.params - trainer.use_amp = False
2022-03-19 18:53:21,278 - INFO - allennlp.common.params - trainer.no_grad = None
2022-03-19 18:53:21,280 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2022-03-19 18:53:21,282 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-03-19 18:53:21,284 - INFO - allennlp.common.params - trainer.moving_average = None
2022-03-19 18:53:21,286 - INFO - allennlp.common.params - trainer.callbacks = None
2022-03-19 18:53:21,288 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True
2022-03-19 18:53:28,348 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw_str_lr
2022-03-19 18:53:28,355 - INFO - allennlp.common.params - trainer.optimizer.lr = 2e-05
2022-03-19 18:53:28,357 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-03-19 18:53:28,360 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-03-19 18:53:28,361 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-03-19 18:53:28,364 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = False
2022-03-19 18:53:28,368 - INFO - allennlp.training.optimizers - Done constructing parameter groups.
2022-03-19 18:53:28,370 - INFO - allennlp.training.optimizers - Group 0: ['_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias', '_classification_layer.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias', '_feedforward_layer._linear_layers.0.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias'], {'weight_decay': 0}
2022-03-19 18:53:28,373 - INFO - allennlp.training.optimizers - Group 1: ['_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight', '_classification_layer.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight', '_feedforward_layer._linear_layers.0.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight'], {}
2022-03-19 18:53:28,376 - WARNING - allennlp.training.optimizers - When constructing parameter groups, layer_norm.weight does not match any parameter name
2022-03-19 18:53:28,378 - INFO - allennlp.training.optimizers - Number of trainable parameters: 125239300
2022-03-19 18:53:28,382 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-03-19 18:53:28,385 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-03-19 18:53:28,387 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight
2022-03-19 18:53:28,389 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight
2022-03-19 18:53:28,390 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight
2022-03-19 18:53:28,392 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight
2022-03-19 18:53:28,394 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias
2022-03-19 18:53:28,395 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight
2022-03-19 18:53:28,397 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias
2022-03-19 18:53:28,400 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight
2022-03-19 18:53:28,401 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias
2022-03-19 18:53:28,403 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight
2022-03-19 18:53:28,405 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias
2022-03-19 18:53:28,407 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight
2022-03-19 18:53:28,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias
2022-03-19 18:53:28,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2022-03-19 18:53:28,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2022-03-19 18:53:28,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight
2022-03-19 18:53:28,417 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias
2022-03-19 18:53:28,419 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight
2022-03-19 18:53:28,421 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias
2022-03-19 18:53:28,423 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight
2022-03-19 18:53:28,425 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias
2022-03-19 18:53:28,427 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight
2022-03-19 18:53:28,428 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias
2022-03-19 18:53:28,430 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight
2022-03-19 18:53:28,432 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias
2022-03-19 18:53:28,434 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight
2022-03-19 18:53:28,435 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias
2022-03-19 18:53:28,437 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight
2022-03-19 18:53:28,438 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias
2022-03-19 18:53:28,443 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2022-03-19 18:53:28,444 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2022-03-19 18:53:28,446 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight
2022-03-19 18:53:28,448 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias
2022-03-19 18:53:28,449 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight
2022-03-19 18:53:28,451 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias
2022-03-19 18:53:28,453 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight
2022-03-19 18:53:28,454 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias
2022-03-19 18:53:28,456 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight
2022-03-19 18:53:28,457 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias
2022-03-19 18:53:28,459 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight
2022-03-19 18:53:28,460 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias
2022-03-19 18:53:28,462 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight
2022-03-19 18:53:28,464 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias
2022-03-19 18:53:28,466 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight
2022-03-19 18:53:28,467 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias
2022-03-19 18:53:28,469 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2022-03-19 18:53:28,470 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2022-03-19 18:53:28,472 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight
2022-03-19 18:53:28,474 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias
2022-03-19 18:53:28,476 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight
2022-03-19 18:53:28,478 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias
2022-03-19 18:53:28,479 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight
2022-03-19 18:53:28,481 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias
2022-03-19 18:53:28,482 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight
2022-03-19 18:53:28,484 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias
2022-03-19 18:53:28,485 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight
2022-03-19 18:53:28,487 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias
2022-03-19 18:53:28,488 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight
2022-03-19 18:53:28,490 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias
2022-03-19 18:53:28,491 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight
2022-03-19 18:53:28,493 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias
2022-03-19 18:53:28,494 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2022-03-19 18:53:28,495 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2022-03-19 18:53:28,497 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight
2022-03-19 18:53:28,499 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias
2022-03-19 18:53:28,501 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight
2022-03-19 18:53:28,502 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias
2022-03-19 18:53:28,504 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight
2022-03-19 18:53:28,506 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias
2022-03-19 18:53:28,507 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight
2022-03-19 18:53:28,509 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias
2022-03-19 18:53:28,510 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight
2022-03-19 18:53:28,512 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias
2022-03-19 18:53:28,513 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight
2022-03-19 18:53:28,515 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias
2022-03-19 18:53:28,516 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight
2022-03-19 18:53:28,518 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias
2022-03-19 18:53:28,520 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2022-03-19 18:53:28,522 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2022-03-19 18:53:28,524 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight
2022-03-19 18:53:28,525 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias
2022-03-19 18:53:28,526 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight
2022-03-19 18:53:28,528 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias
2022-03-19 18:53:28,529 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight
2022-03-19 18:53:28,531 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias
2022-03-19 18:53:28,532 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight
2022-03-19 18:53:28,534 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias
2022-03-19 18:53:28,535 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight
2022-03-19 18:53:28,537 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias
2022-03-19 18:53:28,538 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight
2022-03-19 18:53:28,539 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias
2022-03-19 18:53:28,541 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight
2022-03-19 18:53:28,542 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias
2022-03-19 18:53:28,544 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2022-03-19 18:53:28,546 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2022-03-19 18:53:28,548 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight
2022-03-19 18:53:28,550 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias
2022-03-19 18:53:28,552 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight
2022-03-19 18:53:28,553 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias
2022-03-19 18:53:28,554 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight
2022-03-19 18:53:28,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias
2022-03-19 18:53:28,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight
2022-03-19 18:53:28,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias
2022-03-19 18:53:28,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight
2022-03-19 18:53:28,561 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias
2022-03-19 18:53:28,563 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight
2022-03-19 18:53:28,564 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias
2022-03-19 18:53:28,566 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight
2022-03-19 18:53:28,567 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias
2022-03-19 18:53:28,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2022-03-19 18:53:28,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2022-03-19 18:53:28,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight
2022-03-19 18:53:28,573 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias
2022-03-19 18:53:28,574 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight
2022-03-19 18:53:28,576 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias
2022-03-19 18:53:28,577 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight
2022-03-19 18:53:28,579 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias
2022-03-19 18:53:28,580 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight
2022-03-19 18:53:28,581 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias
2022-03-19 18:53:28,583 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight
2022-03-19 18:53:28,585 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias
2022-03-19 18:53:28,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight
2022-03-19 18:53:28,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias
2022-03-19 18:53:28,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight
2022-03-19 18:53:28,591 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias
2022-03-19 18:53:28,592 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2022-03-19 18:53:28,594 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2022-03-19 18:53:28,595 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight
2022-03-19 18:53:28,597 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias
2022-03-19 18:53:28,600 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight
2022-03-19 18:53:28,602 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias
2022-03-19 18:53:28,603 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight
2022-03-19 18:53:28,605 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias
2022-03-19 18:53:28,606 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight
2022-03-19 18:53:28,607 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias
2022-03-19 18:53:28,609 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight
2022-03-19 18:53:28,610 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias
2022-03-19 18:53:28,612 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight
2022-03-19 18:53:28,613 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias
2022-03-19 18:53:28,615 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight
2022-03-19 18:53:28,616 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias
2022-03-19 18:53:28,617 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2022-03-19 18:53:28,619 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2022-03-19 18:53:28,620 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight
2022-03-19 18:53:28,622 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias
2022-03-19 18:53:28,624 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight
2022-03-19 18:53:28,626 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias
2022-03-19 18:53:28,627 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight
2022-03-19 18:53:28,628 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias
2022-03-19 18:53:28,630 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight
2022-03-19 18:53:28,631 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias
2022-03-19 18:53:28,633 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight
2022-03-19 18:53:28,634 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias
2022-03-19 18:53:28,635 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight
2022-03-19 18:53:28,637 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias
2022-03-19 18:53:28,638 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight
2022-03-19 18:53:28,640 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias
2022-03-19 18:53:28,642 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2022-03-19 18:53:28,643 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2022-03-19 18:53:28,645 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight
2022-03-19 18:53:28,646 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias
2022-03-19 18:53:28,648 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight
2022-03-19 18:53:28,649 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias
2022-03-19 18:53:28,651 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight
2022-03-19 18:53:28,652 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias
2022-03-19 18:53:28,653 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight
2022-03-19 18:53:28,655 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias
2022-03-19 18:53:28,657 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight
2022-03-19 18:53:28,658 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias
2022-03-19 18:53:28,660 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight
2022-03-19 18:53:28,661 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias
2022-03-19 18:53:28,663 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight
2022-03-19 18:53:28,665 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias
2022-03-19 18:53:28,666 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2022-03-19 18:53:28,668 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2022-03-19 18:53:28,670 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight
2022-03-19 18:53:28,672 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias
2022-03-19 18:53:28,673 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight
2022-03-19 18:53:28,675 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias
2022-03-19 18:53:28,676 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight
2022-03-19 18:53:28,678 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias
2022-03-19 18:53:28,679 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight
2022-03-19 18:53:28,680 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias
2022-03-19 18:53:28,682 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight
2022-03-19 18:53:28,683 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias
2022-03-19 18:53:28,686 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight
2022-03-19 18:53:28,687 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias
2022-03-19 18:53:28,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight
2022-03-19 18:53:28,690 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias
2022-03-19 18:53:28,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2022-03-19 18:53:28,693 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2022-03-19 18:53:28,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight
2022-03-19 18:53:28,696 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias
2022-03-19 18:53:28,697 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight
2022-03-19 18:53:28,699 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias
2022-03-19 18:53:28,701 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight
2022-03-19 18:53:28,702 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias
2022-03-19 18:53:28,705 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight
2022-03-19 18:53:28,706 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias
2022-03-19 18:53:28,707 - INFO - allennlp.common.util - _feedforward_layer._linear_layers.0.weight
2022-03-19 18:53:28,709 - INFO - allennlp.common.util - _feedforward_layer._linear_layers.0.bias
2022-03-19 18:53:28,710 - INFO - allennlp.common.util - _classification_layer.weight
2022-03-19 18:53:28,712 - INFO - allennlp.common.util - _classification_layer.bias
2022-03-19 18:53:28,713 - INFO - allennlp.common.params - trainer.checkpointer.type = roberta_default
2022-03-19 18:53:28,715 - INFO - allennlp.common.params - trainer.checkpointer.keep_serialized_model_every_num_seconds = None
2022-03-19 18:53:28,716 - INFO - allennlp.common.params - trainer.checkpointer.num_serialized_models_to_keep = 0
2022-03-19 18:53:28,718 - INFO - allennlp.common.params - trainer.checkpointer.model_save_interval = None
2022-03-19 18:53:28,720 - INFO - allennlp.common.params - trainer.checkpointer.num_epochs = 10
2022-03-19 18:53:28,721 - INFO - allennlp.common.params - trainer.checkpointer.skip_early_stopping = False
2022-03-19 18:53:28,729 - INFO - allennlp.training.trainer - Beginning training.
2022-03-19 18:53:28,731 - INFO - allennlp.training.trainer - Epoch 0/9
2022-03-19 18:53:28,732 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.2G
2022-03-19 18:53:28,734 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-19 18:53:28,736 - INFO - allennlp.training.trainer - Training
2022-03-19 18:53:28,738 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-19 18:53:28,841 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-19 18:53:28,847 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-19 18:53:38,812 - INFO - tqdm - f1: 0.7404, accuracy: 0.7346, batch_loss: 0.1803, loss: 0.6834 ||:   1%|1         | 81/7188 [00:10<10:36, 11.16it/s]
2022-03-19 18:53:48,918 - INFO - tqdm - f1: 0.8249, accuracy: 0.8240, batch_loss: 0.3164, loss: 0.4816 ||:   3%|2         | 185/7188 [00:20<12:05,  9.65it/s]
2022-03-19 18:53:59,030 - INFO - tqdm - f1: 0.8454, accuracy: 0.8439, batch_loss: 0.7291, loss: 0.4385 ||:   4%|4         | 291/7188 [00:30<16:34,  6.94it/s]
2022-03-19 18:54:09,084 - INFO - tqdm - f1: 0.8601, accuracy: 0.8589, batch_loss: 0.4315, loss: 0.4064 ||:   6%|5         | 397/7188 [00:40<24:29,  4.62it/s]
2022-03-19 18:54:19,264 - INFO - tqdm - f1: 0.8665, accuracy: 0.8658, batch_loss: 0.2020, loss: 0.3858 ||:   7%|6         | 503/7188 [00:50<25:18,  4.40it/s]
2022-03-19 18:54:29,605 - INFO - tqdm - f1: 0.8726, accuracy: 0.8718, batch_loss: 0.2355, loss: 0.3679 ||:   9%|8         | 611/7188 [01:00<24:38,  4.45it/s]
2022-03-19 18:54:39,897 - INFO - tqdm - f1: 0.8764, accuracy: 0.8755, batch_loss: 0.3238, loss: 0.3583 ||:  10%|#         | 719/7188 [01:11<23:48,  4.53it/s]
2022-03-19 18:54:50,068 - INFO - tqdm - f1: 0.8786, accuracy: 0.8775, batch_loss: 0.3047, loss: 0.3532 ||:  12%|#1        | 827/7188 [01:21<23:27,  4.52it/s]
2022-03-19 18:55:00,075 - INFO - tqdm - f1: 0.8826, accuracy: 0.8816, batch_loss: 0.2561, loss: 0.3429 ||:  13%|#2        | 933/7188 [01:31<15:03,  6.92it/s]
2022-03-19 18:55:10,159 - INFO - tqdm - f1: 0.8853, accuracy: 0.8844, batch_loss: 0.0834, loss: 0.3348 ||:  14%|#4        | 1037/7188 [01:41<14:28,  7.08it/s]
2022-03-19 18:55:20,247 - INFO - tqdm - f1: 0.8869, accuracy: 0.8861, batch_loss: 0.1712, loss: 0.3287 ||:  16%|#5        | 1143/7188 [01:51<14:24,  7.00it/s]
2022-03-19 18:55:30,343 - INFO - tqdm - f1: 0.8889, accuracy: 0.8882, batch_loss: 0.0680, loss: 0.3219 ||:  17%|#7        | 1249/7188 [02:01<11:54,  8.32it/s]
2022-03-19 18:55:40,403 - INFO - tqdm - f1: 0.8901, accuracy: 0.8894, batch_loss: 0.1913, loss: 0.3191 ||:  19%|#8        | 1355/7188 [02:11<10:16,  9.47it/s]
2022-03-19 18:55:50,445 - INFO - tqdm - f1: 0.8918, accuracy: 0.8913, batch_loss: 0.0421, loss: 0.3135 ||:  20%|##        | 1459/7188 [02:21<10:06,  9.45it/s]
2022-03-19 18:56:00,810 - INFO - tqdm - f1: 0.8939, accuracy: 0.8932, batch_loss: 0.1705, loss: 0.3091 ||:  22%|##1       | 1569/7188 [02:32<20:51,  4.49it/s]
2022-03-19 18:56:10,905 - INFO - tqdm - f1: 0.8945, accuracy: 0.8939, batch_loss: 0.1834, loss: 0.3072 ||:  23%|##3       | 1671/7188 [02:42<06:25, 14.33it/s]
2022-03-19 18:56:21,006 - INFO - tqdm - f1: 0.8943, accuracy: 0.8937, batch_loss: 0.4430, loss: 0.3065 ||:  25%|##4       | 1775/7188 [02:52<06:04, 14.83it/s]
2022-03-19 18:56:31,138 - INFO - tqdm - f1: 0.8959, accuracy: 0.8954, batch_loss: 0.1254, loss: 0.3021 ||:  26%|##6       | 1881/7188 [03:02<06:20, 13.96it/s]
2022-03-19 18:56:41,249 - INFO - tqdm - f1: 0.8973, accuracy: 0.8968, batch_loss: 0.0885, loss: 0.2982 ||:  28%|##7       | 1987/7188 [03:12<06:10, 14.04it/s]
2022-03-19 18:56:51,341 - INFO - tqdm - f1: 0.8984, accuracy: 0.8979, batch_loss: 0.0328, loss: 0.2947 ||:  29%|##9       | 2091/7188 [03:22<05:58, 14.23it/s]
2022-03-19 18:57:01,471 - INFO - tqdm - f1: 0.8993, accuracy: 0.8988, batch_loss: 0.3078, loss: 0.2920 ||:  31%|###       | 2195/7188 [03:32<05:55, 14.03it/s]
2022-03-19 18:57:11,477 - INFO - tqdm - f1: 0.8994, accuracy: 0.8990, batch_loss: 0.1096, loss: 0.2917 ||:  32%|###1      | 2297/7188 [03:42<05:46, 14.13it/s]
2022-03-19 18:57:21,540 - INFO - tqdm - f1: 0.9004, accuracy: 0.9000, batch_loss: 0.9141, loss: 0.2899 ||:  33%|###3      | 2401/7188 [03:52<05:24, 14.75it/s]
2022-03-19 18:57:31,562 - INFO - tqdm - f1: 0.9014, accuracy: 0.9010, batch_loss: 0.3471, loss: 0.2874 ||:  35%|###4      | 2505/7188 [04:02<05:25, 14.37it/s]
2022-03-19 18:57:41,639 - INFO - tqdm - f1: 0.9025, accuracy: 0.9021, batch_loss: 0.5135, loss: 0.2843 ||:  36%|###6      | 2611/7188 [04:12<05:17, 14.41it/s]
2022-03-19 18:57:51,680 - INFO - tqdm - f1: 0.9032, accuracy: 0.9028, batch_loss: 0.0637, loss: 0.2821 ||:  38%|###7      | 2717/7188 [04:22<05:17, 14.07it/s]
2022-03-19 18:58:01,756 - INFO - tqdm - f1: 0.9037, accuracy: 0.9033, batch_loss: 0.0234, loss: 0.2802 ||:  39%|###9      | 2827/7188 [04:33<04:53, 14.88it/s]
2022-03-19 18:58:11,847 - INFO - tqdm - f1: 0.9045, accuracy: 0.9042, batch_loss: 0.1321, loss: 0.2780 ||:  41%|####      | 2933/7188 [04:43<04:48, 14.75it/s]
2022-03-19 18:58:21,974 - INFO - tqdm - f1: 0.9050, accuracy: 0.9046, batch_loss: 0.1512, loss: 0.2760 ||:  42%|####2     | 3039/7188 [04:53<04:56, 13.98it/s]
2022-03-19 18:58:31,995 - INFO - tqdm - f1: 0.9055, accuracy: 0.9052, batch_loss: 0.2343, loss: 0.2747 ||:  44%|####3     | 3141/7188 [05:03<04:52, 13.84it/s]
2022-03-19 18:58:42,042 - INFO - tqdm - f1: 0.9055, accuracy: 0.9052, batch_loss: 0.4536, loss: 0.2748 ||:  45%|####5     | 3243/7188 [05:13<05:10, 12.71it/s]
2022-03-19 18:58:52,154 - INFO - tqdm - f1: 0.9056, accuracy: 0.9053, batch_loss: 0.3423, loss: 0.2746 ||:  47%|####6     | 3345/7188 [05:23<05:35, 11.46it/s]
2022-03-19 18:59:02,277 - INFO - tqdm - f1: 0.9064, accuracy: 0.9062, batch_loss: 0.1693, loss: 0.2724 ||:  48%|####7     | 3447/7188 [05:33<05:22, 11.59it/s]
2022-03-19 18:59:12,339 - INFO - tqdm - f1: 0.9067, accuracy: 0.9064, batch_loss: 0.0802, loss: 0.2714 ||:  49%|####9     | 3551/7188 [05:43<05:02, 12.03it/s]
2022-03-19 18:59:22,427 - INFO - tqdm - f1: 0.9072, accuracy: 0.9069, batch_loss: 0.5674, loss: 0.2698 ||:  51%|#####     | 3655/7188 [05:53<07:09,  8.22it/s]
2022-03-19 18:59:32,502 - INFO - tqdm - f1: 0.9076, accuracy: 0.9073, batch_loss: 0.1769, loss: 0.2680 ||:  52%|#####2    | 3759/7188 [06:03<06:57,  8.20it/s]
2022-03-19 18:59:42,580 - INFO - tqdm - f1: 0.9080, accuracy: 0.9078, batch_loss: 0.3409, loss: 0.2666 ||:  54%|#####3    | 3863/7188 [06:13<07:58,  6.95it/s]
2022-03-19 18:59:52,583 - INFO - tqdm - f1: 0.9084, accuracy: 0.9081, batch_loss: 0.0698, loss: 0.2656 ||:  55%|#####5    | 3967/7188 [06:23<09:40,  5.55it/s]
2022-03-19 19:00:02,733 - INFO - tqdm - f1: 0.9089, accuracy: 0.9086, batch_loss: 0.5127, loss: 0.2641 ||:  57%|#####6    | 4073/7188 [06:33<11:40,  4.45it/s]
2022-03-19 19:00:12,935 - INFO - tqdm - f1: 0.9093, accuracy: 0.9090, batch_loss: 0.1268, loss: 0.2628 ||:  58%|#####8    | 4181/7188 [06:44<11:17,  4.44it/s]
2022-03-19 19:00:23,075 - INFO - tqdm - f1: 0.9098, accuracy: 0.9095, batch_loss: 0.4203, loss: 0.2612 ||:  60%|#####9    | 4291/7188 [06:54<08:30,  5.67it/s]
2022-03-19 19:00:33,203 - INFO - tqdm - f1: 0.9102, accuracy: 0.9099, batch_loss: 0.1450, loss: 0.2602 ||:  61%|######1   | 4397/7188 [07:04<10:33,  4.41it/s]
2022-03-19 19:00:43,236 - INFO - tqdm - f1: 0.9105, accuracy: 0.9102, batch_loss: 0.0257, loss: 0.2591 ||:  63%|######2   | 4503/7188 [07:14<10:05,  4.43it/s]
2022-03-19 19:00:53,313 - INFO - tqdm - f1: 0.9109, accuracy: 0.9107, batch_loss: 0.1210, loss: 0.2579 ||:  64%|######4   | 4609/7188 [07:24<09:31,  4.52it/s]
2022-03-19 19:01:03,519 - INFO - tqdm - f1: 0.9112, accuracy: 0.9109, batch_loss: 0.2024, loss: 0.2572 ||:  66%|######5   | 4717/7188 [07:34<09:10,  4.49it/s]
2022-03-19 19:01:13,576 - INFO - tqdm - f1: 0.9115, accuracy: 0.9112, batch_loss: 0.0647, loss: 0.2560 ||:  67%|######7   | 4823/7188 [07:44<08:51,  4.45it/s]
2022-03-19 19:01:23,630 - INFO - tqdm - f1: 0.9118, accuracy: 0.9115, batch_loss: 0.2277, loss: 0.2553 ||:  69%|######8   | 4929/7188 [07:54<06:36,  5.69it/s]
2022-03-19 19:01:33,743 - INFO - tqdm - f1: 0.9121, accuracy: 0.9119, batch_loss: 0.1543, loss: 0.2545 ||:  70%|#######   | 5037/7188 [08:05<05:02,  7.10it/s]
2022-03-19 19:01:43,906 - INFO - tqdm - f1: 0.9125, accuracy: 0.9122, batch_loss: 0.0150, loss: 0.2534 ||:  72%|#######1  | 5145/7188 [08:15<07:36,  4.48it/s]
2022-03-19 19:01:54,021 - INFO - tqdm - f1: 0.9129, accuracy: 0.9126, batch_loss: 0.1850, loss: 0.2526 ||:  73%|#######3  | 5253/7188 [08:25<05:46,  5.59it/s]
2022-03-19 19:02:04,121 - INFO - tqdm - f1: 0.9131, accuracy: 0.9128, batch_loss: 0.1165, loss: 0.2518 ||:  75%|#######4  | 5361/7188 [08:35<05:12,  5.84it/s]
2022-03-19 19:02:14,241 - INFO - tqdm - f1: 0.9131, accuracy: 0.9129, batch_loss: 0.4938, loss: 0.2515 ||:  76%|#######6  | 5471/7188 [08:45<04:57,  5.76it/s]
2022-03-19 19:02:24,351 - INFO - tqdm - f1: 0.9133, accuracy: 0.9131, batch_loss: 0.3221, loss: 0.2509 ||:  78%|#######7  | 5581/7188 [08:55<05:51,  4.57it/s]
2022-03-19 19:02:34,753 - INFO - tqdm - f1: 0.9138, accuracy: 0.9135, batch_loss: 0.1104, loss: 0.2500 ||:  79%|#######9  | 5693/7188 [09:06<05:32,  4.50it/s]
2022-03-19 19:02:44,859 - INFO - tqdm - f1: 0.9139, accuracy: 0.9137, batch_loss: 0.1678, loss: 0.2495 ||:  81%|########  | 5801/7188 [09:16<05:07,  4.51it/s]
2022-03-19 19:02:55,092 - INFO - tqdm - f1: 0.9142, accuracy: 0.9140, batch_loss: 0.2654, loss: 0.2485 ||:  82%|########2 | 5913/7188 [09:26<04:42,  4.52it/s]
2022-03-19 19:03:05,125 - INFO - tqdm - f1: 0.9144, accuracy: 0.9142, batch_loss: 0.1552, loss: 0.2479 ||:  84%|########3 | 6023/7188 [09:36<04:13,  4.60it/s]
2022-03-19 19:03:15,158 - INFO - tqdm - f1: 0.9148, accuracy: 0.9145, batch_loss: 0.0270, loss: 0.2470 ||:  85%|########5 | 6131/7188 [09:46<03:55,  4.49it/s]
2022-03-19 19:03:25,406 - INFO - tqdm - f1: 0.9149, accuracy: 0.9147, batch_loss: 0.2831, loss: 0.2463 ||:  87%|########6 | 6243/7188 [09:56<03:30,  4.48it/s]
2022-03-19 19:03:35,492 - INFO - tqdm - f1: 0.9151, accuracy: 0.9150, batch_loss: 0.0322, loss: 0.2450 ||:  88%|########8 | 6351/7188 [10:06<02:29,  5.60it/s]
2022-03-19 19:03:45,495 - INFO - tqdm - f1: 0.9154, accuracy: 0.9152, batch_loss: 0.1562, loss: 0.2440 ||:  90%|########9 | 6459/7188 [10:16<02:42,  4.50it/s]
2022-03-19 19:03:55,601 - INFO - tqdm - f1: 0.9156, accuracy: 0.9154, batch_loss: 0.2302, loss: 0.2436 ||:  91%|#########1| 6565/7188 [10:26<01:51,  5.61it/s]
2022-03-19 19:04:05,641 - INFO - tqdm - f1: 0.9158, accuracy: 0.9156, batch_loss: 0.2439, loss: 0.2428 ||:  93%|#########2| 6669/7188 [10:36<01:56,  4.44it/s]
2022-03-19 19:04:15,727 - INFO - tqdm - f1: 0.9160, accuracy: 0.9159, batch_loss: 0.0616, loss: 0.2419 ||:  94%|#########4| 6775/7188 [10:46<00:58,  7.10it/s]
2022-03-19 19:04:25,828 - INFO - tqdm - f1: 0.9162, accuracy: 0.9162, batch_loss: 0.1190, loss: 0.2408 ||:  96%|#########5| 6881/7188 [10:57<00:43,  7.03it/s]
2022-03-19 19:04:35,880 - INFO - tqdm - f1: 0.9164, accuracy: 0.9163, batch_loss: 0.1469, loss: 0.2403 ||:  97%|#########7| 6987/7188 [11:07<00:28,  7.11it/s]
2022-03-19 19:04:45,969 - INFO - tqdm - f1: 0.9165, accuracy: 0.9164, batch_loss: 0.1668, loss: 0.2402 ||:  99%|#########8| 7093/7188 [11:17<00:20,  4.55it/s]
2022-03-19 19:04:50,936 - INFO - tqdm - f1: 0.9165, accuracy: 0.9165, batch_loss: 0.0326, loss: 0.2398 ||: 100%|#########9| 7153/7188 [11:22<00:02, 14.40it/s]
2022-03-19 19:04:51,079 - INFO - tqdm - f1: 0.9165, accuracy: 0.9165, batch_loss: 0.1333, loss: 0.2397 ||: 100%|#########9| 7155/7188 [11:22<00:02, 14.26it/s]
2022-03-19 19:04:51,205 - INFO - tqdm - f1: 0.9166, accuracy: 0.9165, batch_loss: 0.1476, loss: 0.2397 ||: 100%|#########9| 7157/7188 [11:22<00:02, 14.71it/s]
2022-03-19 19:04:51,330 - INFO - tqdm - f1: 0.9166, accuracy: 0.9165, batch_loss: 0.0118, loss: 0.2397 ||: 100%|#########9| 7159/7188 [11:22<00:01, 15.07it/s]
2022-03-19 19:04:51,460 - INFO - tqdm - f1: 0.9166, accuracy: 0.9165, batch_loss: 0.0579, loss: 0.2396 ||: 100%|#########9| 7161/7188 [11:22<00:01, 15.19it/s]
2022-03-19 19:04:51,610 - INFO - tqdm - f1: 0.9166, accuracy: 0.9166, batch_loss: 0.0295, loss: 0.2396 ||: 100%|#########9| 7163/7188 [11:22<00:01, 14.56it/s]
2022-03-19 19:04:52,801 - INFO - tqdm - f1: 0.9166, accuracy: 0.9166, batch_loss: 0.1319, loss: 0.2395 ||: 100%|#########9| 7165/7188 [11:24<00:05,  4.41it/s]
2022-03-19 19:04:52,952 - INFO - tqdm - f1: 0.9166, accuracy: 0.9166, batch_loss: 0.0247, loss: 0.2395 ||: 100%|#########9| 7167/7188 [11:24<00:03,  5.51it/s]
2022-03-19 19:04:53,087 - INFO - tqdm - f1: 0.9167, accuracy: 0.9166, batch_loss: 0.1356, loss: 0.2395 ||: 100%|#########9| 7169/7188 [11:24<00:02,  6.79it/s]
2022-03-19 19:04:53,215 - INFO - tqdm - f1: 0.9166, accuracy: 0.9166, batch_loss: 0.0437, loss: 0.2395 ||: 100%|#########9| 7171/7188 [11:24<00:02,  8.18it/s]
2022-03-19 19:04:53,342 - INFO - tqdm - f1: 0.9166, accuracy: 0.9166, batch_loss: 0.2035, loss: 0.2395 ||: 100%|#########9| 7173/7188 [11:24<00:01,  9.55it/s]
2022-03-19 19:04:53,483 - INFO - tqdm - f1: 0.9167, accuracy: 0.9166, batch_loss: 0.0886, loss: 0.2394 ||: 100%|#########9| 7175/7188 [11:24<00:01, 10.60it/s]
2022-03-19 19:04:53,616 - INFO - tqdm - f1: 0.9167, accuracy: 0.9166, batch_loss: 0.5354, loss: 0.2395 ||: 100%|#########9| 7177/7188 [11:24<00:00, 11.63it/s]
2022-03-19 19:04:53,743 - INFO - tqdm - f1: 0.9167, accuracy: 0.9166, batch_loss: 0.0294, loss: 0.2394 ||: 100%|#########9| 7179/7188 [11:25<00:00, 12.62it/s]
2022-03-19 19:04:53,885 - INFO - tqdm - f1: 0.9167, accuracy: 0.9166, batch_loss: 0.0289, loss: 0.2394 ||: 100%|#########9| 7181/7188 [11:25<00:00, 13.02it/s]
2022-03-19 19:04:54,031 - INFO - tqdm - f1: 0.9167, accuracy: 0.9167, batch_loss: 0.0833, loss: 0.2394 ||: 100%|#########9| 7183/7188 [11:25<00:00, 13.22it/s]
2022-03-19 19:04:54,174 - INFO - tqdm - f1: 0.9167, accuracy: 0.9167, batch_loss: 0.1474, loss: 0.2394 ||: 100%|#########9| 7185/7188 [11:25<00:00, 13.45it/s]
2022-03-19 19:04:54,310 - INFO - tqdm - f1: 0.9167, accuracy: 0.9167, batch_loss: 0.1387, loss: 0.2393 ||: 100%|#########9| 7187/7188 [11:25<00:00, 13.80it/s]
2022-03-19 19:04:54,419 - INFO - tqdm - f1: 0.9167, accuracy: 0.9167, batch_loss: 0.1187, loss: 0.2393 ||: 100%|##########| 7188/7188 [11:25<00:00, 10.48it/s]
2022-03-19 19:04:54,426 - INFO - allennlp.training.trainer - Validating
2022-03-19 19:04:54,430 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-19 19:04:54,437 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-19 19:04:54,439 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-19 19:05:04,375 - INFO - tqdm - f1: 0.9311, accuracy: 0.9312, batch_loss: 0.1255, loss: 0.2074 ||: 100%|#########9| 312/313 [00:09<00:00, 42.51it/s]
2022-03-19 19:05:04,409 - INFO - tqdm - f1: 0.9311, accuracy: 0.9312, batch_loss: 0.1143, loss: 0.2071 ||: 100%|##########| 313/313 [00:09<00:00, 31.37it/s]
2022-03-19 19:05:04,423 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/ag_base/best.th'.
2022-03-19 19:05:06,910 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-19 19:05:06,912 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.917  |     0.931
2022-03-19 19:05:06,914 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.917  |     0.931
2022-03-19 19:05:06,915 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-19 19:05:06,917 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.239  |     0.207
2022-03-19 19:05:06,918 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7387.395  |       N/A
2022-03-19 19:05:06,920 - INFO - allennlp.training.trainer - Epoch duration: 0:11:38.189100
2022-03-19 19:05:06,922 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:44:43
2022-03-19 19:05:06,924 - INFO - allennlp.training.trainer - Epoch 1/9
2022-03-19 19:05:06,926 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-19 19:05:06,928 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-19 19:05:06,931 - INFO - allennlp.training.trainer - Training
2022-03-19 19:05:06,933 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-19 19:05:17,028 - INFO - tqdm - f1: 0.9402, accuracy: 0.9401, batch_loss: 0.2072, loss: 0.1650 ||:   1%|1         | 97/7188 [00:10<14:16,  8.28it/s]
2022-03-19 19:05:27,048 - INFO - tqdm - f1: 0.9416, accuracy: 0.9415, batch_loss: 0.5298, loss: 0.1708 ||:   3%|2         | 201/7188 [00:20<16:28,  7.07it/s]
2022-03-19 19:05:37,087 - INFO - tqdm - f1: 0.9399, accuracy: 0.9399, batch_loss: 0.0176, loss: 0.1727 ||:   4%|4         | 311/7188 [00:30<16:00,  7.16it/s]
2022-03-19 19:05:47,223 - INFO - tqdm - f1: 0.9405, accuracy: 0.9406, batch_loss: 0.1200, loss: 0.1697 ||:   6%|5         | 417/7188 [00:40<16:17,  6.93it/s]
2022-03-19 19:05:57,279 - INFO - tqdm - f1: 0.9387, accuracy: 0.9387, batch_loss: 0.0710, loss: 0.1764 ||:   7%|7         | 521/7188 [00:50<16:01,  6.93it/s]
2022-03-19 19:06:07,352 - INFO - tqdm - f1: 0.9398, accuracy: 0.9398, batch_loss: 0.0179, loss: 0.1742 ||:   9%|8         | 625/7188 [01:00<15:49,  6.91it/s]
2022-03-19 19:06:17,485 - INFO - tqdm - f1: 0.9394, accuracy: 0.9393, batch_loss: 0.0405, loss: 0.1753 ||:  10%|#         | 731/7188 [01:10<15:28,  6.95it/s]
2022-03-19 19:06:27,579 - INFO - tqdm - f1: 0.9388, accuracy: 0.9388, batch_loss: 0.5514, loss: 0.1761 ||:  12%|#1        | 837/7188 [01:20<18:44,  5.65it/s]
2022-03-19 19:06:37,641 - INFO - tqdm - f1: 0.9389, accuracy: 0.9388, batch_loss: 0.0589, loss: 0.1757 ||:  13%|#3        | 945/7188 [01:30<18:26,  5.64it/s]
2022-03-19 19:06:47,678 - INFO - tqdm - f1: 0.9378, accuracy: 0.9379, batch_loss: 0.2131, loss: 0.1770 ||:  15%|#4        | 1049/7188 [01:40<18:15,  5.60it/s]
2022-03-19 19:06:57,828 - INFO - tqdm - f1: 0.9393, accuracy: 0.9392, batch_loss: 0.0983, loss: 0.1738 ||:  16%|#6        | 1155/7188 [01:50<22:47,  4.41it/s]
2022-03-19 19:07:07,939 - INFO - tqdm - f1: 0.9390, accuracy: 0.9389, batch_loss: 0.0420, loss: 0.1733 ||:  17%|#7        | 1255/7188 [02:01<22:08,  4.47it/s]
2022-03-19 19:07:18,062 - INFO - tqdm - f1: 0.9398, accuracy: 0.9397, batch_loss: 0.0845, loss: 0.1715 ||:  19%|#8        | 1359/7188 [02:11<17:01,  5.71it/s]
2022-03-19 19:07:28,176 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.3616, loss: 0.1698 ||:  20%|##        | 1465/7188 [02:21<16:47,  5.68it/s]
2022-03-19 19:07:38,300 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.2585, loss: 0.1694 ||:  22%|##1       | 1571/7188 [02:31<13:18,  7.03it/s]
2022-03-19 19:07:48,365 - INFO - tqdm - f1: 0.9405, accuracy: 0.9405, batch_loss: 0.1913, loss: 0.1680 ||:  23%|##3       | 1675/7188 [02:41<12:55,  7.11it/s]
2022-03-19 19:07:58,379 - INFO - tqdm - f1: 0.9407, accuracy: 0.9408, batch_loss: 0.2113, loss: 0.1679 ||:  25%|##4       | 1779/7188 [02:51<19:57,  4.52it/s]
2022-03-19 19:08:08,478 - INFO - tqdm - f1: 0.9399, accuracy: 0.9400, batch_loss: 0.0671, loss: 0.1700 ||:  26%|##6       | 1885/7188 [03:01<19:10,  4.61it/s]
2022-03-19 19:08:18,609 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.2042, loss: 0.1696 ||:  28%|##7       | 1991/7188 [03:11<12:26,  6.96it/s]
2022-03-19 19:08:28,683 - INFO - tqdm - f1: 0.9398, accuracy: 0.9399, batch_loss: 0.3868, loss: 0.1704 ||:  29%|##9       | 2099/7188 [03:21<11:58,  7.08it/s]
2022-03-19 19:08:38,765 - INFO - tqdm - f1: 0.9397, accuracy: 0.9397, batch_loss: 0.0886, loss: 0.1714 ||:  31%|###       | 2207/7188 [03:31<11:40,  7.11it/s]
2022-03-19 19:08:48,864 - INFO - tqdm - f1: 0.9393, accuracy: 0.9394, batch_loss: 0.0809, loss: 0.1720 ||:  32%|###2      | 2311/7188 [03:41<14:22,  5.66it/s]
2022-03-19 19:08:58,899 - INFO - tqdm - f1: 0.9388, accuracy: 0.9389, batch_loss: 0.3788, loss: 0.1734 ||:  34%|###3      | 2417/7188 [03:51<17:30,  4.54it/s]
2022-03-19 19:09:09,112 - INFO - tqdm - f1: 0.9387, accuracy: 0.9387, batch_loss: 0.5953, loss: 0.1741 ||:  35%|###5      | 2523/7188 [04:02<18:18,  4.25it/s]
2022-03-19 19:09:19,322 - INFO - tqdm - f1: 0.9388, accuracy: 0.9388, batch_loss: 0.0375, loss: 0.1740 ||:  37%|###6      | 2631/7188 [04:12<14:16,  5.32it/s]
2022-03-19 19:09:29,476 - INFO - tqdm - f1: 0.9389, accuracy: 0.9390, batch_loss: 0.0393, loss: 0.1730 ||:  38%|###8      | 2737/7188 [04:22<16:32,  4.49it/s]
2022-03-19 19:09:39,693 - INFO - tqdm - f1: 0.9394, accuracy: 0.9394, batch_loss: 0.0411, loss: 0.1720 ||:  40%|###9      | 2845/7188 [04:32<15:57,  4.54it/s]
2022-03-19 19:09:49,998 - INFO - tqdm - f1: 0.9394, accuracy: 0.9394, batch_loss: 0.2082, loss: 0.1725 ||:  41%|####1     | 2955/7188 [04:43<15:35,  4.52it/s]
2022-03-19 19:10:00,224 - INFO - tqdm - f1: 0.9393, accuracy: 0.9393, batch_loss: 0.0906, loss: 0.1728 ||:  43%|####2     | 3063/7188 [04:53<15:07,  4.54it/s]
2022-03-19 19:10:10,505 - INFO - tqdm - f1: 0.9392, accuracy: 0.9393, batch_loss: 0.0232, loss: 0.1732 ||:  44%|####4     | 3173/7188 [05:03<15:13,  4.39it/s]
2022-03-19 19:10:20,614 - INFO - tqdm - f1: 0.9394, accuracy: 0.9394, batch_loss: 0.1367, loss: 0.1729 ||:  46%|####5     | 3279/7188 [05:13<14:41,  4.43it/s]
2022-03-19 19:10:30,824 - INFO - tqdm - f1: 0.9397, accuracy: 0.9397, batch_loss: 0.0866, loss: 0.1723 ||:  47%|####7     | 3389/7188 [05:23<13:47,  4.59it/s]
2022-03-19 19:10:41,229 - INFO - tqdm - f1: 0.9397, accuracy: 0.9398, batch_loss: 0.1897, loss: 0.1723 ||:  49%|####8     | 3499/7188 [05:34<13:56,  4.41it/s]
2022-03-19 19:10:51,461 - INFO - tqdm - f1: 0.9397, accuracy: 0.9397, batch_loss: 0.1612, loss: 0.1728 ||:  50%|#####     | 3611/7188 [05:44<13:17,  4.49it/s]
2022-03-19 19:11:01,587 - INFO - tqdm - f1: 0.9397, accuracy: 0.9397, batch_loss: 0.1747, loss: 0.1729 ||:  52%|#####1    | 3717/7188 [05:54<07:03,  8.20it/s]
2022-03-19 19:11:11,607 - INFO - tqdm - f1: 0.9399, accuracy: 0.9399, batch_loss: 0.0128, loss: 0.1723 ||:  53%|#####3    | 3821/7188 [06:04<09:48,  5.72it/s]
2022-03-19 19:11:21,690 - INFO - tqdm - f1: 0.9397, accuracy: 0.9396, batch_loss: 0.0677, loss: 0.1727 ||:  55%|#####4    | 3925/7188 [06:14<12:12,  4.45it/s]
2022-03-19 19:11:31,934 - INFO - tqdm - f1: 0.9399, accuracy: 0.9399, batch_loss: 0.0257, loss: 0.1723 ||:  56%|#####6    | 4035/7188 [06:24<11:43,  4.48it/s]
2022-03-19 19:11:42,047 - INFO - tqdm - f1: 0.9400, accuracy: 0.9400, batch_loss: 0.0326, loss: 0.1718 ||:  58%|#####7    | 4141/7188 [06:35<11:25,  4.45it/s]
2022-03-19 19:11:52,184 - INFO - tqdm - f1: 0.9402, accuracy: 0.9402, batch_loss: 0.2337, loss: 0.1717 ||:  59%|#####9    | 4247/7188 [06:45<08:29,  5.78it/s]
2022-03-19 19:12:02,303 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0220, loss: 0.1710 ||:  61%|######    | 4355/7188 [06:55<06:42,  7.03it/s]
2022-03-19 19:12:12,311 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.0502, loss: 0.1712 ||:  62%|######2   | 4461/7188 [07:05<07:56,  5.73it/s]
2022-03-19 19:12:22,421 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.6160, loss: 0.1711 ||:  64%|######3   | 4569/7188 [07:15<05:11,  8.40it/s]
2022-03-19 19:12:32,544 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.0521, loss: 0.1712 ||:  65%|######5   | 4675/7188 [07:25<06:05,  6.88it/s]
2022-03-19 19:12:42,636 - INFO - tqdm - f1: 0.9402, accuracy: 0.9402, batch_loss: 0.0184, loss: 0.1714 ||:  67%|######6   | 4781/7188 [07:35<05:42,  7.03it/s]
2022-03-19 19:12:52,659 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.0060, loss: 0.1712 ||:  68%|######7   | 4887/7188 [07:45<06:38,  5.77it/s]
2022-03-19 19:13:02,796 - INFO - tqdm - f1: 0.9403, accuracy: 0.9402, batch_loss: 0.1608, loss: 0.1712 ||:  70%|######9   | 4997/7188 [07:55<05:12,  7.02it/s]
2022-03-19 19:13:12,850 - INFO - tqdm - f1: 0.9402, accuracy: 0.9401, batch_loss: 0.4174, loss: 0.1715 ||:  71%|#######1  | 5105/7188 [08:05<07:41,  4.51it/s]
2022-03-19 19:13:23,081 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.2141, loss: 0.1719 ||:  73%|#######2  | 5217/7188 [08:16<07:06,  4.62it/s]
2022-03-19 19:13:33,232 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.0821, loss: 0.1718 ||:  74%|#######4  | 5327/7188 [08:26<06:50,  4.53it/s]
2022-03-19 19:13:43,234 - INFO - tqdm - f1: 0.9400, accuracy: 0.9400, batch_loss: 0.0499, loss: 0.1717 ||:  76%|#######5  | 5431/7188 [08:36<05:10,  5.66it/s]
2022-03-19 19:13:53,309 - INFO - tqdm - f1: 0.9400, accuracy: 0.9400, batch_loss: 0.0351, loss: 0.1716 ||:  77%|#######7  | 5539/7188 [08:46<04:42,  5.84it/s]
2022-03-19 19:14:03,374 - INFO - tqdm - f1: 0.9402, accuracy: 0.9402, batch_loss: 0.0421, loss: 0.1710 ||:  79%|#######8  | 5647/7188 [08:56<05:40,  4.53it/s]
2022-03-19 19:14:13,470 - INFO - tqdm - f1: 0.9402, accuracy: 0.9402, batch_loss: 0.5412, loss: 0.1712 ||:  80%|########  | 5755/7188 [09:06<05:08,  4.64it/s]
2022-03-19 19:14:23,541 - INFO - tqdm - f1: 0.9400, accuracy: 0.9400, batch_loss: 0.0741, loss: 0.1717 ||:  82%|########1 | 5859/7188 [09:16<04:55,  4.49it/s]
2022-03-19 19:14:33,543 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.3672, loss: 0.1715 ||:  83%|########2 | 5965/7188 [09:26<04:24,  4.62it/s]
2022-03-19 19:14:43,662 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0261, loss: 0.1711 ||:  84%|########4 | 6071/7188 [09:36<04:06,  4.53it/s]
2022-03-19 19:14:53,905 - INFO - tqdm - f1: 0.9405, accuracy: 0.9405, batch_loss: 0.0453, loss: 0.1709 ||:  86%|########5 | 6179/7188 [09:46<04:00,  4.19it/s]
2022-03-19 19:15:04,108 - INFO - tqdm - f1: 0.9405, accuracy: 0.9405, batch_loss: 0.0892, loss: 0.1711 ||:  87%|########7 | 6289/7188 [09:57<03:15,  4.59it/s]
2022-03-19 19:15:14,160 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.1760, loss: 0.1712 ||:  89%|########9 | 6399/7188 [10:07<02:53,  4.55it/s]
2022-03-19 19:15:24,273 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0277, loss: 0.1711 ||:  91%|######### | 6509/7188 [10:17<02:29,  4.55it/s]
2022-03-19 19:15:34,596 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.1298, loss: 0.1710 ||:  92%|#########2| 6623/7188 [10:27<02:05,  4.52it/s]
2022-03-19 19:15:44,697 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0629, loss: 0.1712 ||:  94%|#########3| 6735/7188 [10:37<01:37,  4.64it/s]
2022-03-19 19:15:54,978 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.1093, loss: 0.1714 ||:  95%|#########5| 6849/7188 [10:48<01:13,  4.59it/s]
2022-03-19 19:16:05,027 - INFO - tqdm - f1: 0.9402, accuracy: 0.9401, batch_loss: 1.1409, loss: 0.1717 ||:  97%|#########6| 6957/7188 [10:58<00:50,  4.56it/s]
2022-03-19 19:16:15,215 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.0127, loss: 0.1721 ||:  98%|#########8| 7067/7188 [11:08<00:26,  4.53it/s]
2022-03-19 19:16:22,874 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.4113, loss: 0.1719 ||: 100%|#########9| 7153/7188 [11:15<00:02, 12.73it/s]
2022-03-19 19:16:23,013 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.2135, loss: 0.1720 ||: 100%|#########9| 7155/7188 [11:16<00:02, 13.21it/s]
2022-03-19 19:16:23,150 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.2637, loss: 0.1720 ||: 100%|#########9| 7157/7188 [11:16<00:02, 13.58it/s]
2022-03-19 19:16:23,289 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.0563, loss: 0.1719 ||: 100%|#########9| 7159/7188 [11:16<00:02, 13.83it/s]
2022-03-19 19:16:23,429 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.2307, loss: 0.1719 ||: 100%|#########9| 7161/7188 [11:16<00:01, 13.95it/s]
2022-03-19 19:16:23,570 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.0936, loss: 0.1720 ||: 100%|#########9| 7163/7188 [11:16<00:01, 14.02it/s]
2022-03-19 19:16:23,708 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.2013, loss: 0.1720 ||: 100%|#########9| 7165/7188 [11:16<00:01, 14.15it/s]
2022-03-19 19:16:23,841 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.2441, loss: 0.1720 ||: 100%|#########9| 7167/7188 [11:16<00:01, 14.42it/s]
2022-03-19 19:16:23,976 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.3722, loss: 0.1720 ||: 100%|#########9| 7169/7188 [11:17<00:01, 14.54it/s]
2022-03-19 19:16:24,109 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.0224, loss: 0.1719 ||: 100%|#########9| 7171/7188 [11:17<00:01, 14.67it/s]
2022-03-19 19:16:24,237 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.0099, loss: 0.1719 ||: 100%|#########9| 7173/7188 [11:17<00:01, 14.94it/s]
2022-03-19 19:16:25,447 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.2890, loss: 0.1719 ||: 100%|#########9| 7175/7188 [11:18<00:02,  4.38it/s]
2022-03-19 19:16:25,600 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.1092, loss: 0.1720 ||: 100%|#########9| 7177/7188 [11:18<00:02,  5.47it/s]
2022-03-19 19:16:25,737 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.4037, loss: 0.1720 ||: 100%|#########9| 7179/7188 [11:18<00:01,  6.73it/s]
2022-03-19 19:16:25,886 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.5457, loss: 0.1720 ||: 100%|#########9| 7181/7188 [11:18<00:00,  7.92it/s]
2022-03-19 19:16:26,022 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.0142, loss: 0.1720 ||: 100%|#########9| 7183/7188 [11:19<00:00,  9.19it/s]
2022-03-19 19:16:26,153 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.1079, loss: 0.1720 ||: 100%|#########9| 7185/7188 [11:19<00:00, 10.43it/s]
2022-03-19 19:16:26,285 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.1255, loss: 0.1720 ||: 100%|#########9| 7187/7188 [11:19<00:00, 11.52it/s]
2022-03-19 19:16:26,397 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.3102, loss: 0.1720 ||: 100%|##########| 7188/7188 [11:19<00:00, 10.58it/s]
2022-03-19 19:16:26,404 - INFO - allennlp.training.trainer - Validating
2022-03-19 19:16:26,406 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-19 19:16:36,488 - INFO - tqdm - f1: 0.9337, accuracy: 0.9338, batch_loss: 0.2633, loss: 0.1888 ||: 100%|#########9| 312/313 [00:10<00:00, 34.27it/s]
2022-03-19 19:16:36,517 - INFO - tqdm - f1: 0.9338, accuracy: 0.9338, batch_loss: 0.1454, loss: 0.1886 ||: 100%|##########| 313/313 [00:10<00:00, 30.96it/s]
2022-03-19 19:16:36,531 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/ag_base/best.th'.
2022-03-19 19:16:39,222 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-19 19:16:39,228 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.940  |     0.934
2022-03-19 19:16:39,230 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.940  |     0.934
2022-03-19 19:16:39,232 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-19 19:16:39,234 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.172  |     0.189
2022-03-19 19:16:39,235 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7572.609  |       N/A
2022-03-19 19:16:39,238 - INFO - allennlp.training.trainer - Epoch duration: 0:11:32.313753
2022-03-19 19:16:39,240 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:32:42
2022-03-19 19:16:39,242 - INFO - allennlp.training.trainer - Epoch 2/9
2022-03-19 19:16:39,243 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-19 19:16:39,245 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-19 19:16:39,248 - INFO - allennlp.training.trainer - Training
2022-03-19 19:16:39,250 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-19 19:16:49,283 - INFO - tqdm - f1: 0.9494, accuracy: 0.9504, batch_loss: 0.2689, loss: 0.1600 ||:   1%|1         | 97/7188 [00:10<26:17,  4.50it/s]
2022-03-19 19:16:59,459 - INFO - tqdm - f1: 0.9489, accuracy: 0.9493, batch_loss: 0.6100, loss: 0.1548 ||:   3%|2         | 207/7188 [00:20<25:08,  4.63it/s]
2022-03-19 19:17:09,664 - INFO - tqdm - f1: 0.9487, accuracy: 0.9490, batch_loss: 0.1832, loss: 0.1505 ||:   4%|4         | 315/7188 [00:30<25:41,  4.46it/s]
2022-03-19 19:17:19,925 - INFO - tqdm - f1: 0.9481, accuracy: 0.9487, batch_loss: 0.0125, loss: 0.1488 ||:   6%|5         | 425/7188 [00:40<25:05,  4.49it/s]
2022-03-19 19:17:30,388 - INFO - tqdm - f1: 0.9501, accuracy: 0.9505, batch_loss: 0.2077, loss: 0.1432 ||:   7%|7         | 537/7188 [00:51<24:34,  4.51it/s]
2022-03-19 19:17:40,511 - INFO - tqdm - f1: 0.9509, accuracy: 0.9513, batch_loss: 0.1905, loss: 0.1415 ||:   9%|8         | 643/7188 [01:01<23:46,  4.59it/s]
2022-03-19 19:17:50,705 - INFO - tqdm - f1: 0.9505, accuracy: 0.9507, batch_loss: 0.0106, loss: 0.1447 ||:  10%|#         | 751/7188 [01:11<23:06,  4.64it/s]
2022-03-19 19:18:00,867 - INFO - tqdm - f1: 0.9512, accuracy: 0.9515, batch_loss: 0.0684, loss: 0.1423 ||:  12%|#1        | 859/7188 [01:21<23:15,  4.53it/s]
2022-03-19 19:18:11,082 - INFO - tqdm - f1: 0.9520, accuracy: 0.9521, batch_loss: 0.0417, loss: 0.1414 ||:  13%|#3        | 967/7188 [01:31<23:16,  4.45it/s]
2022-03-19 19:18:21,498 - INFO - tqdm - f1: 0.9519, accuracy: 0.9520, batch_loss: 0.0179, loss: 0.1385 ||:  15%|#4        | 1077/7188 [01:42<22:45,  4.47it/s]
2022-03-19 19:18:31,602 - INFO - tqdm - f1: 0.9512, accuracy: 0.9512, batch_loss: 0.1354, loss: 0.1404 ||:  16%|#6        | 1181/7188 [01:52<18:09,  5.51it/s]
2022-03-19 19:18:41,685 - INFO - tqdm - f1: 0.9518, accuracy: 0.9518, batch_loss: 0.0369, loss: 0.1389 ||:  18%|#7        | 1285/7188 [02:02<21:54,  4.49it/s]
2022-03-19 19:18:51,754 - INFO - tqdm - f1: 0.9515, accuracy: 0.9515, batch_loss: 0.0492, loss: 0.1390 ||:  19%|#9        | 1389/7188 [02:12<17:40,  5.47it/s]
2022-03-19 19:19:01,783 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.1058, loss: 0.1377 ||:  21%|##        | 1491/7188 [02:22<21:18,  4.46it/s]
2022-03-19 19:19:11,919 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.2252, loss: 0.1369 ||:  22%|##2       | 1595/7188 [02:32<20:51,  4.47it/s]
2022-03-19 19:19:22,314 - INFO - tqdm - f1: 0.9523, accuracy: 0.9523, batch_loss: 0.1443, loss: 0.1375 ||:  24%|##3       | 1705/7188 [02:43<20:35,  4.44it/s]
2022-03-19 19:19:32,657 - INFO - tqdm - f1: 0.9523, accuracy: 0.9523, batch_loss: 0.0253, loss: 0.1367 ||:  25%|##5       | 1815/7188 [02:53<19:33,  4.58it/s]
2022-03-19 19:19:43,002 - INFO - tqdm - f1: 0.9519, accuracy: 0.9519, batch_loss: 0.0311, loss: 0.1375 ||:  27%|##6       | 1925/7188 [03:03<19:31,  4.49it/s]
2022-03-19 19:19:53,219 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.0061, loss: 0.1373 ||:  28%|##8       | 2033/7188 [03:13<18:34,  4.63it/s]
2022-03-19 19:20:03,522 - INFO - tqdm - f1: 0.9520, accuracy: 0.9519, batch_loss: 0.2307, loss: 0.1375 ||:  30%|##9       | 2145/7188 [03:24<18:51,  4.46it/s]
2022-03-19 19:20:13,673 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0975, loss: 0.1370 ||:  31%|###1      | 2251/7188 [03:34<18:42,  4.40it/s]
2022-03-19 19:20:23,894 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0654, loss: 0.1374 ||:  33%|###2      | 2359/7188 [03:44<17:31,  4.59it/s]
2022-03-19 19:20:34,212 - INFO - tqdm - f1: 0.9523, accuracy: 0.9523, batch_loss: 0.0667, loss: 0.1366 ||:  34%|###4      | 2467/7188 [03:54<18:00,  4.37it/s]
2022-03-19 19:20:44,290 - INFO - tqdm - f1: 0.9526, accuracy: 0.9526, batch_loss: 0.0078, loss: 0.1363 ||:  36%|###5      | 2571/7188 [04:05<13:52,  5.55it/s]
2022-03-19 19:20:54,433 - INFO - tqdm - f1: 0.9527, accuracy: 0.9527, batch_loss: 0.0080, loss: 0.1359 ||:  37%|###7      | 2675/7188 [04:15<16:50,  4.47it/s]
2022-03-19 19:21:04,531 - INFO - tqdm - f1: 0.9533, accuracy: 0.9533, batch_loss: 0.0560, loss: 0.1344 ||:  39%|###8      | 2785/7188 [04:25<12:42,  5.77it/s]
2022-03-19 19:21:14,559 - INFO - tqdm - f1: 0.9534, accuracy: 0.9535, batch_loss: 0.3501, loss: 0.1342 ||:  40%|####      | 2885/7188 [04:35<16:11,  4.43it/s]
2022-03-19 19:21:24,583 - INFO - tqdm - f1: 0.9536, accuracy: 0.9537, batch_loss: 0.1411, loss: 0.1337 ||:  42%|####1     | 2987/7188 [04:45<15:45,  4.44it/s]
2022-03-19 19:21:34,639 - INFO - tqdm - f1: 0.9535, accuracy: 0.9535, batch_loss: 0.2919, loss: 0.1348 ||:  43%|####3     | 3091/7188 [04:55<15:28,  4.41it/s]
2022-03-19 19:21:44,862 - INFO - tqdm - f1: 0.9534, accuracy: 0.9534, batch_loss: 0.0526, loss: 0.1350 ||:  45%|####4     | 3199/7188 [05:05<14:46,  4.50it/s]
2022-03-19 19:21:54,920 - INFO - tqdm - f1: 0.9535, accuracy: 0.9535, batch_loss: 0.4221, loss: 0.1350 ||:  46%|####5     | 3305/7188 [05:15<11:05,  5.83it/s]
2022-03-19 19:22:04,961 - INFO - tqdm - f1: 0.9533, accuracy: 0.9533, batch_loss: 0.4272, loss: 0.1358 ||:  47%|####7     | 3411/7188 [05:25<11:05,  5.68it/s]
2022-03-19 19:22:15,178 - INFO - tqdm - f1: 0.9535, accuracy: 0.9535, batch_loss: 0.0996, loss: 0.1352 ||:  49%|####8     | 3519/7188 [05:35<13:37,  4.49it/s]
2022-03-19 19:22:25,193 - INFO - tqdm - f1: 0.9536, accuracy: 0.9536, batch_loss: 0.0908, loss: 0.1348 ||:  50%|#####     | 3625/7188 [05:45<13:01,  4.56it/s]
2022-03-19 19:22:35,331 - INFO - tqdm - f1: 0.9535, accuracy: 0.9535, batch_loss: 0.0769, loss: 0.1353 ||:  52%|#####1    | 3733/7188 [05:56<12:43,  4.53it/s]
2022-03-19 19:22:45,352 - INFO - tqdm - f1: 0.9535, accuracy: 0.9534, batch_loss: 0.2426, loss: 0.1356 ||:  53%|#####3    | 3839/7188 [06:06<09:38,  5.79it/s]
2022-03-19 19:22:55,369 - INFO - tqdm - f1: 0.9535, accuracy: 0.9535, batch_loss: 0.0884, loss: 0.1354 ||:  55%|#####4    | 3945/7188 [06:16<11:47,  4.58it/s]
2022-03-19 19:23:05,571 - INFO - tqdm - f1: 0.9534, accuracy: 0.9534, batch_loss: 0.2647, loss: 0.1356 ||:  56%|#####6    | 4053/7188 [06:26<11:15,  4.64it/s]
2022-03-19 19:23:15,575 - INFO - tqdm - f1: 0.9534, accuracy: 0.9533, batch_loss: 0.0200, loss: 0.1359 ||:  58%|#####7    | 4159/7188 [06:36<11:13,  4.50it/s]
2022-03-19 19:23:25,803 - INFO - tqdm - f1: 0.9533, accuracy: 0.9532, batch_loss: 0.1051, loss: 0.1366 ||:  59%|#####9    | 4269/7188 [06:46<10:45,  4.52it/s]
2022-03-19 19:23:35,899 - INFO - tqdm - f1: 0.9533, accuracy: 0.9532, batch_loss: 0.1948, loss: 0.1365 ||:  61%|######    | 4379/7188 [06:56<10:04,  4.65it/s]
2022-03-19 19:23:45,991 - INFO - tqdm - f1: 0.9533, accuracy: 0.9532, batch_loss: 0.2584, loss: 0.1368 ||:  62%|######2   | 4485/7188 [07:06<10:14,  4.40it/s]
2022-03-19 19:23:56,216 - INFO - tqdm - f1: 0.9535, accuracy: 0.9534, batch_loss: 0.1047, loss: 0.1361 ||:  64%|######3   | 4593/7188 [07:16<09:31,  4.54it/s]
2022-03-19 19:24:06,358 - INFO - tqdm - f1: 0.9535, accuracy: 0.9535, batch_loss: 0.4382, loss: 0.1360 ||:  65%|######5   | 4699/7188 [07:27<09:19,  4.45it/s]
2022-03-19 19:24:16,478 - INFO - tqdm - f1: 0.9534, accuracy: 0.9533, batch_loss: 0.5211, loss: 0.1364 ||:  67%|######6   | 4805/7188 [07:37<07:03,  5.63it/s]
2022-03-19 19:24:26,663 - INFO - tqdm - f1: 0.9535, accuracy: 0.9534, batch_loss: 0.0194, loss: 0.1364 ||:  68%|######8   | 4911/7188 [07:47<08:29,  4.47it/s]
2022-03-19 19:24:36,828 - INFO - tqdm - f1: 0.9535, accuracy: 0.9534, batch_loss: 0.4748, loss: 0.1361 ||:  70%|######9   | 5017/7188 [07:57<07:18,  4.95it/s]
2022-03-19 19:24:46,925 - INFO - tqdm - f1: 0.9534, accuracy: 0.9533, batch_loss: 0.0106, loss: 0.1361 ||:  71%|#######1  | 5125/7188 [08:07<05:59,  5.73it/s]
2022-03-19 19:24:57,132 - INFO - tqdm - f1: 0.9533, accuracy: 0.9532, batch_loss: 0.0223, loss: 0.1363 ||:  73%|#######2  | 5233/7188 [08:17<07:16,  4.48it/s]
2022-03-19 19:25:07,410 - INFO - tqdm - f1: 0.9532, accuracy: 0.9532, batch_loss: 0.0162, loss: 0.1366 ||:  74%|#######4  | 5343/7188 [08:28<06:48,  4.52it/s]
2022-03-19 19:25:17,456 - INFO - tqdm - f1: 0.9532, accuracy: 0.9532, batch_loss: 0.0709, loss: 0.1365 ||:  76%|#######5  | 5449/7188 [08:38<04:57,  5.85it/s]
2022-03-19 19:25:27,539 - INFO - tqdm - f1: 0.9533, accuracy: 0.9532, batch_loss: 0.0702, loss: 0.1364 ||:  77%|#######7  | 5559/7188 [08:48<05:45,  4.71it/s]
2022-03-19 19:25:37,673 - INFO - tqdm - f1: 0.9533, accuracy: 0.9533, batch_loss: 0.3674, loss: 0.1365 ||:  79%|#######8  | 5669/7188 [08:58<05:30,  4.60it/s]
2022-03-19 19:25:47,870 - INFO - tqdm - f1: 0.9534, accuracy: 0.9534, batch_loss: 0.0315, loss: 0.1364 ||:  80%|########  | 5779/7188 [09:08<05:02,  4.66it/s]
2022-03-19 19:25:57,973 - INFO - tqdm - f1: 0.9532, accuracy: 0.9532, batch_loss: 0.0599, loss: 0.1368 ||:  82%|########1 | 5887/7188 [09:18<04:48,  4.51it/s]
2022-03-19 19:26:08,316 - INFO - tqdm - f1: 0.9531, accuracy: 0.9530, batch_loss: 0.0082, loss: 0.1370 ||:  83%|########3 | 5997/7188 [09:29<04:28,  4.43it/s]
2022-03-19 19:26:18,334 - INFO - tqdm - f1: 0.9530, accuracy: 0.9530, batch_loss: 0.2620, loss: 0.1375 ||:  85%|########4 | 6101/7188 [09:39<04:02,  4.49it/s]
2022-03-19 19:26:28,685 - INFO - tqdm - f1: 0.9528, accuracy: 0.9528, batch_loss: 0.1008, loss: 0.1380 ||:  86%|########6 | 6211/7188 [09:49<03:39,  4.45it/s]
2022-03-19 19:26:38,883 - INFO - tqdm - f1: 0.9528, accuracy: 0.9528, batch_loss: 0.0728, loss: 0.1379 ||:  88%|########7 | 6321/7188 [09:59<03:10,  4.55it/s]
2022-03-19 19:26:48,997 - INFO - tqdm - f1: 0.9528, accuracy: 0.9528, batch_loss: 0.0602, loss: 0.1379 ||:  89%|########9 | 6429/7188 [10:09<02:47,  4.52it/s]
2022-03-19 19:26:59,327 - INFO - tqdm - f1: 0.9528, accuracy: 0.9528, batch_loss: 0.0769, loss: 0.1378 ||:  91%|######### | 6539/7188 [10:20<02:23,  4.51it/s]
2022-03-19 19:27:09,490 - INFO - tqdm - f1: 0.9527, accuracy: 0.9527, batch_loss: 0.0952, loss: 0.1378 ||:  92%|#########2| 6645/7188 [10:30<02:01,  4.47it/s]
2022-03-19 19:27:19,783 - INFO - tqdm - f1: 0.9527, accuracy: 0.9527, batch_loss: 0.0252, loss: 0.1375 ||:  94%|#########4| 6757/7188 [10:40<01:36,  4.46it/s]
2022-03-19 19:27:30,082 - INFO - tqdm - f1: 0.9527, accuracy: 0.9527, batch_loss: 0.0473, loss: 0.1376 ||:  96%|#########5| 6869/7188 [10:50<01:09,  4.56it/s]
2022-03-19 19:27:40,515 - INFO - tqdm - f1: 0.9526, accuracy: 0.9526, batch_loss: 0.0731, loss: 0.1378 ||:  97%|#########7| 6981/7188 [11:01<00:45,  4.57it/s]
2022-03-19 19:27:50,814 - INFO - tqdm - f1: 0.9527, accuracy: 0.9526, batch_loss: 0.0562, loss: 0.1380 ||:  99%|#########8| 7093/7188 [11:11<00:17,  5.58it/s]
2022-03-19 19:27:55,808 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.0237, loss: 0.1384 ||: 100%|#########9| 7153/7188 [11:16<00:02, 14.51it/s]
2022-03-19 19:27:55,954 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.5229, loss: 0.1384 ||: 100%|#########9| 7155/7188 [11:16<00:02, 14.25it/s]
2022-03-19 19:27:56,080 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.2317, loss: 0.1385 ||: 100%|#########9| 7157/7188 [11:16<00:02, 14.70it/s]
2022-03-19 19:27:56,213 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.1745, loss: 0.1385 ||: 100%|#########9| 7159/7188 [11:16<00:01, 14.79it/s]
2022-03-19 19:27:57,414 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.1104, loss: 0.1385 ||: 100%|#########9| 7161/7188 [11:18<00:06,  4.40it/s]
2022-03-19 19:27:57,558 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.1351, loss: 0.1385 ||: 100%|#########9| 7163/7188 [11:18<00:04,  5.53it/s]
2022-03-19 19:27:57,695 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.0140, loss: 0.1385 ||: 100%|#########9| 7165/7188 [11:18<00:03,  6.80it/s]
2022-03-19 19:27:57,836 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.0383, loss: 0.1385 ||: 100%|#########9| 7167/7188 [11:18<00:02,  8.06it/s]
2022-03-19 19:27:57,965 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.4927, loss: 0.1385 ||: 100%|#########9| 7169/7188 [11:18<00:02,  9.41it/s]
2022-03-19 19:27:58,099 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.0092, loss: 0.1385 ||: 100%|#########9| 7171/7188 [11:18<00:01, 10.59it/s]
2022-03-19 19:27:58,237 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.0435, loss: 0.1385 ||: 100%|#########9| 7173/7188 [11:18<00:01, 11.51it/s]
2022-03-19 19:27:58,360 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.0360, loss: 0.1385 ||: 100%|#########9| 7175/7188 [11:19<00:01, 12.63it/s]
2022-03-19 19:27:58,492 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.1277, loss: 0.1385 ||: 100%|#########9| 7177/7188 [11:19<00:00, 13.29it/s]
2022-03-19 19:27:58,641 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.2800, loss: 0.1385 ||: 100%|#########9| 7179/7188 [11:19<00:00, 13.32it/s]
2022-03-19 19:27:58,790 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.2796, loss: 0.1385 ||: 100%|#########9| 7181/7188 [11:19<00:00, 13.35it/s]
2022-03-19 19:27:58,944 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.0977, loss: 0.1385 ||: 100%|#########9| 7183/7188 [11:19<00:00, 13.24it/s]
2022-03-19 19:27:59,090 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.0100, loss: 0.1385 ||: 100%|#########9| 7185/7188 [11:19<00:00, 13.38it/s]
2022-03-19 19:27:59,217 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.0843, loss: 0.1385 ||: 100%|#########9| 7187/7188 [11:19<00:00, 14.00it/s]
2022-03-19 19:27:59,323 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.0645, loss: 0.1385 ||: 100%|##########| 7188/7188 [11:20<00:00, 10.57it/s]
2022-03-19 19:27:59,330 - INFO - allennlp.training.trainer - Validating
2022-03-19 19:27:59,333 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-19 19:28:09,345 - INFO - tqdm - f1: 0.9373, accuracy: 0.9372, batch_loss: 0.2898, loss: 0.1915 ||: 100%|##########| 313/313 [00:10<00:00, 31.27it/s]
2022-03-19 19:28:09,363 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/ag_base/best.th'.
2022-03-19 19:28:11,863 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-19 19:28:11,869 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.952  |     0.937
2022-03-19 19:28:11,871 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.953  |     0.937
2022-03-19 19:28:11,873 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-19 19:28:11,875 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.139  |     0.191
2022-03-19 19:28:11,877 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7573.312  |       N/A
2022-03-19 19:28:11,879 - INFO - allennlp.training.trainer - Epoch duration: 0:11:32.637249
2022-03-19 19:28:11,880 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:21:00
2022-03-19 19:28:11,882 - INFO - allennlp.training.trainer - Epoch 3/9
2022-03-19 19:28:11,884 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-19 19:28:11,886 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-19 19:28:11,889 - INFO - allennlp.training.trainer - Training
2022-03-19 19:28:11,891 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-19 19:28:21,946 - INFO - tqdm - f1: 0.9737, accuracy: 0.9736, batch_loss: 0.0523, loss: 0.0965 ||:   1%|          | 71/7188 [00:10<11:08, 10.65it/s]
2022-03-19 19:28:31,955 - INFO - tqdm - f1: 0.9671, accuracy: 0.9672, batch_loss: 0.0756, loss: 0.1045 ||:   2%|2         | 177/7188 [00:20<10:37, 11.00it/s]
2022-03-19 19:28:42,046 - INFO - tqdm - f1: 0.9632, accuracy: 0.9634, batch_loss: 0.0632, loss: 0.1125 ||:   4%|3         | 285/7188 [00:30<09:24, 12.23it/s]
2022-03-19 19:28:52,169 - INFO - tqdm - f1: 0.9607, accuracy: 0.9607, batch_loss: 0.0214, loss: 0.1160 ||:   5%|5         | 391/7188 [00:40<08:47, 12.89it/s]
2022-03-19 19:29:02,281 - INFO - tqdm - f1: 0.9611, accuracy: 0.9610, batch_loss: 0.0311, loss: 0.1118 ||:   7%|6         | 497/7188 [00:50<10:16, 10.86it/s]
2022-03-19 19:29:12,373 - INFO - tqdm - f1: 0.9616, accuracy: 0.9615, batch_loss: 0.0407, loss: 0.1090 ||:   8%|8         | 603/7188 [01:00<10:05, 10.87it/s]
2022-03-19 19:29:22,380 - INFO - tqdm - f1: 0.9619, accuracy: 0.9619, batch_loss: 0.0423, loss: 0.1086 ||:  10%|9         | 709/7188 [01:10<11:10,  9.67it/s]
2022-03-19 19:29:32,459 - INFO - tqdm - f1: 0.9629, accuracy: 0.9631, batch_loss: 0.0277, loss: 0.1064 ||:  11%|#1        | 817/7188 [01:20<10:38,  9.97it/s]
2022-03-19 19:29:42,582 - INFO - tqdm - f1: 0.9627, accuracy: 0.9628, batch_loss: 0.0647, loss: 0.1066 ||:  13%|#2        | 925/7188 [01:30<10:45,  9.70it/s]
2022-03-19 19:29:52,710 - INFO - tqdm - f1: 0.9629, accuracy: 0.9631, batch_loss: 0.0477, loss: 0.1062 ||:  14%|#4        | 1033/7188 [01:40<10:24,  9.86it/s]
2022-03-19 19:30:02,783 - INFO - tqdm - f1: 0.9629, accuracy: 0.9631, batch_loss: 0.0919, loss: 0.1073 ||:  16%|#5        | 1141/7188 [01:50<10:08,  9.94it/s]
2022-03-19 19:30:12,904 - INFO - tqdm - f1: 0.9626, accuracy: 0.9629, batch_loss: 0.0133, loss: 0.1071 ||:  17%|#7        | 1249/7188 [02:01<11:56,  8.29it/s]
2022-03-19 19:30:22,966 - INFO - tqdm - f1: 0.9624, accuracy: 0.9626, batch_loss: 0.0607, loss: 0.1082 ||:  19%|#8        | 1355/7188 [02:11<13:47,  7.05it/s]
2022-03-19 19:30:33,025 - INFO - tqdm - f1: 0.9622, accuracy: 0.9624, batch_loss: 0.0429, loss: 0.1081 ||:  20%|##        | 1461/7188 [02:21<21:36,  4.42it/s]
2022-03-19 19:30:43,121 - INFO - tqdm - f1: 0.9625, accuracy: 0.9627, batch_loss: 0.4242, loss: 0.1070 ||:  22%|##1       | 1569/7188 [02:31<20:34,  4.55it/s]
2022-03-19 19:30:53,185 - INFO - tqdm - f1: 0.9616, accuracy: 0.9618, batch_loss: 0.2127, loss: 0.1085 ||:  23%|##3       | 1675/7188 [02:41<20:23,  4.50it/s]
2022-03-19 19:31:03,214 - INFO - tqdm - f1: 0.9616, accuracy: 0.9617, batch_loss: 0.0298, loss: 0.1091 ||:  25%|##4       | 1781/7188 [02:51<19:32,  4.61it/s]
2022-03-19 19:31:13,252 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0054, loss: 0.1088 ||:  26%|##6       | 1885/7188 [03:01<15:43,  5.62it/s]
2022-03-19 19:31:23,262 - INFO - tqdm - f1: 0.9621, accuracy: 0.9622, batch_loss: 0.0380, loss: 0.1086 ||:  28%|##7       | 1991/7188 [03:11<18:57,  4.57it/s]
2022-03-19 19:31:33,275 - INFO - tqdm - f1: 0.9625, accuracy: 0.9626, batch_loss: 0.0034, loss: 0.1080 ||:  29%|##9       | 2099/7188 [03:21<18:44,  4.52it/s]
2022-03-19 19:31:43,399 - INFO - tqdm - f1: 0.9625, accuracy: 0.9626, batch_loss: 0.0809, loss: 0.1078 ||:  31%|###       | 2205/7188 [03:31<14:49,  5.60it/s]
2022-03-19 19:31:53,464 - INFO - tqdm - f1: 0.9625, accuracy: 0.9626, batch_loss: 0.0567, loss: 0.1080 ||:  32%|###2      | 2311/7188 [03:41<14:16,  5.70it/s]
2022-03-19 19:32:03,719 - INFO - tqdm - f1: 0.9625, accuracy: 0.9626, batch_loss: 0.2811, loss: 0.1087 ||:  34%|###3      | 2419/7188 [03:51<17:41,  4.49it/s]
2022-03-19 19:32:13,767 - INFO - tqdm - f1: 0.9620, accuracy: 0.9621, batch_loss: 0.2387, loss: 0.1102 ||:  35%|###5      | 2525/7188 [04:01<13:17,  5.84it/s]
2022-03-19 19:32:23,871 - INFO - tqdm - f1: 0.9621, accuracy: 0.9622, batch_loss: 0.0363, loss: 0.1096 ||:  37%|###6      | 2631/7188 [04:11<13:12,  5.75it/s]
2022-03-19 19:32:33,941 - INFO - tqdm - f1: 0.9620, accuracy: 0.9620, batch_loss: 0.1809, loss: 0.1100 ||:  38%|###8      | 2737/7188 [04:22<10:33,  7.02it/s]
2022-03-19 19:32:44,077 - INFO - tqdm - f1: 0.9616, accuracy: 0.9617, batch_loss: 0.0252, loss: 0.1107 ||:  40%|###9      | 2841/7188 [04:32<16:10,  4.48it/s]
2022-03-19 19:32:54,278 - INFO - tqdm - f1: 0.9617, accuracy: 0.9617, batch_loss: 0.1897, loss: 0.1105 ||:  41%|####      | 2947/7188 [04:42<15:47,  4.47it/s]
2022-03-19 19:33:04,515 - INFO - tqdm - f1: 0.9616, accuracy: 0.9617, batch_loss: 0.4852, loss: 0.1112 ||:  42%|####2     | 3053/7188 [04:52<15:06,  4.56it/s]
2022-03-19 19:33:14,810 - INFO - tqdm - f1: 0.9617, accuracy: 0.9617, batch_loss: 0.0103, loss: 0.1109 ||:  44%|####3     | 3161/7188 [05:02<14:53,  4.51it/s]
2022-03-19 19:33:24,885 - INFO - tqdm - f1: 0.9621, accuracy: 0.9622, batch_loss: 0.0207, loss: 0.1100 ||:  45%|####5     | 3265/7188 [05:12<11:46,  5.55it/s]
2022-03-19 19:33:34,933 - INFO - tqdm - f1: 0.9621, accuracy: 0.9622, batch_loss: 0.0343, loss: 0.1102 ||:  47%|####6     | 3367/7188 [05:23<14:22,  4.43it/s]
2022-03-19 19:33:45,067 - INFO - tqdm - f1: 0.9620, accuracy: 0.9620, batch_loss: 0.0113, loss: 0.1108 ||:  48%|####8     | 3473/7188 [05:33<14:02,  4.41it/s]
2022-03-19 19:33:55,149 - INFO - tqdm - f1: 0.9622, accuracy: 0.9622, batch_loss: 0.0427, loss: 0.1102 ||:  50%|####9     | 3579/7188 [05:43<13:13,  4.55it/s]
2022-03-19 19:34:05,294 - INFO - tqdm - f1: 0.9623, accuracy: 0.9623, batch_loss: 0.3288, loss: 0.1101 ||:  51%|#####1    | 3683/7188 [05:53<10:20,  5.65it/s]
2022-03-19 19:34:15,417 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.1055, loss: 0.1103 ||:  53%|#####2    | 3785/7188 [06:03<13:01,  4.36it/s]
2022-03-19 19:34:25,523 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0111, loss: 0.1102 ||:  54%|#####4    | 3891/7188 [06:13<09:48,  5.60it/s]
2022-03-19 19:34:35,533 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0144, loss: 0.1102 ||:  56%|#####5    | 3995/7188 [06:23<12:01,  4.43it/s]
2022-03-19 19:34:45,573 - INFO - tqdm - f1: 0.9620, accuracy: 0.9621, batch_loss: 0.1497, loss: 0.1100 ||:  57%|#####7    | 4101/7188 [06:33<11:21,  4.53it/s]
2022-03-19 19:34:55,714 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0378, loss: 0.1101 ||:  59%|#####8    | 4211/7188 [06:43<10:52,  4.56it/s]
2022-03-19 19:35:05,763 - INFO - tqdm - f1: 0.9619, accuracy: 0.9620, batch_loss: 0.0610, loss: 0.1103 ||:  60%|######    | 4317/7188 [06:53<10:28,  4.57it/s]
2022-03-19 19:35:15,823 - INFO - tqdm - f1: 0.9619, accuracy: 0.9620, batch_loss: 0.1515, loss: 0.1104 ||:  62%|######1   | 4421/7188 [07:03<08:03,  5.72it/s]
2022-03-19 19:35:25,943 - INFO - tqdm - f1: 0.9617, accuracy: 0.9618, batch_loss: 0.1170, loss: 0.1108 ||:  63%|######2   | 4527/7188 [07:14<07:43,  5.74it/s]
2022-03-19 19:35:36,048 - INFO - tqdm - f1: 0.9618, accuracy: 0.9618, batch_loss: 0.0136, loss: 0.1105 ||:  64%|######4   | 4633/7188 [07:24<04:31,  9.41it/s]
2022-03-19 19:35:46,129 - INFO - tqdm - f1: 0.9618, accuracy: 0.9618, batch_loss: 0.0113, loss: 0.1105 ||:  66%|######5   | 4739/7188 [07:34<03:45, 10.84it/s]
2022-03-19 19:35:56,242 - INFO - tqdm - f1: 0.9620, accuracy: 0.9620, batch_loss: 0.2787, loss: 0.1104 ||:  67%|######7   | 4847/7188 [07:44<03:00, 12.96it/s]
2022-03-19 19:36:06,325 - INFO - tqdm - f1: 0.9620, accuracy: 0.9620, batch_loss: 0.0604, loss: 0.1104 ||:  69%|######8   | 4955/7188 [07:54<02:43, 13.67it/s]
2022-03-19 19:36:16,325 - INFO - tqdm - f1: 0.9620, accuracy: 0.9620, batch_loss: 0.0215, loss: 0.1107 ||:  70%|#######   | 5061/7188 [08:04<02:30, 14.10it/s]
2022-03-19 19:36:26,339 - INFO - tqdm - f1: 0.9620, accuracy: 0.9620, batch_loss: 0.1043, loss: 0.1106 ||:  72%|#######1  | 5167/7188 [08:14<02:22, 14.22it/s]
2022-03-19 19:36:36,445 - INFO - tqdm - f1: 0.9620, accuracy: 0.9619, batch_loss: 0.4116, loss: 0.1108 ||:  73%|#######3  | 5273/7188 [08:24<02:14, 14.24it/s]
2022-03-19 19:36:46,525 - INFO - tqdm - f1: 0.9620, accuracy: 0.9620, batch_loss: 0.1011, loss: 0.1107 ||:  75%|#######4  | 5381/7188 [08:34<02:06, 14.25it/s]
2022-03-19 19:36:56,552 - INFO - tqdm - f1: 0.9620, accuracy: 0.9620, batch_loss: 0.0654, loss: 0.1108 ||:  76%|#######6  | 5485/7188 [08:44<02:01, 13.99it/s]
2022-03-19 19:37:06,578 - INFO - tqdm - f1: 0.9620, accuracy: 0.9620, batch_loss: 0.1266, loss: 0.1109 ||:  78%|#######7  | 5591/7188 [08:54<01:52, 14.18it/s]
2022-03-19 19:37:16,669 - INFO - tqdm - f1: 0.9620, accuracy: 0.9620, batch_loss: 0.0286, loss: 0.1111 ||:  79%|#######9  | 5697/7188 [09:04<01:39, 15.04it/s]
2022-03-19 19:37:26,730 - INFO - tqdm - f1: 0.9620, accuracy: 0.9620, batch_loss: 0.0908, loss: 0.1111 ||:  81%|########  | 5803/7188 [09:14<01:38, 14.01it/s]
2022-03-19 19:37:36,833 - INFO - tqdm - f1: 0.9619, accuracy: 0.9619, batch_loss: 0.0039, loss: 0.1112 ||:  82%|########2 | 5911/7188 [09:24<01:31, 13.96it/s]
2022-03-19 19:37:46,835 - INFO - tqdm - f1: 0.9618, accuracy: 0.9617, batch_loss: 0.4314, loss: 0.1113 ||:  84%|########3 | 6015/7188 [09:34<01:32, 12.65it/s]
2022-03-19 19:37:56,928 - INFO - tqdm - f1: 0.9618, accuracy: 0.9617, batch_loss: 0.0144, loss: 0.1113 ||:  85%|########5 | 6121/7188 [09:45<01:28, 12.04it/s]
2022-03-19 19:38:07,003 - INFO - tqdm - f1: 0.9617, accuracy: 0.9617, batch_loss: 0.0140, loss: 0.1114 ||:  87%|########6 | 6225/7188 [09:55<01:30, 10.70it/s]
2022-03-19 19:38:17,038 - INFO - tqdm - f1: 0.9616, accuracy: 0.9616, batch_loss: 0.0470, loss: 0.1117 ||:  88%|########8 | 6329/7188 [10:05<01:11, 11.99it/s]
2022-03-19 19:38:27,098 - INFO - tqdm - f1: 0.9616, accuracy: 0.9616, batch_loss: 0.1209, loss: 0.1117 ||:  90%|########9 | 6435/7188 [10:15<01:08, 10.94it/s]
2022-03-19 19:38:37,183 - INFO - tqdm - f1: 0.9615, accuracy: 0.9614, batch_loss: 0.1020, loss: 0.1121 ||:  91%|#########1| 6543/7188 [10:25<00:58, 11.00it/s]
2022-03-19 19:38:47,207 - INFO - tqdm - f1: 0.9613, accuracy: 0.9612, batch_loss: 0.0889, loss: 0.1125 ||:  93%|#########2| 6649/7188 [10:35<00:39, 13.74it/s]
2022-03-19 19:38:57,288 - INFO - tqdm - f1: 0.9612, accuracy: 0.9611, batch_loss: 0.0802, loss: 0.1128 ||:  94%|#########4| 6757/7188 [10:45<00:29, 14.61it/s]
2022-03-19 19:39:07,291 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.1676, loss: 0.1129 ||:  96%|#########5| 6865/7188 [10:55<00:22, 14.41it/s]
2022-03-19 19:39:17,338 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0043, loss: 0.1127 ||:  97%|#########7| 6973/7188 [11:05<00:15, 14.25it/s]
2022-03-19 19:39:27,408 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0681, loss: 0.1129 ||:  98%|#########8| 7077/7188 [11:15<00:07, 14.37it/s]
2022-03-19 19:39:34,530 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0087, loss: 0.1129 ||: 100%|#########9| 7153/7188 [11:22<00:02, 14.06it/s]
2022-03-19 19:39:34,677 - INFO - tqdm - f1: 0.9611, accuracy: 0.9610, batch_loss: 0.2020, loss: 0.1130 ||: 100%|#########9| 7155/7188 [11:22<00:02, 13.92it/s]
2022-03-19 19:39:34,808 - INFO - tqdm - f1: 0.9611, accuracy: 0.9610, batch_loss: 0.1515, loss: 0.1130 ||: 100%|#########9| 7157/7188 [11:22<00:02, 14.28it/s]
2022-03-19 19:39:34,945 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0158, loss: 0.1130 ||: 100%|#########9| 7159/7188 [11:23<00:02, 14.39it/s]
2022-03-19 19:39:35,079 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0639, loss: 0.1129 ||: 100%|#########9| 7161/7188 [11:23<00:01, 14.52it/s]
2022-03-19 19:39:36,265 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0064, loss: 0.1129 ||: 100%|#########9| 7163/7188 [11:24<00:05,  4.42it/s]
2022-03-19 19:39:36,410 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0166, loss: 0.1129 ||: 100%|#########9| 7165/7188 [11:24<00:04,  5.56it/s]
2022-03-19 19:39:36,551 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0544, loss: 0.1129 ||: 100%|#########9| 7167/7188 [11:24<00:03,  6.80it/s]
2022-03-19 19:39:36,692 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0435, loss: 0.1129 ||: 100%|#########9| 7169/7188 [11:24<00:02,  8.05it/s]
2022-03-19 19:39:36,830 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0045, loss: 0.1129 ||: 100%|#########9| 7171/7188 [11:24<00:01,  9.29it/s]
2022-03-19 19:39:36,964 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0712, loss: 0.1128 ||: 100%|#########9| 7173/7188 [11:25<00:01, 10.48it/s]
2022-03-19 19:39:37,098 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.1396, loss: 0.1128 ||: 100%|#########9| 7175/7188 [11:25<00:01, 11.52it/s]
2022-03-19 19:39:37,229 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.4468, loss: 0.1129 ||: 100%|#########9| 7177/7188 [11:25<00:00, 12.43it/s]
2022-03-19 19:39:37,370 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.3121, loss: 0.1130 ||: 100%|#########9| 7179/7188 [11:25<00:00, 12.91it/s]
2022-03-19 19:39:37,517 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.3930, loss: 0.1130 ||: 100%|#########9| 7181/7188 [11:25<00:00, 13.11it/s]
2022-03-19 19:39:37,660 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0411, loss: 0.1130 ||: 100%|#########9| 7183/7188 [11:25<00:00, 13.37it/s]
2022-03-19 19:39:37,815 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.1240, loss: 0.1130 ||: 100%|#########9| 7185/7188 [11:25<00:00, 13.21it/s]
2022-03-19 19:39:37,958 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0101, loss: 0.1130 ||: 100%|#########9| 7187/7188 [11:26<00:00, 13.45it/s]
2022-03-19 19:39:38,062 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0089, loss: 0.1130 ||: 100%|##########| 7188/7188 [11:26<00:00, 10.48it/s]
2022-03-19 19:39:38,070 - INFO - allennlp.training.trainer - Validating
2022-03-19 19:39:38,073 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-19 19:39:47,939 - INFO - tqdm - f1: 0.9408, accuracy: 0.9408, batch_loss: 0.0028, loss: 0.2068 ||: 100%|##########| 313/313 [00:09<00:00, 31.73it/s]
2022-03-19 19:39:47,958 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/ag_base/best.th'.
2022-03-19 19:39:50,792 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-19 19:39:50,795 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.961  |     0.941
2022-03-19 19:39:50,797 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.961  |     0.941
2022-03-19 19:39:50,802 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-19 19:39:50,804 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.113  |     0.207
2022-03-19 19:39:50,805 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7573.359  |       N/A
2022-03-19 19:39:50,807 - INFO - allennlp.training.trainer - Epoch duration: 0:11:38.925223
2022-03-19 19:39:50,809 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:09:33
2022-03-19 19:39:50,811 - INFO - allennlp.training.trainer - Epoch 4/9
2022-03-19 19:39:50,813 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-19 19:39:50,815 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-19 19:39:50,818 - INFO - allennlp.training.trainer - Training
2022-03-19 19:39:50,820 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-19 19:40:00,916 - INFO - tqdm - f1: 0.9705, accuracy: 0.9704, batch_loss: 0.0099, loss: 0.0844 ||:   1%|1         | 95/7188 [00:10<09:21, 12.63it/s]
2022-03-19 19:40:10,945 - INFO - tqdm - f1: 0.9664, accuracy: 0.9664, batch_loss: 0.1040, loss: 0.0880 ||:   3%|2         | 199/7188 [00:20<08:59, 12.97it/s]
2022-03-19 19:40:21,063 - INFO - tqdm - f1: 0.9705, accuracy: 0.9705, batch_loss: 0.0123, loss: 0.0840 ||:   4%|4         | 305/7188 [00:30<08:48, 13.03it/s]
2022-03-19 19:40:31,078 - INFO - tqdm - f1: 0.9702, accuracy: 0.9704, batch_loss: 0.0676, loss: 0.0846 ||:   6%|5         | 409/7188 [00:40<09:33, 11.82it/s]
2022-03-19 19:40:41,117 - INFO - tqdm - f1: 0.9711, accuracy: 0.9711, batch_loss: 0.1429, loss: 0.0817 ||:   7%|7         | 511/7188 [00:50<09:42, 11.46it/s]
2022-03-19 19:40:51,212 - INFO - tqdm - f1: 0.9720, accuracy: 0.9723, batch_loss: 0.0023, loss: 0.0815 ||:   9%|8         | 615/7188 [01:00<08:41, 12.61it/s]
2022-03-19 19:41:01,300 - INFO - tqdm - f1: 0.9714, accuracy: 0.9716, batch_loss: 0.0695, loss: 0.0832 ||:  10%|#         | 721/7188 [01:10<07:50, 13.75it/s]
2022-03-19 19:41:11,372 - INFO - tqdm - f1: 0.9718, accuracy: 0.9719, batch_loss: 0.0808, loss: 0.0836 ||:  12%|#1        | 827/7188 [01:20<07:42, 13.75it/s]
2022-03-19 19:41:21,493 - INFO - tqdm - f1: 0.9719, accuracy: 0.9720, batch_loss: 0.0257, loss: 0.0828 ||:  13%|#3        | 935/7188 [01:30<07:33, 13.78it/s]
2022-03-19 19:41:31,584 - INFO - tqdm - f1: 0.9712, accuracy: 0.9714, batch_loss: 0.0025, loss: 0.0838 ||:  15%|#4        | 1043/7188 [01:40<07:17, 14.04it/s]
2022-03-19 19:41:41,681 - INFO - tqdm - f1: 0.9705, accuracy: 0.9707, batch_loss: 0.0860, loss: 0.0856 ||:  16%|#5        | 1149/7188 [01:50<07:06, 14.15it/s]
2022-03-19 19:41:51,765 - INFO - tqdm - f1: 0.9701, accuracy: 0.9702, batch_loss: 0.0195, loss: 0.0863 ||:  17%|#7        | 1257/7188 [02:00<06:53, 14.34it/s]
2022-03-19 19:42:01,814 - INFO - tqdm - f1: 0.9701, accuracy: 0.9702, batch_loss: 0.0215, loss: 0.0871 ||:  19%|#8        | 1365/7188 [02:10<06:42, 14.45it/s]
2022-03-19 19:42:11,856 - INFO - tqdm - f1: 0.9698, accuracy: 0.9699, batch_loss: 0.0128, loss: 0.0870 ||:  20%|##        | 1473/7188 [02:21<06:54, 13.80it/s]
2022-03-19 19:42:21,938 - INFO - tqdm - f1: 0.9699, accuracy: 0.9700, batch_loss: 0.0329, loss: 0.0864 ||:  22%|##1       | 1581/7188 [02:31<06:33, 14.23it/s]
2022-03-19 19:42:32,009 - INFO - tqdm - f1: 0.9701, accuracy: 0.9702, batch_loss: 0.0626, loss: 0.0860 ||:  23%|##3       | 1689/7188 [02:41<06:50, 13.39it/s]
2022-03-19 19:42:42,084 - INFO - tqdm - f1: 0.9704, accuracy: 0.9705, batch_loss: 0.0649, loss: 0.0857 ||:  25%|##4       | 1793/7188 [02:51<07:00, 12.82it/s]
2022-03-19 19:42:52,126 - INFO - tqdm - f1: 0.9702, accuracy: 0.9703, batch_loss: 0.1896, loss: 0.0858 ||:  26%|##6       | 1899/7188 [03:01<06:37, 13.31it/s]
2022-03-19 19:43:02,166 - INFO - tqdm - f1: 0.9704, accuracy: 0.9705, batch_loss: 0.0750, loss: 0.0857 ||:  28%|##7       | 2005/7188 [03:11<06:25, 13.44it/s]
2022-03-19 19:43:12,185 - INFO - tqdm - f1: 0.9702, accuracy: 0.9703, batch_loss: 0.0029, loss: 0.0865 ||:  29%|##9       | 2113/7188 [03:21<07:32, 11.22it/s]
2022-03-19 19:43:22,245 - INFO - tqdm - f1: 0.9701, accuracy: 0.9703, batch_loss: 0.0819, loss: 0.0865 ||:  31%|###       | 2219/7188 [03:31<08:37,  9.60it/s]
2022-03-19 19:43:32,358 - INFO - tqdm - f1: 0.9701, accuracy: 0.9701, batch_loss: 0.0154, loss: 0.0867 ||:  32%|###2      | 2327/7188 [03:41<07:35, 10.67it/s]
2022-03-19 19:43:42,472 - INFO - tqdm - f1: 0.9701, accuracy: 0.9701, batch_loss: 0.0486, loss: 0.0870 ||:  34%|###3      | 2435/7188 [03:51<06:58, 11.36it/s]
2022-03-19 19:43:52,537 - INFO - tqdm - f1: 0.9699, accuracy: 0.9699, batch_loss: 0.0934, loss: 0.0876 ||:  35%|###5      | 2543/7188 [04:01<07:55,  9.76it/s]
2022-03-19 19:44:02,614 - INFO - tqdm - f1: 0.9698, accuracy: 0.9698, batch_loss: 0.2478, loss: 0.0883 ||:  37%|###6      | 2647/7188 [04:11<07:16, 10.41it/s]
2022-03-19 19:44:12,672 - INFO - tqdm - f1: 0.9697, accuracy: 0.9698, batch_loss: 0.1821, loss: 0.0883 ||:  38%|###8      | 2755/7188 [04:21<06:11, 11.94it/s]
2022-03-19 19:44:22,788 - INFO - tqdm - f1: 0.9699, accuracy: 0.9700, batch_loss: 0.0257, loss: 0.0876 ||:  40%|###9      | 2863/7188 [04:31<06:00, 12.01it/s]
2022-03-19 19:44:32,898 - INFO - tqdm - f1: 0.9699, accuracy: 0.9700, batch_loss: 0.0148, loss: 0.0878 ||:  41%|####1     | 2971/7188 [04:42<05:43, 12.27it/s]
2022-03-19 19:44:42,924 - INFO - tqdm - f1: 0.9701, accuracy: 0.9701, batch_loss: 0.0696, loss: 0.0878 ||:  43%|####2     | 3079/7188 [04:52<05:09, 13.29it/s]
2022-03-19 19:44:53,031 - INFO - tqdm - f1: 0.9702, accuracy: 0.9703, batch_loss: 0.0247, loss: 0.0876 ||:  44%|####4     | 3187/7188 [05:02<05:02, 13.23it/s]
2022-03-19 19:45:03,131 - INFO - tqdm - f1: 0.9701, accuracy: 0.9701, batch_loss: 0.0155, loss: 0.0880 ||:  46%|####5     | 3293/7188 [05:12<04:44, 13.71it/s]
2022-03-19 19:45:13,133 - INFO - tqdm - f1: 0.9700, accuracy: 0.9700, batch_loss: 0.0093, loss: 0.0880 ||:  47%|####7     | 3399/7188 [05:22<04:48, 13.14it/s]
2022-03-19 19:45:23,237 - INFO - tqdm - f1: 0.9699, accuracy: 0.9699, batch_loss: 0.1390, loss: 0.0884 ||:  49%|####8     | 3507/7188 [05:32<05:11, 11.81it/s]
2022-03-19 19:45:33,258 - INFO - tqdm - f1: 0.9699, accuracy: 0.9699, batch_loss: 0.1365, loss: 0.0883 ||:  50%|#####     | 3611/7188 [05:42<04:59, 11.93it/s]
2022-03-19 19:45:43,357 - INFO - tqdm - f1: 0.9697, accuracy: 0.9697, batch_loss: 0.0962, loss: 0.0887 ||:  52%|#####1    | 3717/7188 [05:52<04:46, 12.09it/s]
2022-03-19 19:45:53,382 - INFO - tqdm - f1: 0.9695, accuracy: 0.9695, batch_loss: 0.0882, loss: 0.0891 ||:  53%|#####3    | 3819/7188 [06:02<06:01,  9.33it/s]
2022-03-19 19:46:03,386 - INFO - tqdm - f1: 0.9696, accuracy: 0.9696, batch_loss: 0.2875, loss: 0.0893 ||:  55%|#####4    | 3923/7188 [06:12<05:01, 10.84it/s]
2022-03-19 19:46:13,458 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0633, loss: 0.0902 ||:  56%|#####6    | 4031/7188 [06:22<04:27, 11.79it/s]
2022-03-19 19:46:23,494 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.1582, loss: 0.0902 ||:  58%|#####7    | 4139/7188 [06:32<03:49, 13.26it/s]
2022-03-19 19:46:33,584 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0871, loss: 0.0903 ||:  59%|#####9    | 4247/7188 [06:42<04:19, 11.34it/s]
2022-03-19 19:46:43,633 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.0349, loss: 0.0905 ||:  61%|######    | 4355/7188 [06:52<03:49, 12.36it/s]
2022-03-19 19:46:53,635 - INFO - tqdm - f1: 0.9693, accuracy: 0.9693, batch_loss: 0.0251, loss: 0.0900 ||:  62%|######2   | 4461/7188 [07:02<03:42, 12.24it/s]
2022-03-19 19:47:03,751 - INFO - tqdm - f1: 0.9693, accuracy: 0.9693, batch_loss: 0.0039, loss: 0.0898 ||:  64%|######3   | 4569/7188 [07:12<03:38, 12.01it/s]
2022-03-19 19:47:13,818 - INFO - tqdm - f1: 0.9693, accuracy: 0.9693, batch_loss: 0.1334, loss: 0.0898 ||:  65%|######5   | 4675/7188 [07:22<03:27, 12.14it/s]
2022-03-19 19:47:23,867 - INFO - tqdm - f1: 0.9694, accuracy: 0.9694, batch_loss: 0.1272, loss: 0.0899 ||:  66%|######6   | 4779/7188 [07:33<04:54,  8.19it/s]
2022-03-19 19:47:33,964 - INFO - tqdm - f1: 0.9695, accuracy: 0.9695, batch_loss: 0.1230, loss: 0.0897 ||:  68%|######7   | 4885/7188 [07:43<05:37,  6.82it/s]
2022-03-19 19:47:44,071 - INFO - tqdm - f1: 0.9696, accuracy: 0.9696, batch_loss: 0.1431, loss: 0.0894 ||:  69%|######9   | 4989/7188 [07:53<05:16,  6.94it/s]
2022-03-19 19:47:54,196 - INFO - tqdm - f1: 0.9695, accuracy: 0.9696, batch_loss: 0.1132, loss: 0.0894 ||:  71%|#######   | 5093/7188 [08:03<03:37,  9.62it/s]
2022-03-19 19:48:04,309 - INFO - tqdm - f1: 0.9694, accuracy: 0.9694, batch_loss: 0.4289, loss: 0.0898 ||:  72%|#######2  | 5197/7188 [08:13<03:33,  9.32it/s]
2022-03-19 19:48:14,364 - INFO - tqdm - f1: 0.9695, accuracy: 0.9695, batch_loss: 0.1463, loss: 0.0894 ||:  74%|#######3  | 5301/7188 [08:23<04:35,  6.85it/s]
2022-03-19 19:48:24,488 - INFO - tqdm - f1: 0.9696, accuracy: 0.9696, batch_loss: 0.0431, loss: 0.0894 ||:  75%|#######5  | 5405/7188 [08:33<05:16,  5.63it/s]
2022-03-19 19:48:34,612 - INFO - tqdm - f1: 0.9695, accuracy: 0.9696, batch_loss: 0.0181, loss: 0.0892 ||:  77%|#######6  | 5509/7188 [08:43<04:56,  5.66it/s]
2022-03-19 19:48:44,707 - INFO - tqdm - f1: 0.9695, accuracy: 0.9695, batch_loss: 0.0251, loss: 0.0894 ||:  78%|#######8  | 5613/7188 [08:53<03:45,  6.99it/s]
2022-03-19 19:48:54,822 - INFO - tqdm - f1: 0.9693, accuracy: 0.9693, batch_loss: 0.0074, loss: 0.0899 ||:  80%|#######9  | 5719/7188 [09:04<02:57,  8.28it/s]
2022-03-19 19:49:04,861 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0314, loss: 0.0905 ||:  81%|########1 | 5823/7188 [09:14<02:44,  8.28it/s]
2022-03-19 19:49:14,959 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0765, loss: 0.0907 ||:  82%|########2 | 5927/7188 [09:24<02:30,  8.36it/s]
2022-03-19 19:49:25,080 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.2545, loss: 0.0909 ||:  84%|########3 | 6033/7188 [09:34<02:16,  8.44it/s]
2022-03-19 19:49:35,154 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0306, loss: 0.0908 ||:  85%|########5 | 6139/7188 [09:44<01:49,  9.59it/s]
2022-03-19 19:49:45,289 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0192, loss: 0.0910 ||:  87%|########6 | 6245/7188 [09:54<01:40,  9.42it/s]
2022-03-19 19:49:55,399 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0800, loss: 0.0908 ||:  88%|########8 | 6351/7188 [10:04<01:25,  9.81it/s]
2022-03-19 19:50:05,484 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.1895, loss: 0.0907 ||:  90%|########9 | 6457/7188 [10:14<00:55, 13.14it/s]
2022-03-19 19:50:15,562 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.3090, loss: 0.0909 ||:  91%|#########1| 6563/7188 [10:24<00:46, 13.50it/s]
2022-03-19 19:50:25,596 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.0052, loss: 0.0910 ||:  93%|#########2| 6669/7188 [10:34<00:38, 13.41it/s]
2022-03-19 19:50:35,634 - INFO - tqdm - f1: 0.9691, accuracy: 0.9690, batch_loss: 0.0043, loss: 0.0913 ||:  94%|#########4| 6775/7188 [10:44<00:30, 13.63it/s]
2022-03-19 19:50:45,747 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.2701, loss: 0.0913 ||:  96%|#########5| 6881/7188 [10:54<00:25, 11.94it/s]
2022-03-19 19:50:55,756 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.0382, loss: 0.0913 ||:  97%|#########7| 6985/7188 [11:04<00:17, 11.84it/s]
2022-03-19 19:51:05,767 - INFO - tqdm - f1: 0.9690, accuracy: 0.9690, batch_loss: 0.1364, loss: 0.0917 ||:  99%|#########8| 7089/7188 [11:14<00:08, 11.06it/s]
2022-03-19 19:51:12,152 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.0370, loss: 0.0919 ||: 100%|#########9| 7153/7188 [11:21<00:05,  6.87it/s]
2022-03-19 19:51:12,286 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.1481, loss: 0.0920 ||: 100%|#########9| 7155/7188 [11:21<00:04,  8.19it/s]
2022-03-19 19:51:12,416 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.0382, loss: 0.0920 ||: 100%|#########9| 7157/7188 [11:21<00:03,  9.53it/s]
2022-03-19 19:51:12,547 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.0375, loss: 0.0920 ||: 100%|#########9| 7159/7188 [11:21<00:02, 10.73it/s]
2022-03-19 19:51:12,683 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.2055, loss: 0.0920 ||: 100%|#########9| 7161/7188 [11:21<00:02, 11.69it/s]
2022-03-19 19:51:12,809 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.0496, loss: 0.0920 ||: 100%|#########9| 7163/7188 [11:21<00:01, 12.68it/s]
2022-03-19 19:51:12,955 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.3053, loss: 0.0920 ||: 100%|#########9| 7165/7188 [11:22<00:01, 12.98it/s]
2022-03-19 19:51:13,096 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.1594, loss: 0.0920 ||: 100%|#########9| 7167/7188 [11:22<00:01, 13.32it/s]
2022-03-19 19:51:13,235 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.0519, loss: 0.0920 ||: 100%|#########9| 7169/7188 [11:22<00:01, 13.63it/s]
2022-03-19 19:51:13,367 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.2317, loss: 0.0920 ||: 100%|#########9| 7171/7188 [11:22<00:01, 14.04it/s]
2022-03-19 19:51:13,500 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.0197, loss: 0.0920 ||: 100%|#########9| 7173/7188 [11:22<00:01, 14.32it/s]
2022-03-19 19:51:13,642 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.0282, loss: 0.0919 ||: 100%|#########9| 7175/7188 [11:22<00:00, 14.27it/s]
2022-03-19 19:51:13,776 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.3914, loss: 0.0920 ||: 100%|#########9| 7177/7188 [11:22<00:00, 14.46it/s]
2022-03-19 19:51:13,906 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.0120, loss: 0.0920 ||: 100%|#########9| 7179/7188 [11:23<00:00, 14.71it/s]
2022-03-19 19:51:14,038 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.0030, loss: 0.0920 ||: 100%|#########9| 7181/7188 [11:23<00:00, 14.85it/s]
2022-03-19 19:51:15,221 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.0408, loss: 0.0919 ||: 100%|#########9| 7183/7188 [11:24<00:01,  4.45it/s]
2022-03-19 19:51:15,353 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.0817, loss: 0.0920 ||: 100%|#########9| 7185/7188 [11:24<00:00,  5.65it/s]
2022-03-19 19:51:15,489 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.1295, loss: 0.0920 ||: 100%|#########9| 7187/7188 [11:24<00:00,  6.93it/s]
2022-03-19 19:51:15,598 - INFO - tqdm - f1: 0.9689, accuracy: 0.9689, batch_loss: 0.0822, loss: 0.0920 ||: 100%|##########| 7188/7188 [11:24<00:00, 10.50it/s]
2022-03-19 19:51:15,606 - INFO - allennlp.training.trainer - Validating
2022-03-19 19:51:15,608 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-19 19:51:24,480 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.5936, loss: 0.2245 ||: 100%|##########| 313/313 [00:08<00:00, 35.29it/s]
2022-03-19 19:51:24,499 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-19 19:51:24,501 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.969  |     0.940
2022-03-19 19:51:24,503 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.969  |     0.940
2022-03-19 19:51:24,504 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-19 19:51:24,506 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.092  |     0.225
2022-03-19 19:51:24,508 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7573.359  |       N/A
2022-03-19 19:51:24,510 - INFO - allennlp.training.trainer - Epoch duration: 0:11:33.698705
2022-03-19 19:51:24,511 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:57:55
2022-03-19 19:51:24,513 - INFO - allennlp.training.trainer - Epoch 5/9
2022-03-19 19:51:24,515 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-19 19:51:24,517 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-19 19:51:24,520 - INFO - allennlp.training.trainer - Training
2022-03-19 19:51:24,522 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-19 19:51:34,525 - INFO - tqdm - f1: 0.9721, accuracy: 0.9720, batch_loss: 0.0086, loss: 0.0823 ||:   1%|1         | 107/7188 [00:10<07:34, 15.59it/s]
2022-03-19 19:51:44,611 - INFO - tqdm - f1: 0.9753, accuracy: 0.9753, batch_loss: 0.0042, loss: 0.0731 ||:   3%|2         | 215/7188 [00:20<07:48, 14.88it/s]
2022-03-19 19:51:54,681 - INFO - tqdm - f1: 0.9761, accuracy: 0.9761, batch_loss: 0.0048, loss: 0.0692 ||:   4%|4         | 319/7188 [00:30<08:10, 14.01it/s]
2022-03-19 19:52:04,762 - INFO - tqdm - f1: 0.9765, accuracy: 0.9765, batch_loss: 0.2461, loss: 0.0710 ||:   6%|5         | 425/7188 [00:40<07:08, 15.78it/s]
2022-03-19 19:52:14,849 - INFO - tqdm - f1: 0.9773, accuracy: 0.9774, batch_loss: 0.0120, loss: 0.0671 ||:   7%|7         | 531/7188 [00:50<07:33, 14.68it/s]
2022-03-19 19:52:24,918 - INFO - tqdm - f1: 0.9784, accuracy: 0.9783, batch_loss: 0.0647, loss: 0.0646 ||:   9%|8         | 635/7188 [01:00<07:06, 15.37it/s]
2022-03-19 19:52:35,020 - INFO - tqdm - f1: 0.9778, accuracy: 0.9777, batch_loss: 0.0166, loss: 0.0664 ||:  10%|#         | 741/7188 [01:10<06:51, 15.67it/s]
2022-03-19 19:52:45,038 - INFO - tqdm - f1: 0.9768, accuracy: 0.9768, batch_loss: 0.0324, loss: 0.0689 ||:  12%|#1        | 847/7188 [01:20<06:56, 15.24it/s]
2022-03-19 19:52:55,122 - INFO - tqdm - f1: 0.9765, accuracy: 0.9765, batch_loss: 0.0133, loss: 0.0698 ||:  13%|#3        | 953/7188 [01:30<07:05, 14.64it/s]
2022-03-19 19:53:05,162 - INFO - tqdm - f1: 0.9769, accuracy: 0.9769, batch_loss: 0.0126, loss: 0.0683 ||:  15%|#4        | 1057/7188 [01:40<06:57, 14.67it/s]
2022-03-19 19:53:15,206 - INFO - tqdm - f1: 0.9767, accuracy: 0.9768, batch_loss: 0.0045, loss: 0.0683 ||:  16%|#6        | 1163/7188 [01:50<06:29, 15.48it/s]
2022-03-19 19:53:25,254 - INFO - tqdm - f1: 0.9771, accuracy: 0.9772, batch_loss: 0.1413, loss: 0.0674 ||:  18%|#7        | 1267/7188 [02:00<06:19, 15.62it/s]
2022-03-19 19:53:35,283 - INFO - tqdm - f1: 0.9767, accuracy: 0.9768, batch_loss: 0.0059, loss: 0.0681 ||:  19%|#9        | 1371/7188 [02:10<06:24, 15.11it/s]
2022-03-19 19:53:45,356 - INFO - tqdm - f1: 0.9770, accuracy: 0.9770, batch_loss: 0.0030, loss: 0.0673 ||:  21%|##        | 1477/7188 [02:20<06:12, 15.33it/s]
2022-03-19 19:53:55,385 - INFO - tqdm - f1: 0.9766, accuracy: 0.9767, batch_loss: 0.0039, loss: 0.0679 ||:  22%|##1       | 1581/7188 [02:30<06:23, 14.61it/s]
2022-03-19 19:54:05,438 - INFO - tqdm - f1: 0.9769, accuracy: 0.9770, batch_loss: 0.0072, loss: 0.0669 ||:  23%|##3       | 1687/7188 [02:40<06:15, 14.65it/s]
2022-03-19 19:54:15,524 - INFO - tqdm - f1: 0.9769, accuracy: 0.9770, batch_loss: 0.0019, loss: 0.0672 ||:  25%|##4       | 1793/7188 [02:51<06:13, 14.45it/s]
2022-03-19 19:54:25,538 - INFO - tqdm - f1: 0.9771, accuracy: 0.9771, batch_loss: 0.0037, loss: 0.0668 ||:  26%|##6       | 1895/7188 [03:01<06:06, 14.44it/s]
2022-03-19 19:54:35,558 - INFO - tqdm - f1: 0.9770, accuracy: 0.9771, batch_loss: 0.0049, loss: 0.0670 ||:  28%|##7       | 1999/7188 [03:11<05:50, 14.79it/s]
2022-03-19 19:54:45,582 - INFO - tqdm - f1: 0.9768, accuracy: 0.9769, batch_loss: 0.0922, loss: 0.0671 ||:  29%|##9       | 2107/7188 [03:21<05:40, 14.93it/s]
2022-03-19 19:54:55,704 - INFO - tqdm - f1: 0.9768, accuracy: 0.9769, batch_loss: 0.0088, loss: 0.0668 ||:  31%|###       | 2213/7188 [03:31<05:56, 13.96it/s]
2022-03-19 19:55:05,735 - INFO - tqdm - f1: 0.9765, accuracy: 0.9766, batch_loss: 0.0992, loss: 0.0672 ||:  32%|###2      | 2319/7188 [03:41<05:34, 14.57it/s]
2022-03-19 19:55:15,771 - INFO - tqdm - f1: 0.9766, accuracy: 0.9767, batch_loss: 0.5130, loss: 0.0676 ||:  34%|###3      | 2425/7188 [03:51<05:26, 14.59it/s]
2022-03-19 19:55:25,841 - INFO - tqdm - f1: 0.9764, accuracy: 0.9764, batch_loss: 0.1270, loss: 0.0689 ||:  35%|###5      | 2531/7188 [04:01<05:05, 15.24it/s]
2022-03-19 19:55:35,848 - INFO - tqdm - f1: 0.9763, accuracy: 0.9764, batch_loss: 0.0197, loss: 0.0688 ||:  37%|###6      | 2637/7188 [04:11<05:03, 15.01it/s]
2022-03-19 19:55:45,926 - INFO - tqdm - f1: 0.9762, accuracy: 0.9763, batch_loss: 0.0350, loss: 0.0691 ||:  38%|###8      | 2745/7188 [04:21<04:51, 15.25it/s]
2022-03-19 19:55:55,992 - INFO - tqdm - f1: 0.9762, accuracy: 0.9762, batch_loss: 0.0549, loss: 0.0694 ||:  40%|###9      | 2853/7188 [04:31<04:43, 15.30it/s]
2022-03-19 19:56:06,091 - INFO - tqdm - f1: 0.9763, accuracy: 0.9763, batch_loss: 0.0067, loss: 0.0695 ||:  41%|####1     | 2961/7188 [04:41<04:47, 14.68it/s]
2022-03-19 19:56:16,091 - INFO - tqdm - f1: 0.9761, accuracy: 0.9762, batch_loss: 0.3304, loss: 0.0700 ||:  43%|####2     | 3069/7188 [04:51<04:40, 14.69it/s]
2022-03-19 19:56:26,183 - INFO - tqdm - f1: 0.9763, accuracy: 0.9763, batch_loss: 0.0829, loss: 0.0698 ||:  44%|####4     | 3179/7188 [05:01<04:30, 14.84it/s]
2022-03-19 19:56:36,199 - INFO - tqdm - f1: 0.9762, accuracy: 0.9763, batch_loss: 0.1432, loss: 0.0702 ||:  46%|####5     | 3283/7188 [05:11<04:47, 13.59it/s]
2022-03-19 19:56:46,243 - INFO - tqdm - f1: 0.9762, accuracy: 0.9762, batch_loss: 0.3763, loss: 0.0701 ||:  47%|####7     | 3389/7188 [05:21<04:44, 13.34it/s]
2022-03-19 19:56:56,305 - INFO - tqdm - f1: 0.9762, accuracy: 0.9763, batch_loss: 0.0050, loss: 0.0704 ||:  49%|####8     | 3493/7188 [05:31<05:17, 11.63it/s]
2022-03-19 19:57:06,386 - INFO - tqdm - f1: 0.9762, accuracy: 0.9762, batch_loss: 0.0773, loss: 0.0705 ||:  50%|#####     | 3599/7188 [05:41<05:00, 11.96it/s]
2022-03-19 19:57:16,414 - INFO - tqdm - f1: 0.9760, accuracy: 0.9760, batch_loss: 0.2182, loss: 0.0711 ||:  52%|#####1    | 3703/7188 [05:51<05:16, 11.00it/s]
2022-03-19 19:57:26,475 - INFO - tqdm - f1: 0.9760, accuracy: 0.9760, batch_loss: 0.0036, loss: 0.0710 ||:  53%|#####2    | 3807/7188 [06:01<04:32, 12.43it/s]
2022-03-19 19:57:36,576 - INFO - tqdm - f1: 0.9761, accuracy: 0.9762, batch_loss: 0.0792, loss: 0.0708 ||:  54%|#####4    | 3911/7188 [06:12<04:17, 12.74it/s]
2022-03-19 19:57:46,577 - INFO - tqdm - f1: 0.9761, accuracy: 0.9761, batch_loss: 0.0531, loss: 0.0710 ||:  56%|#####5    | 4015/7188 [06:22<04:04, 12.99it/s]
2022-03-19 19:57:56,580 - INFO - tqdm - f1: 0.9759, accuracy: 0.9760, batch_loss: 0.1488, loss: 0.0712 ||:  57%|#####7    | 4121/7188 [06:32<03:38, 14.05it/s]
2022-03-19 19:58:06,683 - INFO - tqdm - f1: 0.9758, accuracy: 0.9759, batch_loss: 0.0107, loss: 0.0713 ||:  59%|#####8    | 4227/7188 [06:42<03:35, 13.73it/s]
2022-03-19 19:58:16,804 - INFO - tqdm - f1: 0.9758, accuracy: 0.9759, batch_loss: 0.0034, loss: 0.0713 ||:  60%|######    | 4333/7188 [06:52<03:15, 14.57it/s]
2022-03-19 19:58:26,824 - INFO - tqdm - f1: 0.9757, accuracy: 0.9757, batch_loss: 0.1004, loss: 0.0716 ||:  62%|######1   | 4439/7188 [07:02<03:16, 13.96it/s]
2022-03-19 19:58:36,927 - INFO - tqdm - f1: 0.9756, accuracy: 0.9756, batch_loss: 0.1888, loss: 0.0719 ||:  63%|######3   | 4543/7188 [07:12<03:16, 13.49it/s]
2022-03-19 19:58:47,038 - INFO - tqdm - f1: 0.9755, accuracy: 0.9755, batch_loss: 0.1680, loss: 0.0725 ||:  65%|######4   | 4647/7188 [07:22<03:02, 13.93it/s]
2022-03-19 19:58:57,179 - INFO - tqdm - f1: 0.9754, accuracy: 0.9754, batch_loss: 0.0023, loss: 0.0728 ||:  66%|######6   | 4753/7188 [07:32<02:55, 13.87it/s]
2022-03-19 19:59:07,223 - INFO - tqdm - f1: 0.9753, accuracy: 0.9753, batch_loss: 0.0722, loss: 0.0733 ||:  68%|######7   | 4857/7188 [07:42<02:46, 14.02it/s]
2022-03-19 19:59:17,244 - INFO - tqdm - f1: 0.9754, accuracy: 0.9754, batch_loss: 0.0036, loss: 0.0732 ||:  69%|######9   | 4961/7188 [07:52<02:37, 14.10it/s]
2022-03-19 19:59:27,344 - INFO - tqdm - f1: 0.9755, accuracy: 0.9755, batch_loss: 0.0092, loss: 0.0730 ||:  71%|#######   | 5069/7188 [08:02<02:24, 14.64it/s]
2022-03-19 19:59:37,467 - INFO - tqdm - f1: 0.9754, accuracy: 0.9754, batch_loss: 0.0250, loss: 0.0733 ||:  72%|#######2  | 5177/7188 [08:12<02:15, 14.83it/s]
2022-03-19 19:59:47,496 - INFO - tqdm - f1: 0.9753, accuracy: 0.9753, batch_loss: 0.0418, loss: 0.0735 ||:  73%|#######3  | 5283/7188 [08:22<02:11, 14.54it/s]
2022-03-19 19:59:57,572 - INFO - tqdm - f1: 0.9752, accuracy: 0.9752, batch_loss: 0.1921, loss: 0.0737 ||:  75%|#######4  | 5389/7188 [08:33<02:03, 14.60it/s]
2022-03-19 20:00:07,653 - INFO - tqdm - f1: 0.9752, accuracy: 0.9752, batch_loss: 0.0598, loss: 0.0736 ||:  76%|#######6  | 5497/7188 [08:43<01:52, 14.97it/s]
2022-03-19 20:00:17,741 - INFO - tqdm - f1: 0.9751, accuracy: 0.9751, batch_loss: 0.0629, loss: 0.0740 ||:  78%|#######7  | 5603/7188 [08:53<01:58, 13.41it/s]
2022-03-19 20:00:27,767 - INFO - tqdm - f1: 0.9749, accuracy: 0.9749, batch_loss: 0.0561, loss: 0.0745 ||:  79%|#######9  | 5709/7188 [09:03<01:46, 13.85it/s]
2022-03-19 20:00:37,805 - INFO - tqdm - f1: 0.9749, accuracy: 0.9749, batch_loss: 0.0045, loss: 0.0747 ||:  81%|########  | 5817/7188 [09:13<01:43, 13.29it/s]
2022-03-19 20:00:47,860 - INFO - tqdm - f1: 0.9747, accuracy: 0.9747, batch_loss: 0.1315, loss: 0.0748 ||:  82%|########2 | 5925/7188 [09:23<01:30, 14.01it/s]
2022-03-19 20:00:57,877 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0060, loss: 0.0747 ||:  84%|########3 | 6031/7188 [09:33<01:24, 13.66it/s]
2022-03-19 20:01:07,980 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.1168, loss: 0.0749 ||:  85%|########5 | 6135/7188 [09:43<01:15, 13.99it/s]
2022-03-19 20:01:18,028 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0416, loss: 0.0749 ||:  87%|########6 | 6239/7188 [09:53<01:10, 13.44it/s]
2022-03-19 20:01:28,043 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0892, loss: 0.0751 ||:  88%|########8 | 6347/7188 [10:03<01:09, 12.04it/s]
2022-03-19 20:01:38,044 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.3521, loss: 0.0750 ||:  90%|########9 | 6449/7188 [10:13<01:17,  9.51it/s]
2022-03-19 20:01:48,068 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.2174, loss: 0.0751 ||:  91%|#########1| 6555/7188 [10:23<01:04,  9.76it/s]
2022-03-19 20:01:58,174 - INFO - tqdm - f1: 0.9747, accuracy: 0.9747, batch_loss: 0.0056, loss: 0.0752 ||:  93%|#########2| 6665/7188 [10:33<01:01,  8.52it/s]
2022-03-19 20:02:08,236 - INFO - tqdm - f1: 0.9745, accuracy: 0.9745, batch_loss: 0.0324, loss: 0.0758 ||:  94%|#########4| 6767/7188 [10:43<01:13,  5.69it/s]
2022-03-19 20:02:18,391 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.0038, loss: 0.0759 ||:  96%|#########5| 6871/7188 [10:53<01:10,  4.49it/s]
2022-03-19 20:02:28,840 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.0098, loss: 0.0759 ||:  97%|#########7| 6979/7188 [11:04<00:47,  4.43it/s]
2022-03-19 20:02:39,051 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.0698, loss: 0.0758 ||:  99%|#########8| 7083/7188 [11:14<00:23,  4.46it/s]
2022-03-19 20:02:44,961 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.0021, loss: 0.0760 ||: 100%|#########9| 7153/7188 [11:20<00:02, 14.51it/s]
2022-03-19 20:02:46,124 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.0066, loss: 0.0760 ||: 100%|#########9| 7155/7188 [11:21<00:07,  4.49it/s]
2022-03-19 20:02:46,270 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.0127, loss: 0.0760 ||: 100%|#########9| 7157/7188 [11:21<00:05,  5.62it/s]
2022-03-19 20:02:46,415 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.0416, loss: 0.0760 ||: 100%|#########9| 7159/7188 [11:21<00:04,  6.84it/s]
2022-03-19 20:02:46,567 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.0572, loss: 0.0760 ||: 100%|#########9| 7161/7188 [11:22<00:03,  7.99it/s]
2022-03-19 20:02:46,725 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.3304, loss: 0.0761 ||: 100%|#########9| 7163/7188 [11:22<00:02,  8.98it/s]
2022-03-19 20:02:46,856 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.1103, loss: 0.0761 ||: 100%|#########9| 7165/7188 [11:22<00:02, 10.26it/s]
2022-03-19 20:02:46,992 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.0122, loss: 0.0760 ||: 100%|#########9| 7167/7188 [11:22<00:01, 11.27it/s]
2022-03-19 20:02:47,131 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.0879, loss: 0.0761 ||: 100%|#########9| 7169/7188 [11:22<00:01, 12.07it/s]
2022-03-19 20:02:47,277 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.0415, loss: 0.0760 ||: 100%|#########9| 7171/7188 [11:22<00:01, 12.52it/s]
2022-03-19 20:02:47,419 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.1142, loss: 0.0760 ||: 100%|#########9| 7173/7188 [11:22<00:01, 12.94it/s]
2022-03-19 20:02:47,570 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.0114, loss: 0.0760 ||: 100%|#########9| 7175/7188 [11:23<00:00, 13.04it/s]
2022-03-19 20:02:47,718 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.0031, loss: 0.0760 ||: 100%|#########9| 7177/7188 [11:23<00:00, 13.17it/s]
2022-03-19 20:02:47,859 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.0086, loss: 0.0760 ||: 100%|#########9| 7179/7188 [11:23<00:00, 13.46it/s]
2022-03-19 20:02:47,996 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.0430, loss: 0.0760 ||: 100%|#########9| 7181/7188 [11:23<00:00, 13.77it/s]
2022-03-19 20:02:48,135 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.5943, loss: 0.0761 ||: 100%|#########9| 7183/7188 [11:23<00:00, 13.96it/s]
2022-03-19 20:02:48,281 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.4435, loss: 0.0762 ||: 100%|#########9| 7185/7188 [11:23<00:00, 13.88it/s]
2022-03-19 20:02:48,414 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.0246, loss: 0.0762 ||: 100%|#########9| 7187/7188 [11:23<00:00, 14.21it/s]
2022-03-19 20:02:48,535 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.1088, loss: 0.0762 ||: 100%|##########| 7188/7188 [11:24<00:00, 10.51it/s]
2022-03-19 20:02:48,543 - INFO - allennlp.training.trainer - Validating
2022-03-19 20:02:48,545 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-19 20:02:58,563 - INFO - tqdm - f1: 0.9388, accuracy: 0.9389, batch_loss: 0.0828, loss: 0.2382 ||:  94%|#########4| 295/313 [00:10<00:00, 42.11it/s]
2022-03-19 20:02:59,062 - INFO - tqdm - f1: 0.9384, accuracy: 0.9384, batch_loss: 0.3361, loss: 0.2400 ||: 100%|##########| 313/313 [00:10<00:00, 29.77it/s]
2022-03-19 20:02:59,081 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-19 20:02:59,083 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.974  |     0.938
2022-03-19 20:02:59,085 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.974  |     0.938
2022-03-19 20:02:59,087 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-19 20:02:59,089 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.076  |     0.240
2022-03-19 20:02:59,092 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7573.359  |       N/A
2022-03-19 20:02:59,094 - INFO - allennlp.training.trainer - Epoch duration: 0:11:34.580568
2022-03-19 20:02:59,097 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:46:20
2022-03-19 20:02:59,099 - INFO - allennlp.training.trainer - Epoch 6/9
2022-03-19 20:02:59,100 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-19 20:02:59,102 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-19 20:02:59,105 - INFO - allennlp.training.trainer - Training
2022-03-19 20:02:59,107 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-19 20:03:09,142 - INFO - tqdm - f1: 0.9832, accuracy: 0.9830, batch_loss: 0.0794, loss: 0.0546 ||:   1%|1         | 99/7188 [00:10<08:16, 14.27it/s]
2022-03-19 20:03:19,250 - INFO - tqdm - f1: 0.9846, accuracy: 0.9846, batch_loss: 0.0549, loss: 0.0514 ||:   3%|2         | 207/7188 [00:20<08:00, 14.52it/s]
2022-03-19 20:03:29,336 - INFO - tqdm - f1: 0.9814, accuracy: 0.9815, batch_loss: 0.0180, loss: 0.0557 ||:   4%|4         | 315/7188 [00:30<08:00, 14.31it/s]
2022-03-19 20:03:39,365 - INFO - tqdm - f1: 0.9803, accuracy: 0.9804, batch_loss: 0.0349, loss: 0.0573 ||:   6%|5         | 421/7188 [00:40<07:56, 14.20it/s]
2022-03-19 20:03:49,494 - INFO - tqdm - f1: 0.9810, accuracy: 0.9810, batch_loss: 0.0226, loss: 0.0558 ||:   7%|7         | 531/7188 [00:50<08:21, 13.27it/s]
2022-03-19 20:03:59,581 - INFO - tqdm - f1: 0.9799, accuracy: 0.9799, batch_loss: 0.0160, loss: 0.0599 ||:   9%|8         | 637/7188 [01:00<09:12, 11.86it/s]
2022-03-19 20:04:09,641 - INFO - tqdm - f1: 0.9795, accuracy: 0.9795, batch_loss: 0.0333, loss: 0.0608 ||:  10%|#         | 741/7188 [01:10<12:33,  8.56it/s]
2022-03-19 20:04:19,661 - INFO - tqdm - f1: 0.9791, accuracy: 0.9790, batch_loss: 0.0995, loss: 0.0614 ||:  12%|#1        | 843/7188 [01:20<15:10,  6.97it/s]
2022-03-19 20:04:30,274 - INFO - tqdm - f1: 0.9792, accuracy: 0.9792, batch_loss: 0.4559, loss: 0.0608 ||:  13%|#3        | 957/7188 [01:31<22:55,  4.53it/s]
2022-03-19 20:04:40,917 - INFO - tqdm - f1: 0.9791, accuracy: 0.9791, batch_loss: 0.0152, loss: 0.0610 ||:  15%|#4        | 1067/7188 [01:41<22:11,  4.60it/s]
2022-03-19 20:04:51,754 - INFO - tqdm - f1: 0.9793, accuracy: 0.9794, batch_loss: 0.0027, loss: 0.0612 ||:  16%|#6        | 1183/7188 [01:52<22:29,  4.45it/s]
2022-03-19 20:05:02,006 - INFO - tqdm - f1: 0.9793, accuracy: 0.9794, batch_loss: 0.0472, loss: 0.0611 ||:  18%|#7        | 1289/7188 [02:02<21:32,  4.57it/s]
2022-03-19 20:05:12,124 - INFO - tqdm - f1: 0.9794, accuracy: 0.9794, batch_loss: 0.0448, loss: 0.0605 ||:  19%|#9        | 1395/7188 [02:13<21:21,  4.52it/s]
2022-03-19 20:05:22,298 - INFO - tqdm - f1: 0.9798, accuracy: 0.9798, batch_loss: 0.0032, loss: 0.0598 ||:  21%|##        | 1503/7188 [02:23<20:41,  4.58it/s]
2022-03-19 20:05:32,330 - INFO - tqdm - f1: 0.9798, accuracy: 0.9799, batch_loss: 0.0037, loss: 0.0598 ||:  22%|##2       | 1609/7188 [02:33<20:05,  4.63it/s]
2022-03-19 20:05:42,673 - INFO - tqdm - f1: 0.9798, accuracy: 0.9798, batch_loss: 0.0403, loss: 0.0599 ||:  24%|##3       | 1719/7188 [02:43<20:19,  4.49it/s]
2022-03-19 20:05:53,071 - INFO - tqdm - f1: 0.9800, accuracy: 0.9800, batch_loss: 0.0366, loss: 0.0594 ||:  25%|##5       | 1829/7188 [02:53<19:27,  4.59it/s]
2022-03-19 20:06:03,135 - INFO - tqdm - f1: 0.9799, accuracy: 0.9799, batch_loss: 0.3074, loss: 0.0596 ||:  27%|##6       | 1937/7188 [03:04<19:30,  4.48it/s]
2022-03-19 20:06:13,394 - INFO - tqdm - f1: 0.9799, accuracy: 0.9800, batch_loss: 0.0026, loss: 0.0594 ||:  28%|##8       | 2047/7188 [03:14<18:18,  4.68it/s]
2022-03-19 20:06:23,408 - INFO - tqdm - f1: 0.9801, accuracy: 0.9801, batch_loss: 0.4483, loss: 0.0591 ||:  30%|##9       | 2153/7188 [03:24<18:04,  4.64it/s]
2022-03-19 20:06:33,799 - INFO - tqdm - f1: 0.9802, accuracy: 0.9802, batch_loss: 0.0909, loss: 0.0592 ||:  32%|###1      | 2267/7188 [03:34<17:23,  4.72it/s]
2022-03-19 20:06:43,916 - INFO - tqdm - f1: 0.9799, accuracy: 0.9799, batch_loss: 0.3840, loss: 0.0605 ||:  33%|###3      | 2385/7188 [03:44<05:07, 15.61it/s]
2022-03-19 20:06:54,067 - INFO - tqdm - f1: 0.9799, accuracy: 0.9800, batch_loss: 0.0037, loss: 0.0606 ||:  35%|###4      | 2488/7188 [03:54<11:55,  6.57it/s]
2022-03-19 20:07:04,116 - INFO - tqdm - f1: 0.9800, accuracy: 0.9800, batch_loss: 0.0064, loss: 0.0605 ||:  36%|###5      | 2558/7188 [04:05<08:59,  8.57it/s]
2022-03-19 20:07:14,157 - INFO - tqdm - f1: 0.9801, accuracy: 0.9801, batch_loss: 0.0082, loss: 0.0605 ||:  37%|###6      | 2645/7188 [04:15<11:52,  6.38it/s]
2022-03-19 20:07:24,206 - INFO - tqdm - f1: 0.9799, accuracy: 0.9799, batch_loss: 0.0023, loss: 0.0605 ||:  38%|###7      | 2711/7188 [04:25<11:49,  6.31it/s]
2022-03-19 20:07:34,212 - INFO - tqdm - f1: 0.9797, accuracy: 0.9797, batch_loss: 0.0044, loss: 0.0609 ||:  39%|###8      | 2800/7188 [04:35<07:21,  9.95it/s]
2022-03-19 20:07:44,275 - INFO - tqdm - f1: 0.9797, accuracy: 0.9797, batch_loss: 0.0093, loss: 0.0609 ||:  40%|###9      | 2869/7188 [04:45<11:54,  6.04it/s]
2022-03-19 20:07:54,420 - INFO - tqdm - f1: 0.9798, accuracy: 0.9798, batch_loss: 0.0039, loss: 0.0609 ||:  41%|####1     | 2950/7188 [04:55<07:10,  9.85it/s]
2022-03-19 20:08:04,539 - INFO - tqdm - f1: 0.9795, accuracy: 0.9796, batch_loss: 0.0074, loss: 0.0611 ||:  42%|####2     | 3030/7188 [05:05<11:52,  5.84it/s]
2022-03-19 20:08:14,657 - INFO - tqdm - f1: 0.9795, accuracy: 0.9795, batch_loss: 0.0951, loss: 0.0611 ||:  43%|####3     | 3101/7188 [05:15<07:45,  8.78it/s]
2022-03-19 20:08:24,758 - INFO - tqdm - f1: 0.9795, accuracy: 0.9795, batch_loss: 0.0038, loss: 0.0612 ||:  44%|####4     | 3190/7188 [05:25<10:25,  6.39it/s]
2022-03-19 20:08:34,909 - INFO - tqdm - f1: 0.9795, accuracy: 0.9795, batch_loss: 0.0043, loss: 0.0612 ||:  45%|####5     | 3254/7188 [05:35<10:16,  6.38it/s]
2022-03-19 20:08:44,997 - INFO - tqdm - f1: 0.9795, accuracy: 0.9795, batch_loss: 0.0055, loss: 0.0612 ||:  46%|####6     | 3342/7188 [05:45<07:31,  8.52it/s]
2022-03-19 20:08:55,048 - INFO - tqdm - f1: 0.9795, accuracy: 0.9795, batch_loss: 0.0032, loss: 0.0610 ||:  47%|####7     | 3412/7188 [05:55<10:25,  6.03it/s]
2022-03-19 20:09:05,140 - INFO - tqdm - f1: 0.9795, accuracy: 0.9795, batch_loss: 0.0073, loss: 0.0612 ||:  49%|####8     | 3493/7188 [06:06<06:41,  9.21it/s]
2022-03-19 20:09:15,251 - INFO - tqdm - f1: 0.9795, accuracy: 0.9795, batch_loss: 0.0036, loss: 0.0611 ||:  50%|####9     | 3570/7188 [06:16<09:19,  6.47it/s]
2022-03-19 20:09:25,415 - INFO - tqdm - f1: 0.9795, accuracy: 0.9795, batch_loss: 0.0104, loss: 0.0611 ||:  51%|#####     | 3644/7188 [06:26<06:27,  9.14it/s]
2022-03-19 20:09:35,535 - INFO - tqdm - f1: 0.9795, accuracy: 0.9795, batch_loss: 0.0018, loss: 0.0615 ||:  52%|#####1    | 3729/7188 [06:36<09:53,  5.83it/s]
2022-03-19 20:09:45,585 - INFO - tqdm - f1: 0.9793, accuracy: 0.9793, batch_loss: 0.2211, loss: 0.0619 ||:  53%|#####2    | 3792/7188 [06:46<09:28,  5.98it/s]
2022-03-19 20:09:55,599 - INFO - tqdm - f1: 0.9794, accuracy: 0.9794, batch_loss: 0.0034, loss: 0.0617 ||:  54%|#####3    | 3871/7188 [06:56<08:02,  6.87it/s]
2022-03-19 20:10:05,868 - INFO - tqdm - f1: 0.9792, accuracy: 0.9792, batch_loss: 0.1442, loss: 0.0619 ||:  54%|#####4    | 3909/7188 [07:06<27:21,  2.00it/s]
2022-03-19 20:10:16,899 - INFO - tqdm - f1: 0.9792, accuracy: 0.9792, batch_loss: 0.2137, loss: 0.0619 ||:  55%|#####5    | 3964/7188 [07:17<22:48,  2.36it/s]
2022-03-19 20:10:26,984 - INFO - tqdm - f1: 0.9791, accuracy: 0.9791, batch_loss: 0.0241, loss: 0.0623 ||:  56%|#####6    | 4030/7188 [07:27<07:12,  7.30it/s]
2022-03-19 20:10:37,365 - INFO - tqdm - f1: 0.9791, accuracy: 0.9791, batch_loss: 0.5125, loss: 0.0625 ||:  57%|#####6    | 4076/7188 [07:38<23:55,  2.17it/s]
2022-03-19 20:10:47,466 - INFO - tqdm - f1: 0.9790, accuracy: 0.9790, batch_loss: 0.0276, loss: 0.0627 ||:  57%|#####7    | 4125/7188 [07:48<07:46,  6.57it/s]
2022-03-19 20:10:57,484 - INFO - tqdm - f1: 0.9790, accuracy: 0.9789, batch_loss: 0.0317, loss: 0.0628 ||:  58%|#####8    | 4175/7188 [07:58<06:40,  7.52it/s]
2022-03-19 20:11:07,488 - INFO - tqdm - f1: 0.9790, accuracy: 0.9790, batch_loss: 0.0143, loss: 0.0627 ||:  59%|#####8    | 4222/7188 [08:08<08:07,  6.09it/s]
2022-03-19 20:11:17,770 - INFO - tqdm - f1: 0.9790, accuracy: 0.9790, batch_loss: 0.5500, loss: 0.0627 ||:  59%|#####9    | 4276/7188 [08:18<23:19,  2.08it/s]
2022-03-19 20:11:27,803 - INFO - tqdm - f1: 0.9790, accuracy: 0.9790, batch_loss: 0.0271, loss: 0.0627 ||:  60%|######    | 4346/7188 [08:28<09:49,  4.82it/s]
2022-03-19 20:11:37,921 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0217, loss: 0.0632 ||:  61%|######1   | 4398/7188 [08:38<08:58,  5.18it/s]
2022-03-19 20:11:47,972 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.1281, loss: 0.0631 ||:  62%|######2   | 4458/7188 [08:48<12:12,  3.73it/s]
2022-03-19 20:11:58,036 - INFO - tqdm - f1: 0.9788, accuracy: 0.9788, batch_loss: 0.0622, loss: 0.0633 ||:  63%|######2   | 4509/7188 [08:58<12:54,  3.46it/s]
2022-03-19 20:12:08,251 - INFO - tqdm - f1: 0.9788, accuracy: 0.9788, batch_loss: 0.0445, loss: 0.0635 ||:  64%|######3   | 4571/7188 [09:09<22:23,  1.95it/s]
2022-03-19 20:12:19,331 - INFO - tqdm - f1: 0.9787, accuracy: 0.9787, batch_loss: 0.1059, loss: 0.0637 ||:  64%|######4   | 4621/7188 [09:20<21:41,  1.97it/s]
2022-03-19 20:12:30,171 - INFO - tqdm - f1: 0.9787, accuracy: 0.9788, batch_loss: 0.0772, loss: 0.0636 ||:  65%|######5   | 4686/7188 [09:31<10:50,  3.85it/s]
2022-03-19 20:12:40,231 - INFO - tqdm - f1: 0.9787, accuracy: 0.9787, batch_loss: 0.0098, loss: 0.0639 ||:  66%|######5   | 4736/7188 [09:41<06:03,  6.74it/s]
2022-03-19 20:12:50,285 - INFO - tqdm - f1: 0.9787, accuracy: 0.9787, batch_loss: 0.0411, loss: 0.0640 ||:  67%|######6   | 4788/7188 [09:51<05:14,  7.63it/s]
2022-03-19 20:13:00,341 - INFO - tqdm - f1: 0.9787, accuracy: 0.9787, batch_loss: 0.0370, loss: 0.0639 ||:  67%|######7   | 4834/7188 [10:01<12:02,  3.26it/s]
2022-03-19 20:13:10,416 - INFO - tqdm - f1: 0.9788, accuracy: 0.9788, batch_loss: 0.0046, loss: 0.0640 ||:  68%|######8   | 4890/7188 [10:11<13:24,  2.86it/s]
2022-03-19 20:13:20,510 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0376, loss: 0.0638 ||:  69%|######8   | 4947/7188 [10:21<11:17,  3.31it/s]
2022-03-19 20:13:31,333 - INFO - tqdm - f1: 0.9788, accuracy: 0.9789, batch_loss: 0.0063, loss: 0.0640 ||:  70%|######9   | 5002/7188 [10:32<17:30,  2.08it/s]
2022-03-19 20:13:41,414 - INFO - tqdm - f1: 0.9787, accuracy: 0.9787, batch_loss: 0.0029, loss: 0.0643 ||:  71%|#######   | 5086/7188 [10:42<02:24, 14.56it/s]
2022-03-19 20:13:51,517 - INFO - tqdm - f1: 0.9787, accuracy: 0.9787, batch_loss: 0.0106, loss: 0.0644 ||:  72%|#######2  | 5192/7188 [10:52<02:10, 15.32it/s]
2022-03-19 20:14:01,539 - INFO - tqdm - f1: 0.9786, accuracy: 0.9786, batch_loss: 0.0065, loss: 0.0647 ||:  74%|#######3  | 5298/7188 [11:02<02:01, 15.55it/s]
2022-03-19 20:14:11,570 - INFO - tqdm - f1: 0.9786, accuracy: 0.9786, batch_loss: 0.0981, loss: 0.0646 ||:  75%|#######5  | 5402/7188 [11:12<02:01, 14.74it/s]
2022-03-19 20:14:21,596 - INFO - tqdm - f1: 0.9785, accuracy: 0.9785, batch_loss: 0.0068, loss: 0.0649 ||:  77%|#######6  | 5504/7188 [11:22<01:53, 14.82it/s]
2022-03-19 20:14:31,648 - INFO - tqdm - f1: 0.9783, accuracy: 0.9783, batch_loss: 0.0259, loss: 0.0655 ||:  78%|#######8  | 5610/7188 [11:32<01:53, 13.88it/s]
2022-03-19 20:14:41,766 - INFO - tqdm - f1: 0.9783, accuracy: 0.9783, batch_loss: 0.0035, loss: 0.0654 ||:  79%|#######9  | 5714/7188 [11:42<01:46, 13.81it/s]
2022-03-19 20:14:51,802 - INFO - tqdm - f1: 0.9783, accuracy: 0.9783, batch_loss: 0.0887, loss: 0.0658 ||:  81%|########  | 5816/7188 [11:52<01:54, 11.95it/s]
2022-03-19 20:15:01,936 - INFO - tqdm - f1: 0.9782, accuracy: 0.9782, batch_loss: 0.1523, loss: 0.0659 ||:  82%|########2 | 5920/7188 [12:02<04:41,  4.50it/s]
2022-03-19 20:15:12,064 - INFO - tqdm - f1: 0.9782, accuracy: 0.9782, batch_loss: 0.0980, loss: 0.0659 ||:  84%|########3 | 6026/7188 [12:12<01:49, 10.66it/s]
2022-03-19 20:15:22,159 - INFO - tqdm - f1: 0.9782, accuracy: 0.9782, batch_loss: 0.1801, loss: 0.0658 ||:  85%|########5 | 6118/7188 [12:23<01:26, 12.44it/s]
2022-03-19 20:15:32,225 - INFO - tqdm - f1: 0.9782, accuracy: 0.9782, batch_loss: 0.0058, loss: 0.0658 ||:  86%|########6 | 6197/7188 [12:33<02:05,  7.91it/s]
2022-03-19 20:15:42,344 - INFO - tqdm - f1: 0.9782, accuracy: 0.9782, batch_loss: 0.0067, loss: 0.0661 ||:  88%|########7 | 6290/7188 [12:43<02:04,  7.19it/s]
2022-03-19 20:15:52,735 - INFO - tqdm - f1: 0.9782, accuracy: 0.9782, batch_loss: 0.0668, loss: 0.0660 ||:  89%|########8 | 6377/7188 [12:53<03:28,  3.90it/s]
2022-03-19 20:16:02,843 - INFO - tqdm - f1: 0.9781, accuracy: 0.9781, batch_loss: 0.0039, loss: 0.0661 ||:  90%|######### | 6472/7188 [13:03<00:49, 14.50it/s]
2022-03-19 20:16:12,926 - INFO - tqdm - f1: 0.9782, accuracy: 0.9782, batch_loss: 0.0615, loss: 0.0659 ||:  91%|#########1| 6562/7188 [13:13<00:56, 11.12it/s]
2022-03-19 20:16:23,001 - INFO - tqdm - f1: 0.9781, accuracy: 0.9781, batch_loss: 0.0036, loss: 0.0659 ||:  93%|#########2| 6650/7188 [13:23<01:43,  5.19it/s]
2022-03-19 20:16:33,549 - INFO - tqdm - f1: 0.9781, accuracy: 0.9781, batch_loss: 0.0938, loss: 0.0660 ||:  94%|#########3| 6748/7188 [13:34<01:49,  4.00it/s]
2022-03-19 20:16:44,428 - INFO - tqdm - f1: 0.9781, accuracy: 0.9781, batch_loss: 0.0025, loss: 0.0662 ||:  95%|#########5| 6855/7188 [13:45<01:19,  4.17it/s]
2022-03-19 20:16:55,540 - INFO - tqdm - f1: 0.9781, accuracy: 0.9781, batch_loss: 0.0077, loss: 0.0662 ||:  97%|#########6| 6961/7188 [13:56<00:56,  4.05it/s]
2022-03-19 20:17:06,538 - INFO - tqdm - f1: 0.9781, accuracy: 0.9781, batch_loss: 0.4761, loss: 0.0662 ||:  98%|#########8| 7058/7188 [14:07<00:30,  4.20it/s]
2022-03-19 20:17:16,292 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0145, loss: 0.0662 ||: 100%|#########9| 7154/7188 [14:17<00:03, 11.33it/s]
2022-03-19 20:17:16,521 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0127, loss: 0.0662 ||: 100%|#########9| 7156/7188 [14:17<00:03, 10.43it/s]
2022-03-19 20:17:17,819 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0631, loss: 0.0662 ||: 100%|#########9| 7158/7188 [14:18<00:07,  3.82it/s]
2022-03-19 20:17:17,993 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0555, loss: 0.0662 ||: 100%|#########9| 7160/7188 [14:18<00:05,  4.77it/s]
2022-03-19 20:17:18,143 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.1399, loss: 0.0662 ||: 100%|#########9| 7162/7188 [14:19<00:04,  5.91it/s]
2022-03-19 20:17:18,284 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0143, loss: 0.0662 ||: 100%|#########9| 7164/7188 [14:19<00:03,  7.17it/s]
2022-03-19 20:17:18,412 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0446, loss: 0.0662 ||: 100%|#########9| 7166/7188 [14:19<00:02,  8.64it/s]
2022-03-19 20:17:18,598 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0442, loss: 0.0662 ||: 100%|#########9| 7168/7188 [14:19<00:02,  9.09it/s]
2022-03-19 20:17:18,817 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0097, loss: 0.0662 ||: 100%|#########9| 7170/7188 [14:19<00:01,  9.10it/s]
2022-03-19 20:17:18,960 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0228, loss: 0.0662 ||: 100%|#########9| 7172/7188 [14:19<00:01, 10.16it/s]
2022-03-19 20:17:19,109 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.1315, loss: 0.0662 ||: 100%|#########9| 7174/7188 [14:19<00:01, 10.97it/s]
2022-03-19 20:17:19,226 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0086, loss: 0.0662 ||: 100%|#########9| 7176/7188 [14:20<00:00, 12.28it/s]
2022-03-19 20:17:19,379 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0085, loss: 0.0662 ||: 100%|#########9| 7178/7188 [14:20<00:00, 12.52it/s]
2022-03-19 20:17:19,508 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0231, loss: 0.0662 ||: 100%|#########9| 7180/7188 [14:20<00:00, 13.27it/s]
2022-03-19 20:17:19,675 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0073, loss: 0.0662 ||: 100%|#########9| 7182/7188 [14:20<00:00, 12.87it/s]
2022-03-19 20:17:19,810 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.0033, loss: 0.0662 ||: 100%|#########9| 7184/7188 [14:20<00:00, 13.39it/s]
2022-03-19 20:17:19,935 - INFO - tqdm - f1: 0.9780, accuracy: 0.9780, batch_loss: 0.1926, loss: 0.0662 ||: 100%|#########9| 7186/7188 [14:20<00:00, 14.08it/s]
2022-03-19 20:17:20,047 - INFO - tqdm - f1: 0.9781, accuracy: 0.9780, batch_loss: 0.0084, loss: 0.0662 ||: 100%|##########| 7188/7188 [14:20<00:00, 15.04it/s]
2022-03-19 20:17:20,088 - INFO - tqdm - f1: 0.9781, accuracy: 0.9780, batch_loss: 0.0084, loss: 0.0662 ||: 100%|##########| 7188/7188 [14:20<00:00,  8.35it/s]
2022-03-19 20:17:20,095 - INFO - allennlp.training.trainer - Validating
2022-03-19 20:17:20,099 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-19 20:17:30,215 - INFO - tqdm - f1: 0.9434, accuracy: 0.9436, batch_loss: 0.0224, loss: 0.2463 ||:  61%|######    | 190/313 [00:10<00:05, 24.31it/s]
2022-03-19 20:17:36,781 - INFO - tqdm - f1: 0.9388, accuracy: 0.9388, batch_loss: 0.0063, loss: 0.2665 ||: 100%|##########| 313/313 [00:16<00:00, 22.12it/s]
2022-03-19 20:17:36,789 - INFO - tqdm - f1: 0.9388, accuracy: 0.9388, batch_loss: 0.0063, loss: 0.2665 ||: 100%|##########| 313/313 [00:16<00:00, 18.76it/s]
2022-03-19 20:17:36,794 - INFO - allennlp.training.trainer - Ran out of patience.  Stopping training.
2022-03-19 20:17:36,796 - INFO - dont_stop_pretraining.training.roberta_checkpointer - loading best weights
2022-03-19 20:17:37,183 - INFO - allennlp.commands.train - The model will be evaluated using the best epoch weights.
2022-03-19 20:17:37,190 - INFO - allennlp.training.util - Iterating over dataset
2022-03-19 20:17:37,192 - INFO - tqdm - 0it [00:00, ?it/s]
2022-03-19 20:17:37,205 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-19 20:17:37,207 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-19 20:17:47,252 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.20 ||: : 183it [00:10, 12.73it/s]
2022-03-19 20:17:58,079 - INFO - tqdm - f1: 0.94, accuracy: 0.94, loss: 0.22 ||: : 392it [00:20,  7.55it/s]
2022-03-19 20:18:00,759 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 3,
  "peak_worker_0_memory_MB": 7573.359375,
  "peak_gpu_0_memory_MB": 0,
  "training_duration": "1:09:30.339412",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_f1": 0.9743458032608032,
  "training_accuracy": 0.9743304347826087,
  "training_loss": 0.0762081927290193,
  "training_worker_0_memory_MB": 7573.359375,
  "training_gpu_0_memory_MB": 0.0,
  "validation_f1": 0.9383758157491684,
  "validation_accuracy": 0.9384,
  "validation_loss": 0.23999796894538147,
  "best_validation_f1": 0.9408459216356277,
  "best_validation_accuracy": 0.9408,
  "best_validation_loss": 0.20680241605130056,
  "test_f1": 0.9375472366809845,
  "test_accuracy": 0.9375,
  "test_loss": 0.21501701290302566
}
2022-03-19 20:18:00,837 - INFO - allennlp.models.archival - archiving weights and vocabulary to model_logs/ag_base/model.tar.gz
