2022-03-21 05:43:38,387 - INFO - allennlp.common.params - random_seed = 314
2022-03-21 05:43:38,390 - INFO - allennlp.common.params - numpy_seed = 314
2022-03-21 05:43:38,391 - INFO - allennlp.common.params - pytorch_seed = 314
2022-03-21 05:43:38,396 - INFO - allennlp.common.checks - Pytorch version: 1.8.0+cu111
2022-03-21 05:43:38,398 - INFO - allennlp.common.params - type = default
2022-03-21 05:43:38,400 - INFO - allennlp.common.params - dataset_reader.type = text_classification_json_with_sampling
2022-03-21 05:43:38,402 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer
2022-03-21 05:43:38,403 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2022-03-21 05:43:38,404 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = roberta-base
2022-03-21 05:43:38,406 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags
2022-03-21 05:43:38,407 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512
2022-03-21 05:43:38,408 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None
2022-03-21 05:43:51,544 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer
2022-03-21 05:43:51,550 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = roberta-base
2022-03-21 05:43:51,551 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True
2022-03-21 05:43:51,553 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = 512
2022-03-21 05:43:51,554 - INFO - allennlp.common.params - dataset_reader.tokenizer.stride = 0
2022-03-21 05:43:51,556 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None
2022-03-21 05:43:51,558 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = 512
2022-03-21 05:43:51,560 - INFO - allennlp.common.params - dataset_reader.sample = None
2022-03-21 05:43:51,561 - INFO - allennlp.common.params - dataset_reader.skip_label_indexing = False
2022-03-21 05:43:51,563 - INFO - allennlp.common.params - train_data_path = datasets/rct-20k/train.jsonl
2022-03-21 05:43:51,565 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f9f08f56150>
2022-03-21 05:43:51,567 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-03-21 05:43:51,569 - INFO - allennlp.common.params - validation_dataset_reader.type = text_classification_json_with_sampling
2022-03-21 05:43:51,571 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer
2022-03-21 05:43:51,572 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2022-03-21 05:43:51,574 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = roberta-base
2022-03-21 05:43:51,575 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags
2022-03-21 05:43:51,577 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512
2022-03-21 05:43:51,578 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None
2022-03-21 05:43:51,581 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.type = pretrained_transformer
2022-03-21 05:43:51,582 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.model_name = roberta-base
2022-03-21 05:43:51,584 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.add_special_tokens = True
2022-03-21 05:43:51,585 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.max_length = 512
2022-03-21 05:43:51,587 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.stride = 0
2022-03-21 05:43:51,588 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.tokenizer_kwargs = None
2022-03-21 05:43:51,590 - INFO - allennlp.common.params - validation_dataset_reader.max_sequence_length = 512
2022-03-21 05:43:51,592 - INFO - allennlp.common.params - validation_dataset_reader.sample = None
2022-03-21 05:43:51,594 - INFO - allennlp.common.params - validation_dataset_reader.skip_label_indexing = False
2022-03-21 05:43:51,595 - INFO - allennlp.common.params - validation_data_path = datasets/rct-20k/dev.jsonl
2022-03-21 05:43:51,596 - INFO - allennlp.common.params - validation_data_loader = None
2022-03-21 05:43:51,598 - INFO - allennlp.common.params - test_data_path = datasets/rct-20k/test.jsonl
2022-03-21 05:43:51,599 - INFO - allennlp.common.params - evaluate_on_test = True
2022-03-21 05:43:51,601 - INFO - allennlp.common.params - batch_weight_key = 
2022-03-21 05:43:51,602 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-21 05:43:51,604 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-21 05:43:51,606 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-21 05:43:51,607 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-21 05:43:51,609 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-21 05:43:51,610 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-21 05:43:51,612 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-21 05:43:51,613 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-21 05:43:51,615 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-21 05:43:51,616 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-21 05:43:51,618 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-21 05:43:51,619 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-21 05:43:51,621 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-21 05:43:51,622 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-21 05:43:51,627 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-21 05:44:01,707 - INFO - tqdm - loading instances: 36616it [00:10, 4261.39it/s]
2022-03-21 05:44:11,744 - INFO - tqdm - loading instances: 69341it [00:20, 4256.65it/s]
2022-03-21 05:44:21,759 - INFO - tqdm - loading instances: 103566it [00:30, 2059.03it/s]
2022-03-21 05:44:31,765 - INFO - tqdm - loading instances: 140732it [00:40, 4268.47it/s]
2022-03-21 05:44:41,828 - INFO - tqdm - loading instances: 177007it [00:50, 4257.75it/s]
2022-03-21 05:44:42,542 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-21 05:44:42,548 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-21 05:44:42,549 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-21 05:44:42,551 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-21 05:44:42,553 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-21 05:44:42,554 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-21 05:44:42,556 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-21 05:44:42,557 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-21 05:44:42,559 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-21 05:44:42,560 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-21 05:44:42,562 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-21 05:44:42,563 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-21 05:44:42,565 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-21 05:44:42,566 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-21 05:44:42,568 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-21 05:44:52,036 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-21 05:44:52,042 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-21 05:44:52,044 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-21 05:44:52,045 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-21 05:44:52,046 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-21 05:44:52,048 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-21 05:44:52,049 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-21 05:44:52,050 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-21 05:44:52,052 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-21 05:44:52,053 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-21 05:44:52,054 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-21 05:44:52,056 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-21 05:44:52,057 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-21 05:44:52,058 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-21 05:44:52,059 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-21 05:44:59,178 - INFO - allennlp.common.params - type = from_instances
2022-03-21 05:44:59,184 - INFO - allennlp.common.params - min_count = None
2022-03-21 05:44:59,185 - INFO - allennlp.common.params - max_vocab_size = None
2022-03-21 05:44:59,187 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-03-21 05:44:59,188 - INFO - allennlp.common.params - pretrained_files = None
2022-03-21 05:44:59,190 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-03-21 05:44:59,191 - INFO - allennlp.common.params - tokens_to_add = None
2022-03-21 05:44:59,192 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-03-21 05:44:59,194 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-03-21 05:44:59,195 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-03-21 05:44:59,197 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-03-21 05:44:59,199 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-03-21 05:45:00,325 - INFO - allennlp.common.params - model.type = basic_classifier_with_f1
2022-03-21 05:45:00,335 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2022-03-21 05:45:00,338 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer
2022-03-21 05:45:00,340 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = roberta-base
2022-03-21 05:45:00,341 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512
2022-03-21 05:45:00,343 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_module = None
2022-03-21 05:45:00,344 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True
2022-03-21 05:45:00,346 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True
2022-03-21 05:45:00,347 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None
2022-03-21 05:45:00,349 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None
2022-03-21 05:45:00,350 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None
2022-03-21 05:45:00,351 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None
2022-03-21 05:45:00,352 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None
2022-03-21 05:45:06,791 - INFO - allennlp.common.params - model.seq2vec_encoder.type = cls_pooler
2022-03-21 05:45:06,797 - INFO - allennlp.common.params - model.seq2vec_encoder.embedding_dim = 768
2022-03-21 05:45:06,800 - INFO - allennlp.common.params - model.seq2vec_encoder.cls_is_last_token = False
2022-03-21 05:45:06,801 - INFO - allennlp.common.params - model.feedforward_layer.input_dim = 768
2022-03-21 05:45:06,803 - INFO - allennlp.common.params - model.feedforward_layer.num_layers = 1
2022-03-21 05:45:06,804 - INFO - allennlp.common.params - model.feedforward_layer.hidden_dims = 768
2022-03-21 05:45:06,806 - INFO - allennlp.common.params - model.feedforward_layer.activations = tanh
2022-03-21 05:45:06,807 - INFO - allennlp.common.params - type = tanh
2022-03-21 05:45:06,809 - INFO - allennlp.common.params - model.feedforward_layer.dropout = 0.0
2022-03-21 05:45:06,818 - INFO - allennlp.common.params - model.seq2seq_encoder = None
2022-03-21 05:45:06,819 - INFO - allennlp.common.params - model.dropout = 0.1
2022-03-21 05:45:06,821 - INFO - allennlp.common.params - model.num_labels = None
2022-03-21 05:45:06,822 - INFO - allennlp.common.params - model.label_namespace = labels
2022-03-21 05:45:06,823 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f9f08f66b90>
2022-03-21 05:45:06,825 - INFO - allennlp.common.params - model.regularizer = None
2022-03-21 05:45:06,826 - INFO - allennlp.common.params - model.track_weights = False
2022-03-21 05:45:06,828 - INFO - allennlp.common.params - model.disable_layers = []
2022-03-21 05:45:06,830 - INFO - allennlp.nn.initializers - Initializing parameters
2022-03-21 05:45:06,833 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-03-21 05:45:06,834 - INFO - allennlp.nn.initializers -    _classification_layer.bias
2022-03-21 05:45:06,836 - INFO - allennlp.nn.initializers -    _classification_layer.weight
2022-03-21 05:45:06,837 - INFO - allennlp.nn.initializers -    _feedforward_layer._linear_layers.0.bias
2022-03-21 05:45:06,838 - INFO - allennlp.nn.initializers -    _feedforward_layer._linear_layers.0.weight
2022-03-21 05:45:06,840 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias
2022-03-21 05:45:06,841 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight
2022-03-21 05:45:06,842 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight
2022-03-21 05:45:06,844 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight
2022-03-21 05:45:06,845 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight
2022-03-21 05:45:06,846 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2022-03-21 05:45:06,848 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2022-03-21 05:45:06,849 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias
2022-03-21 05:45:06,851 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight
2022-03-21 05:45:06,852 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias
2022-03-21 05:45:06,853 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight
2022-03-21 05:45:06,855 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias
2022-03-21 05:45:06,857 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight
2022-03-21 05:45:06,858 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias
2022-03-21 05:45:06,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight
2022-03-21 05:45:06,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias
2022-03-21 05:45:06,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight
2022-03-21 05:45:06,866 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias
2022-03-21 05:45:06,867 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight
2022-03-21 05:45:06,868 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias
2022-03-21 05:45:06,870 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight
2022-03-21 05:45:06,871 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2022-03-21 05:45:06,872 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2022-03-21 05:45:06,874 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias
2022-03-21 05:45:06,875 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight
2022-03-21 05:45:06,876 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias
2022-03-21 05:45:06,878 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight
2022-03-21 05:45:06,879 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias
2022-03-21 05:45:06,880 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight
2022-03-21 05:45:06,882 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias
2022-03-21 05:45:06,883 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight
2022-03-21 05:45:06,885 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias
2022-03-21 05:45:06,886 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight
2022-03-21 05:45:06,887 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias
2022-03-21 05:45:06,889 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight
2022-03-21 05:45:06,890 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias
2022-03-21 05:45:06,891 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight
2022-03-21 05:45:06,893 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2022-03-21 05:45:06,895 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2022-03-21 05:45:06,896 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias
2022-03-21 05:45:06,897 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight
2022-03-21 05:45:06,899 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias
2022-03-21 05:45:06,900 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight
2022-03-21 05:45:06,901 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias
2022-03-21 05:45:06,903 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight
2022-03-21 05:45:06,904 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias
2022-03-21 05:45:06,905 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight
2022-03-21 05:45:06,907 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias
2022-03-21 05:45:06,912 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight
2022-03-21 05:45:06,913 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias
2022-03-21 05:45:06,915 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight
2022-03-21 05:45:06,916 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias
2022-03-21 05:45:06,917 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight
2022-03-21 05:45:06,919 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2022-03-21 05:45:06,920 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2022-03-21 05:45:06,921 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias
2022-03-21 05:45:06,923 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight
2022-03-21 05:45:06,924 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias
2022-03-21 05:45:06,925 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight
2022-03-21 05:45:06,926 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias
2022-03-21 05:45:06,928 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight
2022-03-21 05:45:06,929 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias
2022-03-21 05:45:06,931 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight
2022-03-21 05:45:06,933 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias
2022-03-21 05:45:06,934 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight
2022-03-21 05:45:06,935 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias
2022-03-21 05:45:06,936 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight
2022-03-21 05:45:06,938 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias
2022-03-21 05:45:06,939 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight
2022-03-21 05:45:06,940 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2022-03-21 05:45:06,942 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2022-03-21 05:45:06,943 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias
2022-03-21 05:45:06,944 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight
2022-03-21 05:45:06,946 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias
2022-03-21 05:45:06,947 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight
2022-03-21 05:45:06,948 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias
2022-03-21 05:45:06,949 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight
2022-03-21 05:45:06,951 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias
2022-03-21 05:45:06,952 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight
2022-03-21 05:45:06,953 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias
2022-03-21 05:45:06,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight
2022-03-21 05:45:06,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias
2022-03-21 05:45:06,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight
2022-03-21 05:45:06,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias
2022-03-21 05:45:06,964 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight
2022-03-21 05:45:06,965 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2022-03-21 05:45:06,967 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2022-03-21 05:45:06,968 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias
2022-03-21 05:45:06,970 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight
2022-03-21 05:45:06,971 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias
2022-03-21 05:45:06,972 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight
2022-03-21 05:45:06,974 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias
2022-03-21 05:45:06,975 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight
2022-03-21 05:45:06,976 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias
2022-03-21 05:45:06,978 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight
2022-03-21 05:45:06,979 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias
2022-03-21 05:45:06,980 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight
2022-03-21 05:45:06,981 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias
2022-03-21 05:45:06,983 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight
2022-03-21 05:45:06,984 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias
2022-03-21 05:45:06,986 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight
2022-03-21 05:45:06,987 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2022-03-21 05:45:06,990 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2022-03-21 05:45:06,992 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias
2022-03-21 05:45:06,994 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight
2022-03-21 05:45:06,997 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias
2022-03-21 05:45:06,998 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight
2022-03-21 05:45:07,000 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias
2022-03-21 05:45:07,001 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight
2022-03-21 05:45:07,002 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias
2022-03-21 05:45:07,004 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight
2022-03-21 05:45:07,006 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias
2022-03-21 05:45:07,007 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight
2022-03-21 05:45:07,008 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias
2022-03-21 05:45:07,009 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight
2022-03-21 05:45:07,012 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias
2022-03-21 05:45:07,013 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight
2022-03-21 05:45:07,014 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2022-03-21 05:45:07,015 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2022-03-21 05:45:07,017 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias
2022-03-21 05:45:07,018 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight
2022-03-21 05:45:07,019 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias
2022-03-21 05:45:07,020 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight
2022-03-21 05:45:07,022 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias
2022-03-21 05:45:07,023 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight
2022-03-21 05:45:07,024 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias
2022-03-21 05:45:07,025 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight
2022-03-21 05:45:07,027 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias
2022-03-21 05:45:07,028 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight
2022-03-21 05:45:07,029 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias
2022-03-21 05:45:07,031 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight
2022-03-21 05:45:07,032 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias
2022-03-21 05:45:07,033 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight
2022-03-21 05:45:07,034 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2022-03-21 05:45:07,036 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2022-03-21 05:45:07,037 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias
2022-03-21 05:45:07,039 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight
2022-03-21 05:45:07,040 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias
2022-03-21 05:45:07,041 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight
2022-03-21 05:45:07,043 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias
2022-03-21 05:45:07,044 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight
2022-03-21 05:45:07,045 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias
2022-03-21 05:45:07,046 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight
2022-03-21 05:45:07,047 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias
2022-03-21 05:45:07,049 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight
2022-03-21 05:45:07,050 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias
2022-03-21 05:45:07,051 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight
2022-03-21 05:45:07,052 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias
2022-03-21 05:45:07,054 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight
2022-03-21 05:45:07,055 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2022-03-21 05:45:07,056 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2022-03-21 05:45:07,057 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias
2022-03-21 05:45:07,059 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight
2022-03-21 05:45:07,060 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias
2022-03-21 05:45:07,061 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight
2022-03-21 05:45:07,063 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias
2022-03-21 05:45:07,064 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight
2022-03-21 05:45:07,065 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias
2022-03-21 05:45:07,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight
2022-03-21 05:45:07,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias
2022-03-21 05:45:07,070 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight
2022-03-21 05:45:07,071 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias
2022-03-21 05:45:07,073 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight
2022-03-21 05:45:07,074 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias
2022-03-21 05:45:07,075 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight
2022-03-21 05:45:07,076 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2022-03-21 05:45:07,078 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2022-03-21 05:45:07,079 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias
2022-03-21 05:45:07,080 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight
2022-03-21 05:45:07,081 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias
2022-03-21 05:45:07,083 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight
2022-03-21 05:45:07,084 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias
2022-03-21 05:45:07,085 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight
2022-03-21 05:45:07,087 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias
2022-03-21 05:45:07,088 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight
2022-03-21 05:45:07,089 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias
2022-03-21 05:45:07,091 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight
2022-03-21 05:45:07,092 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias
2022-03-21 05:45:07,094 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight
2022-03-21 05:45:07,095 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias
2022-03-21 05:45:07,096 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight
2022-03-21 05:45:07,098 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2022-03-21 05:45:07,099 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2022-03-21 05:45:07,101 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias
2022-03-21 05:45:07,102 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight
2022-03-21 05:45:07,104 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias
2022-03-21 05:45:07,105 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight
2022-03-21 05:45:07,106 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias
2022-03-21 05:45:07,108 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight
2022-03-21 05:45:07,109 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias
2022-03-21 05:45:07,110 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight
2022-03-21 05:45:07,112 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias
2022-03-21 05:45:07,113 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight
2022-03-21 05:45:07,114 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias
2022-03-21 05:45:07,116 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight
2022-03-21 05:45:07,117 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias
2022-03-21 05:45:07,119 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight
2022-03-21 05:45:07,120 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias
2022-03-21 05:45:07,121 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight
2022-03-21 05:45:16,692 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-03-21 05:45:16,699 - INFO - allennlp.common.params - trainer.patience = 3
2022-03-21 05:45:16,700 - INFO - allennlp.common.params - trainer.validation_metric = +f1
2022-03-21 05:45:16,702 - INFO - allennlp.common.params - trainer.num_epochs = 10
2022-03-21 05:45:16,704 - INFO - allennlp.common.params - trainer.cuda_device = 2
2022-03-21 05:45:16,705 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-03-21 05:45:16,706 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-03-21 05:45:16,708 - INFO - allennlp.common.params - trainer.distributed = False
2022-03-21 05:45:16,709 - INFO - allennlp.common.params - trainer.world_size = 1
2022-03-21 05:45:16,711 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1
2022-03-21 05:45:16,713 - INFO - allennlp.common.params - trainer.use_amp = False
2022-03-21 05:45:16,715 - INFO - allennlp.common.params - trainer.no_grad = None
2022-03-21 05:45:16,716 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2022-03-21 05:45:16,718 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-03-21 05:45:16,719 - INFO - allennlp.common.params - trainer.moving_average = None
2022-03-21 05:45:16,721 - INFO - allennlp.common.params - trainer.callbacks = None
2022-03-21 05:45:16,724 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True
2022-03-21 05:45:21,351 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw_str_lr
2022-03-21 05:45:21,353 - INFO - allennlp.common.params - trainer.optimizer.lr = 2e-05
2022-03-21 05:45:21,355 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-03-21 05:45:21,357 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-03-21 05:45:21,358 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-03-21 05:45:21,360 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = False
2022-03-21 05:45:21,363 - INFO - allennlp.training.optimizers - Done constructing parameter groups.
2022-03-21 05:45:21,364 - INFO - allennlp.training.optimizers - Group 0: ['_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias', '_classification_layer.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias', '_feedforward_layer._linear_layers.0.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias'], {'weight_decay': 0}
2022-03-21 05:45:21,367 - INFO - allennlp.training.optimizers - Group 1: ['_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight', '_classification_layer.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight', '_feedforward_layer._linear_layers.0.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight'], {}
2022-03-21 05:45:21,369 - WARNING - allennlp.training.optimizers - When constructing parameter groups, layer_norm.weight does not match any parameter name
2022-03-21 05:45:21,371 - INFO - allennlp.training.optimizers - Number of trainable parameters: 125240069
2022-03-21 05:45:21,373 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-03-21 05:45:21,375 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-03-21 05:45:21,377 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight
2022-03-21 05:45:21,378 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight
2022-03-21 05:45:21,379 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight
2022-03-21 05:45:21,381 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight
2022-03-21 05:45:21,382 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias
2022-03-21 05:45:21,384 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight
2022-03-21 05:45:21,385 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias
2022-03-21 05:45:21,387 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight
2022-03-21 05:45:21,388 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias
2022-03-21 05:45:21,389 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight
2022-03-21 05:45:21,391 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias
2022-03-21 05:45:21,392 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight
2022-03-21 05:45:21,393 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias
2022-03-21 05:45:21,395 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2022-03-21 05:45:21,396 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2022-03-21 05:45:21,399 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight
2022-03-21 05:45:21,400 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias
2022-03-21 05:45:21,401 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight
2022-03-21 05:45:21,403 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias
2022-03-21 05:45:21,404 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight
2022-03-21 05:45:21,405 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias
2022-03-21 05:45:21,406 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight
2022-03-21 05:45:21,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias
2022-03-21 05:45:21,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight
2022-03-21 05:45:21,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias
2022-03-21 05:45:21,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight
2022-03-21 05:45:21,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias
2022-03-21 05:45:21,416 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight
2022-03-21 05:45:21,417 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias
2022-03-21 05:45:21,419 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2022-03-21 05:45:21,421 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2022-03-21 05:45:21,422 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight
2022-03-21 05:45:21,424 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias
2022-03-21 05:45:21,425 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight
2022-03-21 05:45:21,427 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias
2022-03-21 05:45:21,428 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight
2022-03-21 05:45:21,429 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias
2022-03-21 05:45:21,431 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight
2022-03-21 05:45:21,432 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias
2022-03-21 05:45:21,434 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight
2022-03-21 05:45:21,435 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias
2022-03-21 05:45:21,437 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight
2022-03-21 05:45:21,439 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias
2022-03-21 05:45:21,440 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight
2022-03-21 05:45:21,442 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias
2022-03-21 05:45:21,443 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2022-03-21 05:45:21,445 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2022-03-21 05:45:21,446 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight
2022-03-21 05:45:21,447 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias
2022-03-21 05:45:21,448 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight
2022-03-21 05:45:21,450 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias
2022-03-21 05:45:21,451 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight
2022-03-21 05:45:21,453 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias
2022-03-21 05:45:21,454 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight
2022-03-21 05:45:21,455 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias
2022-03-21 05:45:21,457 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight
2022-03-21 05:45:21,458 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias
2022-03-21 05:45:21,460 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight
2022-03-21 05:45:21,461 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias
2022-03-21 05:45:21,462 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight
2022-03-21 05:45:21,464 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias
2022-03-21 05:45:21,465 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2022-03-21 05:45:21,467 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2022-03-21 05:45:21,468 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight
2022-03-21 05:45:21,469 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias
2022-03-21 05:45:21,471 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight
2022-03-21 05:45:21,472 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias
2022-03-21 05:45:21,475 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight
2022-03-21 05:45:21,476 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias
2022-03-21 05:45:21,477 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight
2022-03-21 05:45:21,479 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias
2022-03-21 05:45:21,480 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight
2022-03-21 05:45:21,481 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias
2022-03-21 05:45:21,483 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight
2022-03-21 05:45:21,484 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias
2022-03-21 05:45:21,485 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight
2022-03-21 05:45:21,487 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias
2022-03-21 05:45:21,489 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2022-03-21 05:45:21,490 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2022-03-21 05:45:21,492 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight
2022-03-21 05:45:21,493 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias
2022-03-21 05:45:21,494 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight
2022-03-21 05:45:21,496 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias
2022-03-21 05:45:21,497 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight
2022-03-21 05:45:21,499 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias
2022-03-21 05:45:21,500 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight
2022-03-21 05:45:21,503 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias
2022-03-21 05:45:21,505 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight
2022-03-21 05:45:21,506 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias
2022-03-21 05:45:21,508 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight
2022-03-21 05:45:21,509 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias
2022-03-21 05:45:21,510 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight
2022-03-21 05:45:21,512 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias
2022-03-21 05:45:21,514 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2022-03-21 05:45:21,516 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2022-03-21 05:45:21,517 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight
2022-03-21 05:45:21,519 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias
2022-03-21 05:45:21,520 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight
2022-03-21 05:45:21,522 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias
2022-03-21 05:45:21,523 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight
2022-03-21 05:45:21,525 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias
2022-03-21 05:45:21,526 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight
2022-03-21 05:45:21,528 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias
2022-03-21 05:45:21,529 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight
2022-03-21 05:45:21,530 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias
2022-03-21 05:45:21,532 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight
2022-03-21 05:45:21,533 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias
2022-03-21 05:45:21,535 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight
2022-03-21 05:45:21,536 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias
2022-03-21 05:45:21,538 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2022-03-21 05:45:21,539 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2022-03-21 05:45:21,541 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight
2022-03-21 05:45:21,542 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias
2022-03-21 05:45:21,544 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight
2022-03-21 05:45:21,545 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias
2022-03-21 05:45:21,547 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight
2022-03-21 05:45:21,548 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias
2022-03-21 05:45:21,550 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight
2022-03-21 05:45:21,552 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias
2022-03-21 05:45:21,554 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight
2022-03-21 05:45:21,555 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias
2022-03-21 05:45:21,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight
2022-03-21 05:45:21,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias
2022-03-21 05:45:21,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight
2022-03-21 05:45:21,561 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias
2022-03-21 05:45:21,563 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2022-03-21 05:45:21,564 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2022-03-21 05:45:21,565 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight
2022-03-21 05:45:21,567 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias
2022-03-21 05:45:21,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight
2022-03-21 05:45:21,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias
2022-03-21 05:45:21,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight
2022-03-21 05:45:21,573 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias
2022-03-21 05:45:21,574 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight
2022-03-21 05:45:21,575 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias
2022-03-21 05:45:21,577 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight
2022-03-21 05:45:21,578 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias
2022-03-21 05:45:21,580 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight
2022-03-21 05:45:21,581 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias
2022-03-21 05:45:21,582 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight
2022-03-21 05:45:21,583 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias
2022-03-21 05:45:21,585 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2022-03-21 05:45:21,586 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2022-03-21 05:45:21,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight
2022-03-21 05:45:21,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias
2022-03-21 05:45:21,591 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight
2022-03-21 05:45:21,592 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias
2022-03-21 05:45:21,594 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight
2022-03-21 05:45:21,595 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias
2022-03-21 05:45:21,596 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight
2022-03-21 05:45:21,598 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias
2022-03-21 05:45:21,599 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight
2022-03-21 05:45:21,601 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias
2022-03-21 05:45:21,602 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight
2022-03-21 05:45:21,603 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias
2022-03-21 05:45:21,609 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight
2022-03-21 05:45:21,613 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias
2022-03-21 05:45:21,615 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2022-03-21 05:45:21,616 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2022-03-21 05:45:21,618 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight
2022-03-21 05:45:21,619 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias
2022-03-21 05:45:21,621 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight
2022-03-21 05:45:21,622 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias
2022-03-21 05:45:21,624 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight
2022-03-21 05:45:21,625 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias
2022-03-21 05:45:21,627 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight
2022-03-21 05:45:21,628 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias
2022-03-21 05:45:21,630 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight
2022-03-21 05:45:21,631 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias
2022-03-21 05:45:21,633 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight
2022-03-21 05:45:21,635 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias
2022-03-21 05:45:21,636 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight
2022-03-21 05:45:21,638 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias
2022-03-21 05:45:21,639 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2022-03-21 05:45:21,641 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2022-03-21 05:45:21,642 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight
2022-03-21 05:45:21,644 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias
2022-03-21 05:45:21,645 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight
2022-03-21 05:45:21,646 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias
2022-03-21 05:45:21,648 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight
2022-03-21 05:45:21,649 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias
2022-03-21 05:45:21,651 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight
2022-03-21 05:45:21,652 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias
2022-03-21 05:45:21,653 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight
2022-03-21 05:45:21,655 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias
2022-03-21 05:45:21,656 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight
2022-03-21 05:45:21,659 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias
2022-03-21 05:45:21,660 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight
2022-03-21 05:45:21,662 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias
2022-03-21 05:45:21,663 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2022-03-21 05:45:21,665 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2022-03-21 05:45:21,666 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight
2022-03-21 05:45:21,667 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias
2022-03-21 05:45:21,669 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight
2022-03-21 05:45:21,670 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias
2022-03-21 05:45:21,671 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight
2022-03-21 05:45:21,674 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias
2022-03-21 05:45:21,675 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight
2022-03-21 05:45:21,676 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias
2022-03-21 05:45:21,678 - INFO - allennlp.common.util - _feedforward_layer._linear_layers.0.weight
2022-03-21 05:45:21,679 - INFO - allennlp.common.util - _feedforward_layer._linear_layers.0.bias
2022-03-21 05:45:21,681 - INFO - allennlp.common.util - _classification_layer.weight
2022-03-21 05:45:21,682 - INFO - allennlp.common.util - _classification_layer.bias
2022-03-21 05:45:21,684 - INFO - allennlp.common.params - trainer.checkpointer.type = roberta_default
2022-03-21 05:45:21,685 - INFO - allennlp.common.params - trainer.checkpointer.keep_serialized_model_every_num_seconds = None
2022-03-21 05:45:21,687 - INFO - allennlp.common.params - trainer.checkpointer.num_serialized_models_to_keep = 0
2022-03-21 05:45:21,688 - INFO - allennlp.common.params - trainer.checkpointer.model_save_interval = None
2022-03-21 05:45:21,690 - INFO - allennlp.common.params - trainer.checkpointer.num_epochs = 10
2022-03-21 05:45:21,691 - INFO - allennlp.common.params - trainer.checkpointer.skip_early_stopping = False
2022-03-21 05:45:21,696 - INFO - allennlp.training.trainer - Beginning training.
2022-03-21 05:45:21,698 - INFO - allennlp.training.trainer - Epoch 0/9
2022-03-21 05:45:21,699 - INFO - allennlp.training.trainer - Worker 0 memory usage: 8.3G
2022-03-21 05:45:21,701 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 05:45:21,703 - INFO - allennlp.training.trainer - Training
2022-03-21 05:45:21,705 - INFO - tqdm - 0%|          | 0/11253 [00:00<?, ?it/s]
2022-03-21 05:45:21,881 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-21 05:45:21,883 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-21 05:45:31,815 - INFO - tqdm - f1: 0.4848, accuracy: 0.6348, batch_loss: 0.8124, loss: 0.9310 ||:   1%|1         | 121/11253 [00:10<12:04, 15.36it/s]
2022-03-21 05:45:41,932 - INFO - tqdm - f1: 0.6155, accuracy: 0.7186, batch_loss: 0.7713, loss: 0.7385 ||:   2%|2         | 271/11253 [00:20<12:12, 15.00it/s]
2022-03-21 05:45:52,050 - INFO - tqdm - f1: 0.6625, accuracy: 0.7521, batch_loss: 0.4426, loss: 0.6663 ||:   4%|3         | 421/11253 [00:30<12:17, 14.68it/s]
2022-03-21 05:46:02,143 - INFO - tqdm - f1: 0.6956, accuracy: 0.7729, batch_loss: 0.4778, loss: 0.6178 ||:   5%|5         | 571/11253 [00:40<11:52, 14.99it/s]
2022-03-21 05:46:12,197 - INFO - tqdm - f1: 0.7087, accuracy: 0.7845, batch_loss: 0.2320, loss: 0.5861 ||:   6%|6         | 719/11253 [00:50<11:35, 15.14it/s]
2022-03-21 05:46:22,202 - INFO - tqdm - f1: 0.7162, accuracy: 0.7915, batch_loss: 0.2135, loss: 0.5661 ||:   8%|7         | 867/11253 [01:00<11:49, 14.63it/s]
2022-03-21 05:46:32,267 - INFO - tqdm - f1: 0.7205, accuracy: 0.7961, batch_loss: 0.3935, loss: 0.5530 ||:   9%|9         | 1013/11253 [01:10<11:16, 15.13it/s]
2022-03-21 05:46:42,342 - INFO - tqdm - f1: 0.7284, accuracy: 0.8010, batch_loss: 0.1167, loss: 0.5400 ||:  10%|#         | 1161/11253 [01:20<11:18, 14.87it/s]
2022-03-21 05:46:52,344 - INFO - tqdm - f1: 0.7316, accuracy: 0.8050, batch_loss: 0.3453, loss: 0.5279 ||:  12%|#1        | 1311/11253 [01:30<11:07, 14.89it/s]
2022-03-21 05:47:02,410 - INFO - tqdm - f1: 0.7369, accuracy: 0.8095, batch_loss: 0.4438, loss: 0.5170 ||:  13%|#2        | 1461/11253 [01:40<10:16, 15.87it/s]
2022-03-21 05:47:12,493 - INFO - tqdm - f1: 0.7389, accuracy: 0.8116, batch_loss: 0.5781, loss: 0.5097 ||:  14%|#4        | 1611/11253 [01:50<10:37, 15.12it/s]
2022-03-21 05:47:22,541 - INFO - tqdm - f1: 0.7429, accuracy: 0.8145, batch_loss: 0.5533, loss: 0.5033 ||:  16%|#5        | 1761/11253 [02:00<10:17, 15.37it/s]
2022-03-21 05:47:32,578 - INFO - tqdm - f1: 0.7471, accuracy: 0.8169, batch_loss: 0.4506, loss: 0.4963 ||:  17%|#6        | 1911/11253 [02:10<10:30, 14.83it/s]
2022-03-21 05:47:42,704 - INFO - tqdm - f1: 0.7498, accuracy: 0.8193, batch_loss: 0.2032, loss: 0.4904 ||:  18%|#8        | 2061/11253 [02:20<10:31, 14.56it/s]
2022-03-21 05:47:52,771 - INFO - tqdm - f1: 0.7510, accuracy: 0.8202, batch_loss: 0.3272, loss: 0.4876 ||:  20%|#9        | 2211/11253 [02:31<10:28, 14.39it/s]
2022-03-21 05:48:02,878 - INFO - tqdm - f1: 0.7530, accuracy: 0.8217, batch_loss: 0.7678, loss: 0.4838 ||:  21%|##        | 2363/11253 [02:41<10:35, 13.99it/s]
2022-03-21 05:48:12,898 - INFO - tqdm - f1: 0.7552, accuracy: 0.8236, batch_loss: 0.3528, loss: 0.4793 ||:  22%|##2       | 2515/11253 [02:51<10:11, 14.29it/s]
2022-03-21 05:48:22,919 - INFO - tqdm - f1: 0.7573, accuracy: 0.8253, batch_loss: 0.1454, loss: 0.4758 ||:  24%|##3       | 2657/11253 [03:01<10:49, 13.23it/s]
2022-03-21 05:48:32,956 - INFO - tqdm - f1: 0.7591, accuracy: 0.8264, batch_loss: 0.1760, loss: 0.4737 ||:  25%|##4       | 2799/11253 [03:11<09:31, 14.79it/s]
2022-03-21 05:48:42,970 - INFO - tqdm - f1: 0.7603, accuracy: 0.8279, batch_loss: 0.3378, loss: 0.4705 ||:  26%|##6       | 2947/11253 [03:21<09:21, 14.79it/s]
2022-03-21 05:48:53,072 - INFO - tqdm - f1: 0.7621, accuracy: 0.8291, batch_loss: 0.6139, loss: 0.4670 ||:  27%|##7       | 3087/11253 [03:31<10:29, 12.97it/s]
2022-03-21 05:49:03,093 - INFO - tqdm - f1: 0.7633, accuracy: 0.8300, batch_loss: 0.5138, loss: 0.4651 ||:  29%|##8       | 3217/11253 [03:41<10:32, 12.71it/s]
2022-03-21 05:49:13,180 - INFO - tqdm - f1: 0.7641, accuracy: 0.8307, batch_loss: 0.4562, loss: 0.4629 ||:  30%|##9       | 3353/11253 [03:51<09:01, 14.58it/s]
2022-03-21 05:49:23,223 - INFO - tqdm - f1: 0.7648, accuracy: 0.8316, batch_loss: 0.5613, loss: 0.4602 ||:  31%|###1      | 3495/11253 [04:01<08:15, 15.65it/s]
2022-03-21 05:49:33,314 - INFO - tqdm - f1: 0.7655, accuracy: 0.8323, batch_loss: 0.6715, loss: 0.4575 ||:  32%|###2      | 3643/11253 [04:11<08:45, 14.48it/s]
2022-03-21 05:49:43,352 - INFO - tqdm - f1: 0.7664, accuracy: 0.8331, batch_loss: 0.5483, loss: 0.4558 ||:  34%|###3      | 3789/11253 [04:21<08:05, 15.37it/s]
2022-03-21 05:49:53,485 - INFO - tqdm - f1: 0.7672, accuracy: 0.8338, batch_loss: 0.3362, loss: 0.4539 ||:  35%|###4      | 3937/11253 [04:31<08:27, 14.41it/s]
2022-03-21 05:50:03,531 - INFO - tqdm - f1: 0.7679, accuracy: 0.8341, batch_loss: 0.3213, loss: 0.4532 ||:  36%|###6      | 4085/11253 [04:41<07:50, 15.23it/s]
2022-03-21 05:50:13,648 - INFO - tqdm - f1: 0.7693, accuracy: 0.8348, batch_loss: 0.3385, loss: 0.4512 ||:  38%|###7      | 4237/11253 [04:51<07:51, 14.89it/s]
2022-03-21 05:50:23,752 - INFO - tqdm - f1: 0.7702, accuracy: 0.8354, batch_loss: 0.0829, loss: 0.4499 ||:  39%|###8      | 4385/11253 [05:02<08:29, 13.47it/s]
2022-03-21 05:50:33,858 - INFO - tqdm - f1: 0.7713, accuracy: 0.8363, batch_loss: 0.5537, loss: 0.4479 ||:  40%|####      | 4531/11253 [05:12<07:56, 14.09it/s]
2022-03-21 05:50:43,901 - INFO - tqdm - f1: 0.7723, accuracy: 0.8368, batch_loss: 0.2751, loss: 0.4469 ||:  42%|####1     | 4679/11253 [05:22<07:28, 14.65it/s]
2022-03-21 05:50:53,999 - INFO - tqdm - f1: 0.7732, accuracy: 0.8376, batch_loss: 0.1216, loss: 0.4462 ||:  43%|####2     | 4827/11253 [05:32<07:28, 14.33it/s]
2022-03-21 05:51:04,120 - INFO - tqdm - f1: 0.7742, accuracy: 0.8384, batch_loss: 0.5760, loss: 0.4439 ||:  44%|####4     | 4979/11253 [05:42<06:58, 14.98it/s]
2022-03-21 05:51:14,234 - INFO - tqdm - f1: 0.7744, accuracy: 0.8386, batch_loss: 0.4405, loss: 0.4433 ||:  46%|####5     | 5133/11253 [05:52<06:53, 14.80it/s]
2022-03-21 05:51:24,344 - INFO - tqdm - f1: 0.7756, accuracy: 0.8395, batch_loss: 0.2359, loss: 0.4410 ||:  47%|####6     | 5277/11253 [06:02<07:08, 13.95it/s]
2022-03-21 05:51:34,438 - INFO - tqdm - f1: 0.7760, accuracy: 0.8396, batch_loss: 0.6940, loss: 0.4405 ||:  48%|####8     | 5425/11253 [06:12<06:30, 14.94it/s]
2022-03-21 05:51:44,513 - INFO - tqdm - f1: 0.7761, accuracy: 0.8401, batch_loss: 0.3984, loss: 0.4389 ||:  50%|####9     | 5573/11253 [06:22<06:33, 14.42it/s]
2022-03-21 05:51:54,550 - INFO - tqdm - f1: 0.7766, accuracy: 0.8406, batch_loss: 0.1603, loss: 0.4371 ||:  51%|#####     | 5721/11253 [06:32<06:13, 14.81it/s]
2022-03-21 05:52:04,614 - INFO - tqdm - f1: 0.7775, accuracy: 0.8413, batch_loss: 0.1849, loss: 0.4358 ||:  52%|#####2    | 5869/11253 [06:42<05:47, 15.49it/s]
2022-03-21 05:52:14,751 - INFO - tqdm - f1: 0.7777, accuracy: 0.8415, batch_loss: 0.1802, loss: 0.4350 ||:  53%|#####3    | 6015/11253 [06:53<06:02, 14.44it/s]
2022-03-21 05:52:24,787 - INFO - tqdm - f1: 0.7782, accuracy: 0.8420, batch_loss: 0.3693, loss: 0.4342 ||:  55%|#####4    | 6165/11253 [07:03<05:40, 14.94it/s]
2022-03-21 05:52:34,806 - INFO - tqdm - f1: 0.7790, accuracy: 0.8427, batch_loss: 0.0505, loss: 0.4322 ||:  56%|#####6    | 6313/11253 [07:13<05:39, 14.56it/s]
2022-03-21 05:52:44,825 - INFO - tqdm - f1: 0.7791, accuracy: 0.8429, batch_loss: 0.4195, loss: 0.4311 ||:  57%|#####7    | 6463/11253 [07:23<05:23, 14.79it/s]
2022-03-21 05:52:54,941 - INFO - tqdm - f1: 0.7792, accuracy: 0.8431, batch_loss: 0.2855, loss: 0.4305 ||:  59%|#####8    | 6609/11253 [07:33<05:13, 14.83it/s]
2022-03-21 05:53:05,033 - INFO - tqdm - f1: 0.7795, accuracy: 0.8435, batch_loss: 0.5745, loss: 0.4295 ||:  60%|######    | 6759/11253 [07:43<05:17, 14.17it/s]
2022-03-21 05:53:15,141 - INFO - tqdm - f1: 0.7799, accuracy: 0.8439, batch_loss: 0.1647, loss: 0.4288 ||:  61%|######1   | 6907/11253 [07:53<05:10, 14.00it/s]
2022-03-21 05:53:25,205 - INFO - tqdm - f1: 0.7805, accuracy: 0.8442, batch_loss: 0.4654, loss: 0.4281 ||:  63%|######2   | 7057/11253 [08:03<04:31, 15.46it/s]
2022-03-21 05:53:35,318 - INFO - tqdm - f1: 0.7810, accuracy: 0.8447, batch_loss: 0.7738, loss: 0.4273 ||:  64%|######4   | 7207/11253 [08:13<04:37, 14.56it/s]
2022-03-21 05:53:45,458 - INFO - tqdm - f1: 0.7817, accuracy: 0.8452, batch_loss: 0.8634, loss: 0.4262 ||:  65%|######5   | 7353/11253 [08:23<04:21, 14.94it/s]
2022-03-21 05:53:55,586 - INFO - tqdm - f1: 0.7819, accuracy: 0.8454, batch_loss: 0.3539, loss: 0.4256 ||:  67%|######6   | 7505/11253 [08:33<04:03, 15.41it/s]
2022-03-21 05:54:05,688 - INFO - tqdm - f1: 0.7828, accuracy: 0.8460, batch_loss: 0.0161, loss: 0.4244 ||:  68%|######8   | 7655/11253 [08:43<04:01, 14.90it/s]
2022-03-21 05:54:15,747 - INFO - tqdm - f1: 0.7833, accuracy: 0.8465, batch_loss: 0.0895, loss: 0.4235 ||:  69%|######9   | 7807/11253 [08:54<03:47, 15.12it/s]
2022-03-21 05:54:25,775 - INFO - tqdm - f1: 0.7836, accuracy: 0.8468, batch_loss: 0.4588, loss: 0.4225 ||:  71%|#######   | 7953/11253 [09:04<03:47, 14.49it/s]
2022-03-21 05:54:35,886 - INFO - tqdm - f1: 0.7837, accuracy: 0.8471, batch_loss: 0.3361, loss: 0.4219 ||:  72%|#######1  | 8101/11253 [09:14<03:41, 14.25it/s]
2022-03-21 05:54:45,944 - INFO - tqdm - f1: 0.7842, accuracy: 0.8476, batch_loss: 0.0543, loss: 0.4207 ||:  73%|#######3  | 8249/11253 [09:24<03:23, 14.78it/s]
2022-03-21 05:54:56,040 - INFO - tqdm - f1: 0.7844, accuracy: 0.8477, batch_loss: 0.2796, loss: 0.4203 ||:  75%|#######4  | 8401/11253 [09:34<03:13, 14.76it/s]
2022-03-21 05:55:06,125 - INFO - tqdm - f1: 0.7848, accuracy: 0.8480, batch_loss: 0.5608, loss: 0.4197 ||:  76%|#######6  | 8553/11253 [09:44<03:04, 14.61it/s]
2022-03-21 05:55:16,187 - INFO - tqdm - f1: 0.7855, accuracy: 0.8484, batch_loss: 0.4253, loss: 0.4189 ||:  77%|#######7  | 8699/11253 [09:54<02:57, 14.36it/s]
2022-03-21 05:55:26,245 - INFO - tqdm - f1: 0.7857, accuracy: 0.8485, batch_loss: 0.4303, loss: 0.4183 ||:  79%|#######8  | 8849/11253 [10:04<02:39, 15.12it/s]
2022-03-21 05:55:36,301 - INFO - tqdm - f1: 0.7862, accuracy: 0.8490, batch_loss: 0.1173, loss: 0.4174 ||:  80%|#######9  | 8999/11253 [10:14<02:32, 14.76it/s]
2022-03-21 05:55:46,387 - INFO - tqdm - f1: 0.7862, accuracy: 0.8491, batch_loss: 0.3840, loss: 0.4170 ||:  81%|########1 | 9153/11253 [10:24<02:18, 15.20it/s]
2022-03-21 05:55:56,488 - INFO - tqdm - f1: 0.7866, accuracy: 0.8493, batch_loss: 0.3830, loss: 0.4167 ||:  83%|########2 | 9299/11253 [10:34<02:18, 14.06it/s]
2022-03-21 05:56:06,624 - INFO - tqdm - f1: 0.7871, accuracy: 0.8495, batch_loss: 0.2080, loss: 0.4162 ||:  84%|########3 | 9447/11253 [10:44<02:04, 14.48it/s]
2022-03-21 05:56:16,668 - INFO - tqdm - f1: 0.7874, accuracy: 0.8497, batch_loss: 0.2115, loss: 0.4155 ||:  85%|########5 | 9597/11253 [10:54<01:46, 15.61it/s]
2022-03-21 05:56:26,674 - INFO - tqdm - f1: 0.7876, accuracy: 0.8498, batch_loss: 0.2434, loss: 0.4149 ||:  87%|########6 | 9747/11253 [11:04<01:37, 15.52it/s]
2022-03-21 05:56:36,785 - INFO - tqdm - f1: 0.7876, accuracy: 0.8499, batch_loss: 0.2114, loss: 0.4147 ||:  88%|########7 | 9897/11253 [11:15<01:34, 14.42it/s]
2022-03-21 05:56:46,840 - INFO - tqdm - f1: 0.7878, accuracy: 0.8500, batch_loss: 0.5174, loss: 0.4144 ||:  89%|########9 | 10043/11253 [11:25<01:18, 15.46it/s]
2022-03-21 05:56:56,898 - INFO - tqdm - f1: 0.7878, accuracy: 0.8501, batch_loss: 0.9208, loss: 0.4139 ||:  91%|######### | 10193/11253 [11:35<01:09, 15.22it/s]
2022-03-21 05:57:07,008 - INFO - tqdm - f1: 0.7881, accuracy: 0.8504, batch_loss: 0.5266, loss: 0.4134 ||:  92%|#########1| 10343/11253 [11:45<01:02, 14.50it/s]
2022-03-21 05:57:17,134 - INFO - tqdm - f1: 0.7884, accuracy: 0.8506, batch_loss: 0.5296, loss: 0.4128 ||:  93%|#########3| 10491/11253 [11:55<00:51, 14.81it/s]
2022-03-21 05:57:27,181 - INFO - tqdm - f1: 0.7886, accuracy: 0.8507, batch_loss: 0.4760, loss: 0.4125 ||:  95%|#########4| 10637/11253 [12:05<00:44, 13.73it/s]
2022-03-21 05:57:37,241 - INFO - tqdm - f1: 0.7886, accuracy: 0.8507, batch_loss: 0.2578, loss: 0.4123 ||:  96%|#########5| 10783/11253 [12:15<00:33, 14.03it/s]
2022-03-21 05:57:47,269 - INFO - tqdm - f1: 0.7888, accuracy: 0.8509, batch_loss: 0.0106, loss: 0.4117 ||:  97%|#########7| 10933/11253 [12:25<00:22, 13.91it/s]
2022-03-21 05:57:57,341 - INFO - tqdm - f1: 0.7889, accuracy: 0.8511, batch_loss: 0.1357, loss: 0.4113 ||:  99%|#########8| 11085/11253 [12:35<00:11, 15.25it/s]
2022-03-21 05:58:04,947 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.4572, loss: 0.4113 ||: 100%|#########9| 11197/11253 [12:43<00:03, 15.59it/s]
2022-03-21 05:58:05,103 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.3672, loss: 0.4113 ||: 100%|#########9| 11199/11253 [12:43<00:03, 14.65it/s]
2022-03-21 05:58:05,248 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.7740, loss: 0.4113 ||: 100%|#########9| 11201/11253 [12:43<00:03, 14.38it/s]
2022-03-21 05:58:05,383 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.4957, loss: 0.4114 ||: 100%|#########9| 11203/11253 [12:43<00:03, 14.52it/s]
2022-03-21 05:58:05,523 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.5570, loss: 0.4114 ||: 100%|#########9| 11205/11253 [12:43<00:03, 14.43it/s]
2022-03-21 05:58:05,665 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.3554, loss: 0.4113 ||: 100%|#########9| 11207/11253 [12:43<00:03, 14.33it/s]
2022-03-21 05:58:05,786 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.8251, loss: 0.4114 ||: 100%|#########9| 11209/11253 [12:44<00:02, 14.93it/s]
2022-03-21 05:58:05,920 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.2763, loss: 0.4114 ||: 100%|#########9| 11211/11253 [12:44<00:02, 14.94it/s]
2022-03-21 05:58:06,064 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.2838, loss: 0.4114 ||: 100%|#########9| 11213/11253 [12:44<00:02, 14.60it/s]
2022-03-21 05:58:06,232 - INFO - tqdm - f1: 0.7888, accuracy: 0.8510, batch_loss: 0.4008, loss: 0.4114 ||: 100%|#########9| 11215/11253 [12:44<00:02, 13.65it/s]
2022-03-21 05:58:06,376 - INFO - tqdm - f1: 0.7888, accuracy: 0.8510, batch_loss: 0.2291, loss: 0.4114 ||: 100%|#########9| 11217/11253 [12:44<00:02, 13.75it/s]
2022-03-21 05:58:06,500 - INFO - tqdm - f1: 0.7888, accuracy: 0.8510, batch_loss: 0.5561, loss: 0.4114 ||: 100%|#########9| 11219/11253 [12:44<00:02, 14.36it/s]
2022-03-21 05:58:06,629 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.8374, loss: 0.4114 ||: 100%|#########9| 11221/11253 [12:44<00:02, 14.70it/s]
2022-03-21 05:58:06,756 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.3029, loss: 0.4115 ||: 100%|#########9| 11223/11253 [12:45<00:02, 14.98it/s]
2022-03-21 05:58:06,880 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.3871, loss: 0.4115 ||: 100%|#########9| 11225/11253 [12:45<00:01, 15.33it/s]
2022-03-21 05:58:07,020 - INFO - tqdm - f1: 0.7887, accuracy: 0.8509, batch_loss: 0.5936, loss: 0.4115 ||: 100%|#########9| 11227/11253 [12:45<00:01, 14.99it/s]
2022-03-21 05:58:07,163 - INFO - tqdm - f1: 0.7886, accuracy: 0.8509, batch_loss: 0.6056, loss: 0.4116 ||: 100%|#########9| 11229/11253 [12:45<00:01, 14.67it/s]
2022-03-21 05:58:07,307 - INFO - tqdm - f1: 0.7886, accuracy: 0.8509, batch_loss: 0.4806, loss: 0.4116 ||: 100%|#########9| 11231/11253 [12:45<00:01, 14.44it/s]
2022-03-21 05:58:07,437 - INFO - tqdm - f1: 0.7886, accuracy: 0.8509, batch_loss: 0.3881, loss: 0.4116 ||: 100%|#########9| 11233/11253 [12:45<00:01, 14.70it/s]
2022-03-21 05:58:07,568 - INFO - tqdm - f1: 0.7886, accuracy: 0.8509, batch_loss: 0.3317, loss: 0.4116 ||: 100%|#########9| 11235/11253 [12:45<00:01, 14.86it/s]
2022-03-21 05:58:07,700 - INFO - tqdm - f1: 0.7886, accuracy: 0.8509, batch_loss: 0.2028, loss: 0.4116 ||: 100%|#########9| 11237/11253 [12:45<00:01, 14.97it/s]
2022-03-21 05:58:07,827 - INFO - tqdm - f1: 0.7886, accuracy: 0.8509, batch_loss: 0.6050, loss: 0.4116 ||: 100%|#########9| 11239/11253 [12:46<00:00, 15.19it/s]
2022-03-21 05:58:07,954 - INFO - tqdm - f1: 0.7887, accuracy: 0.8509, batch_loss: 0.2322, loss: 0.4116 ||: 100%|#########9| 11241/11253 [12:46<00:00, 15.36it/s]
2022-03-21 05:58:08,102 - INFO - tqdm - f1: 0.7887, accuracy: 0.8509, batch_loss: 0.2365, loss: 0.4116 ||: 100%|#########9| 11243/11253 [12:46<00:00, 14.73it/s]
2022-03-21 05:58:08,248 - INFO - tqdm - f1: 0.7887, accuracy: 0.8509, batch_loss: 0.2142, loss: 0.4116 ||: 100%|#########9| 11245/11253 [12:46<00:00, 14.43it/s]
2022-03-21 05:58:08,380 - INFO - tqdm - f1: 0.7887, accuracy: 0.8509, batch_loss: 0.3220, loss: 0.4115 ||: 100%|#########9| 11247/11253 [12:46<00:00, 14.63it/s]
2022-03-21 05:58:08,498 - INFO - tqdm - f1: 0.7887, accuracy: 0.8509, batch_loss: 0.2034, loss: 0.4115 ||: 100%|#########9| 11249/11253 [12:46<00:00, 15.26it/s]
2022-03-21 05:58:08,624 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.2165, loss: 0.4115 ||: 100%|#########9| 11251/11253 [12:46<00:00, 15.43it/s]
2022-03-21 05:58:08,751 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.4335, loss: 0.4115 ||: 100%|##########| 11253/11253 [12:47<00:00, 15.54it/s]
2022-03-21 05:58:08,815 - INFO - tqdm - f1: 0.7887, accuracy: 0.8510, batch_loss: 0.4335, loss: 0.4115 ||: 100%|##########| 11253/11253 [12:47<00:00, 14.67it/s]
2022-03-21 05:58:08,822 - INFO - allennlp.training.trainer - Validating
2022-03-21 05:58:08,824 - INFO - tqdm - 0%|          | 0/1889 [00:00<?, ?it/s]
2022-03-21 05:58:08,865 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-21 05:58:08,867 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-21 05:58:18,851 - INFO - tqdm - f1: 0.8048, accuracy: 0.8701, batch_loss: 0.0541, loss: 0.3455 ||:  22%|##2       | 420/1889 [00:10<00:29, 49.90it/s]
2022-03-21 05:58:28,971 - INFO - tqdm - f1: 0.8055, accuracy: 0.8682, batch_loss: 0.8160, loss: 0.3621 ||:  46%|####5     | 861/1889 [00:20<00:22, 46.27it/s]
2022-03-21 05:58:39,116 - INFO - tqdm - f1: 0.8100, accuracy: 0.8710, batch_loss: 0.0160, loss: 0.3552 ||:  69%|######8   | 1300/1889 [00:30<00:15, 37.68it/s]
2022-03-21 05:58:49,179 - INFO - tqdm - f1: 0.8098, accuracy: 0.8698, batch_loss: 0.1572, loss: 0.3571 ||:  92%|#########2| 1745/1889 [00:40<00:03, 42.04it/s]
2022-03-21 05:58:52,525 - INFO - tqdm - f1: 0.8107, accuracy: 0.8699, batch_loss: 0.5208, loss: 0.3571 ||: 100%|#########9| 1884/1889 [00:43<00:00, 42.90it/s]
2022-03-21 05:58:52,629 - INFO - tqdm - f1: 0.8108, accuracy: 0.8700, batch_loss: 0.1108, loss: 0.3568 ||: 100%|##########| 1889/1889 [00:43<00:00, 44.30it/s]
2022-03-21 05:58:52,644 - INFO - tqdm - f1: 0.8108, accuracy: 0.8700, batch_loss: 0.1108, loss: 0.3568 ||: 100%|##########| 1889/1889 [00:43<00:00, 43.11it/s]
2022-03-21 05:58:52,661 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/rct-20k_base_hyper_small_seed_314/best.th'.
2022-03-21 05:58:55,494 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 05:58:55,495 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.851  |     0.870
2022-03-21 05:58:55,497 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.789  |     0.811
2022-03-21 05:58:55,499 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 05:58:55,501 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.411  |     0.357
2022-03-21 05:58:55,502 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8522.688  |       N/A
2022-03-21 05:58:55,504 - INFO - allennlp.training.trainer - Epoch duration: 0:13:33.806015
2022-03-21 05:58:55,506 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:02:04
2022-03-21 05:58:55,507 - INFO - allennlp.training.trainer - Epoch 1/9
2022-03-21 05:58:55,509 - INFO - allennlp.training.trainer - Worker 0 memory usage: 8.4G
2022-03-21 05:58:55,511 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 05:58:55,513 - INFO - allennlp.training.trainer - Training
2022-03-21 05:58:55,515 - INFO - tqdm - 0%|          | 0/11253 [00:00<?, ?it/s]
2022-03-21 05:59:05,542 - INFO - tqdm - f1: 0.8224, accuracy: 0.8826, batch_loss: 0.2879, loss: 0.3329 ||:   1%|1         | 131/11253 [00:10<12:19, 15.04it/s]
2022-03-21 05:59:15,598 - INFO - tqdm - f1: 0.8229, accuracy: 0.8850, batch_loss: 0.3214, loss: 0.3232 ||:   2%|2         | 281/11253 [00:20<12:20, 14.82it/s]
2022-03-21 05:59:25,709 - INFO - tqdm - f1: 0.8170, accuracy: 0.8788, batch_loss: 0.5935, loss: 0.3384 ||:   4%|3         | 433/11253 [00:30<11:58, 15.05it/s]
2022-03-21 05:59:35,776 - INFO - tqdm - f1: 0.8187, accuracy: 0.8791, batch_loss: 0.3020, loss: 0.3380 ||:   5%|5         | 583/11253 [00:40<11:59, 14.84it/s]
2022-03-21 05:59:45,850 - INFO - tqdm - f1: 0.8146, accuracy: 0.8759, batch_loss: 0.3829, loss: 0.3444 ||:   6%|6         | 731/11253 [00:50<11:02, 15.88it/s]
2022-03-21 05:59:55,868 - INFO - tqdm - f1: 0.8139, accuracy: 0.8768, batch_loss: 0.2508, loss: 0.3429 ||:   8%|7         | 879/11253 [01:00<11:10, 15.47it/s]
2022-03-21 06:00:05,911 - INFO - tqdm - f1: 0.8136, accuracy: 0.8761, batch_loss: 0.2621, loss: 0.3445 ||:   9%|9         | 1031/11253 [01:10<10:43, 15.90it/s]
2022-03-21 06:00:15,924 - INFO - tqdm - f1: 0.8147, accuracy: 0.8751, batch_loss: 0.3256, loss: 0.3448 ||:  10%|#         | 1179/11253 [01:20<11:24, 14.72it/s]
2022-03-21 06:00:25,956 - INFO - tqdm - f1: 0.8165, accuracy: 0.8758, batch_loss: 0.3677, loss: 0.3456 ||:  12%|#1        | 1327/11253 [01:30<11:17, 14.64it/s]
2022-03-21 06:00:36,086 - INFO - tqdm - f1: 0.8185, accuracy: 0.8758, batch_loss: 0.2424, loss: 0.3461 ||:  13%|#3        | 1475/11253 [01:40<11:22, 14.34it/s]
2022-03-21 06:00:46,237 - INFO - tqdm - f1: 0.8170, accuracy: 0.8750, batch_loss: 0.1917, loss: 0.3474 ||:  14%|#4        | 1623/11253 [01:50<11:26, 14.03it/s]
2022-03-21 06:00:56,337 - INFO - tqdm - f1: 0.8175, accuracy: 0.8752, batch_loss: 0.4994, loss: 0.3461 ||:  16%|#5        | 1773/11253 [02:00<11:07, 14.20it/s]
2022-03-21 06:01:06,436 - INFO - tqdm - f1: 0.8175, accuracy: 0.8752, batch_loss: 0.3050, loss: 0.3457 ||:  17%|#7        | 1925/11253 [02:10<10:39, 14.58it/s]
2022-03-21 06:01:16,492 - INFO - tqdm - f1: 0.8168, accuracy: 0.8749, batch_loss: 0.1026, loss: 0.3451 ||:  18%|#8        | 2071/11253 [02:20<10:48, 14.17it/s]
2022-03-21 06:01:26,576 - INFO - tqdm - f1: 0.8171, accuracy: 0.8751, batch_loss: 0.3117, loss: 0.3457 ||:  20%|#9        | 2219/11253 [02:31<10:12, 14.74it/s]
2022-03-21 06:01:36,664 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.3044, loss: 0.3438 ||:  21%|##1       | 2373/11253 [02:41<09:39, 15.33it/s]
2022-03-21 06:01:46,740 - INFO - tqdm - f1: 0.8164, accuracy: 0.8752, batch_loss: 0.1741, loss: 0.3445 ||:  22%|##2       | 2525/11253 [02:51<09:23, 15.48it/s]
2022-03-21 06:01:56,808 - INFO - tqdm - f1: 0.8170, accuracy: 0.8760, batch_loss: 0.4176, loss: 0.3439 ||:  24%|##3       | 2675/11253 [03:01<09:16, 15.42it/s]
2022-03-21 06:02:06,878 - INFO - tqdm - f1: 0.8175, accuracy: 0.8764, batch_loss: 0.3916, loss: 0.3431 ||:  25%|##5       | 2819/11253 [03:11<09:23, 14.97it/s]
2022-03-21 06:02:16,917 - INFO - tqdm - f1: 0.8182, accuracy: 0.8769, batch_loss: 0.2371, loss: 0.3430 ||:  26%|##6       | 2967/11253 [03:21<08:51, 15.59it/s]
2022-03-21 06:02:26,977 - INFO - tqdm - f1: 0.8174, accuracy: 0.8764, batch_loss: 0.5314, loss: 0.3433 ||:  28%|##7       | 3115/11253 [03:31<09:18, 14.56it/s]
2022-03-21 06:02:37,082 - INFO - tqdm - f1: 0.8167, accuracy: 0.8763, batch_loss: 0.4927, loss: 0.3435 ||:  29%|##8       | 3263/11253 [03:41<08:53, 14.97it/s]
2022-03-21 06:02:47,180 - INFO - tqdm - f1: 0.8165, accuracy: 0.8764, batch_loss: 0.5107, loss: 0.3434 ||:  30%|###       | 3411/11253 [03:51<08:34, 15.24it/s]
2022-03-21 06:02:57,323 - INFO - tqdm - f1: 0.8163, accuracy: 0.8764, batch_loss: 0.3056, loss: 0.3431 ||:  32%|###1      | 3559/11253 [04:01<09:10, 13.98it/s]
2022-03-21 06:03:07,427 - INFO - tqdm - f1: 0.8159, accuracy: 0.8760, batch_loss: 0.1511, loss: 0.3438 ||:  33%|###2      | 3705/11253 [04:11<08:42, 14.46it/s]
2022-03-21 06:03:17,468 - INFO - tqdm - f1: 0.8153, accuracy: 0.8758, batch_loss: 0.0386, loss: 0.3435 ||:  34%|###4      | 3855/11253 [04:21<09:01, 13.67it/s]
2022-03-21 06:03:27,581 - INFO - tqdm - f1: 0.8149, accuracy: 0.8755, batch_loss: 0.2772, loss: 0.3447 ||:  36%|###5      | 3999/11253 [04:32<08:11, 14.77it/s]
2022-03-21 06:03:37,663 - INFO - tqdm - f1: 0.8149, accuracy: 0.8754, batch_loss: 0.2560, loss: 0.3452 ||:  37%|###6      | 4145/11253 [04:42<08:03, 14.70it/s]
2022-03-21 06:03:47,698 - INFO - tqdm - f1: 0.8151, accuracy: 0.8754, batch_loss: 0.3191, loss: 0.3453 ||:  38%|###8      | 4293/11253 [04:52<07:56, 14.60it/s]
2022-03-21 06:03:57,789 - INFO - tqdm - f1: 0.8152, accuracy: 0.8753, batch_loss: 0.3022, loss: 0.3452 ||:  39%|###9      | 4443/11253 [05:02<07:30, 15.12it/s]
2022-03-21 06:04:07,874 - INFO - tqdm - f1: 0.8154, accuracy: 0.8754, batch_loss: 0.1452, loss: 0.3452 ||:  41%|####      | 4589/11253 [05:12<07:18, 15.20it/s]
2022-03-21 06:04:18,003 - INFO - tqdm - f1: 0.8161, accuracy: 0.8756, batch_loss: 0.4789, loss: 0.3446 ||:  42%|####2     | 4735/11253 [05:22<07:22, 14.73it/s]
2022-03-21 06:04:28,035 - INFO - tqdm - f1: 0.8162, accuracy: 0.8756, batch_loss: 0.4407, loss: 0.3443 ||:  43%|####3     | 4883/11253 [05:32<07:00, 15.13it/s]
2022-03-21 06:04:38,119 - INFO - tqdm - f1: 0.8163, accuracy: 0.8757, batch_loss: 0.3079, loss: 0.3442 ||:  45%|####4     | 5035/11253 [05:42<06:49, 15.19it/s]
2022-03-21 06:04:48,160 - INFO - tqdm - f1: 0.8172, accuracy: 0.8762, batch_loss: 0.1824, loss: 0.3434 ||:  46%|####6     | 5187/11253 [05:52<06:25, 15.75it/s]
2022-03-21 06:04:58,168 - INFO - tqdm - f1: 0.8172, accuracy: 0.8763, batch_loss: 0.8485, loss: 0.3432 ||:  47%|####7     | 5335/11253 [06:02<07:06, 13.86it/s]
2022-03-21 06:05:08,243 - INFO - tqdm - f1: 0.8168, accuracy: 0.8760, batch_loss: 0.2164, loss: 0.3440 ||:  49%|####8     | 5479/11253 [06:12<06:20, 15.16it/s]
2022-03-21 06:05:18,263 - INFO - tqdm - f1: 0.8166, accuracy: 0.8757, batch_loss: 0.5372, loss: 0.3447 ||:  50%|#####     | 5629/11253 [06:22<05:55, 15.84it/s]
2022-03-21 06:05:28,294 - INFO - tqdm - f1: 0.8167, accuracy: 0.8758, batch_loss: 0.4199, loss: 0.3446 ||:  51%|#####1    | 5783/11253 [06:32<05:44, 15.88it/s]
2022-03-21 06:05:38,363 - INFO - tqdm - f1: 0.8170, accuracy: 0.8759, batch_loss: 0.4662, loss: 0.3445 ||:  53%|#####2    | 5933/11253 [06:42<06:03, 14.65it/s]
2022-03-21 06:05:48,484 - INFO - tqdm - f1: 0.8172, accuracy: 0.8759, batch_loss: 0.0947, loss: 0.3443 ||:  54%|#####4    | 6081/11253 [06:52<05:48, 14.86it/s]
2022-03-21 06:05:58,882 - INFO - tqdm - f1: 0.8172, accuracy: 0.8760, batch_loss: 0.6431, loss: 0.3444 ||:  55%|#####5    | 6229/11253 [07:03<09:42,  8.62it/s]
2022-03-21 06:06:08,888 - INFO - tqdm - f1: 0.8168, accuracy: 0.8756, batch_loss: 0.1098, loss: 0.3451 ||:  57%|#####6    | 6377/11253 [07:13<05:50, 13.89it/s]
2022-03-21 06:06:18,940 - INFO - tqdm - f1: 0.8165, accuracy: 0.8755, batch_loss: 0.4944, loss: 0.3455 ||:  58%|#####7    | 6525/11253 [07:23<05:11, 15.20it/s]
2022-03-21 06:06:29,070 - INFO - tqdm - f1: 0.8163, accuracy: 0.8753, batch_loss: 0.3947, loss: 0.3459 ||:  59%|#####9    | 6677/11253 [07:33<05:18, 14.36it/s]
2022-03-21 06:06:39,181 - INFO - tqdm - f1: 0.8161, accuracy: 0.8750, batch_loss: 0.2633, loss: 0.3465 ||:  61%|######    | 6823/11253 [07:43<05:02, 14.66it/s]
2022-03-21 06:06:49,267 - INFO - tqdm - f1: 0.8160, accuracy: 0.8751, batch_loss: 0.4522, loss: 0.3464 ||:  62%|######1   | 6971/11253 [07:53<04:52, 14.64it/s]
2022-03-21 06:06:59,330 - INFO - tqdm - f1: 0.8160, accuracy: 0.8751, batch_loss: 0.5932, loss: 0.3463 ||:  63%|######3   | 7121/11253 [08:03<04:31, 15.22it/s]
2022-03-21 06:07:09,370 - INFO - tqdm - f1: 0.8163, accuracy: 0.8752, batch_loss: 0.1306, loss: 0.3462 ||:  65%|######4   | 7271/11253 [08:13<04:32, 14.61it/s]
2022-03-21 06:07:19,496 - INFO - tqdm - f1: 0.8161, accuracy: 0.8751, batch_loss: 0.3251, loss: 0.3465 ||:  66%|######5   | 7417/11253 [08:23<04:39, 13.70it/s]
2022-03-21 06:07:29,649 - INFO - tqdm - f1: 0.8160, accuracy: 0.8751, batch_loss: 0.0117, loss: 0.3465 ||:  67%|######7   | 7561/11253 [08:34<04:31, 13.62it/s]
2022-03-21 06:07:39,682 - INFO - tqdm - f1: 0.8160, accuracy: 0.8751, batch_loss: 0.1318, loss: 0.3467 ||:  69%|######8   | 7711/11253 [08:44<04:12, 14.01it/s]
2022-03-21 06:07:49,786 - INFO - tqdm - f1: 0.8159, accuracy: 0.8750, batch_loss: 0.2655, loss: 0.3467 ||:  70%|######9   | 7861/11253 [08:54<03:49, 14.75it/s]
2022-03-21 06:07:59,836 - INFO - tqdm - f1: 0.8160, accuracy: 0.8749, batch_loss: 0.0525, loss: 0.3465 ||:  71%|#######1  | 8015/11253 [09:04<03:37, 14.88it/s]
2022-03-21 06:08:09,901 - INFO - tqdm - f1: 0.8160, accuracy: 0.8749, batch_loss: 0.2508, loss: 0.3463 ||:  73%|#######2  | 8159/11253 [09:14<03:38, 14.13it/s]
2022-03-21 06:08:19,958 - INFO - tqdm - f1: 0.8160, accuracy: 0.8749, batch_loss: 0.6517, loss: 0.3465 ||:  74%|#######3  | 8307/11253 [09:24<03:17, 14.91it/s]
2022-03-21 06:08:29,960 - INFO - tqdm - f1: 0.8162, accuracy: 0.8749, batch_loss: 0.0616, loss: 0.3468 ||:  75%|#######5  | 8451/11253 [09:34<03:12, 14.56it/s]
2022-03-21 06:08:40,072 - INFO - tqdm - f1: 0.8163, accuracy: 0.8749, batch_loss: 0.2485, loss: 0.3466 ||:  76%|#######6  | 8603/11253 [09:44<02:55, 15.09it/s]
2022-03-21 06:08:50,100 - INFO - tqdm - f1: 0.8165, accuracy: 0.8749, batch_loss: 0.2824, loss: 0.3464 ||:  78%|#######7  | 8751/11253 [09:54<02:44, 15.22it/s]
2022-03-21 06:09:00,196 - INFO - tqdm - f1: 0.8162, accuracy: 0.8747, batch_loss: 0.1459, loss: 0.3470 ||:  79%|#######9  | 8893/11253 [10:04<02:42, 14.53it/s]
2022-03-21 06:09:10,214 - INFO - tqdm - f1: 0.8166, accuracy: 0.8749, batch_loss: 0.6658, loss: 0.3466 ||:  80%|########  | 9043/11253 [10:14<02:28, 14.85it/s]
2022-03-21 06:09:20,225 - INFO - tqdm - f1: 0.8167, accuracy: 0.8749, batch_loss: 0.2072, loss: 0.3465 ||:  82%|########1 | 9189/11253 [10:24<02:20, 14.64it/s]
2022-03-21 06:09:30,233 - INFO - tqdm - f1: 0.8166, accuracy: 0.8749, batch_loss: 0.4403, loss: 0.3465 ||:  83%|########2 | 9339/11253 [10:34<02:05, 15.30it/s]
2022-03-21 06:09:40,268 - INFO - tqdm - f1: 0.8168, accuracy: 0.8749, batch_loss: 0.6718, loss: 0.3462 ||:  84%|########4 | 9487/11253 [10:44<01:59, 14.77it/s]
2022-03-21 06:09:50,301 - INFO - tqdm - f1: 0.8166, accuracy: 0.8748, batch_loss: 0.2015, loss: 0.3467 ||:  85%|########5 | 9617/11253 [10:54<01:45, 15.52it/s]
2022-03-21 06:10:00,327 - INFO - tqdm - f1: 0.8167, accuracy: 0.8749, batch_loss: 0.5951, loss: 0.3466 ||:  87%|########6 | 9767/11253 [11:04<01:38, 15.04it/s]
2022-03-21 06:10:10,435 - INFO - tqdm - f1: 0.8169, accuracy: 0.8751, batch_loss: 0.2255, loss: 0.3463 ||:  88%|########8 | 9917/11253 [11:14<01:24, 15.83it/s]
2022-03-21 06:10:20,483 - INFO - tqdm - f1: 0.8172, accuracy: 0.8752, batch_loss: 0.1153, loss: 0.3461 ||:  89%|########9 | 10065/11253 [11:24<01:17, 15.33it/s]
2022-03-21 06:10:30,504 - INFO - tqdm - f1: 0.8170, accuracy: 0.8751, batch_loss: 0.1320, loss: 0.3462 ||:  91%|######### | 10209/11253 [11:34<01:08, 15.31it/s]
2022-03-21 06:10:40,561 - INFO - tqdm - f1: 0.8172, accuracy: 0.8752, batch_loss: 0.3637, loss: 0.3460 ||:  92%|#########2| 10357/11253 [11:45<00:58, 15.23it/s]
2022-03-21 06:10:50,695 - INFO - tqdm - f1: 0.8172, accuracy: 0.8751, batch_loss: 0.3738, loss: 0.3460 ||:  93%|#########3| 10503/11253 [11:55<00:51, 14.55it/s]
2022-03-21 06:11:00,733 - INFO - tqdm - f1: 0.8174, accuracy: 0.8753, batch_loss: 0.5738, loss: 0.3458 ||:  95%|#########4| 10653/11253 [12:05<00:39, 15.05it/s]
2022-03-21 06:11:10,846 - INFO - tqdm - f1: 0.8173, accuracy: 0.8752, batch_loss: 0.4079, loss: 0.3458 ||:  96%|#########5| 10797/11253 [12:15<00:32, 14.01it/s]
2022-03-21 06:11:20,949 - INFO - tqdm - f1: 0.8173, accuracy: 0.8754, batch_loss: 0.1194, loss: 0.3455 ||:  97%|#########7| 10943/11253 [12:25<00:21, 14.14it/s]
2022-03-21 06:11:31,059 - INFO - tqdm - f1: 0.8173, accuracy: 0.8754, batch_loss: 0.4501, loss: 0.3454 ||:  99%|#########8| 11089/11253 [12:35<00:11, 14.74it/s]
2022-03-21 06:11:38,353 - INFO - tqdm - f1: 0.8175, accuracy: 0.8754, batch_loss: 0.3012, loss: 0.3454 ||: 100%|#########9| 11197/11253 [12:42<00:03, 15.09it/s]
2022-03-21 06:11:38,494 - INFO - tqdm - f1: 0.8175, accuracy: 0.8754, batch_loss: 0.6124, loss: 0.3454 ||: 100%|#########9| 11199/11253 [12:42<00:03, 14.80it/s]
2022-03-21 06:11:38,620 - INFO - tqdm - f1: 0.8175, accuracy: 0.8754, batch_loss: 0.4205, loss: 0.3454 ||: 100%|#########9| 11201/11253 [12:43<00:03, 15.12it/s]
2022-03-21 06:11:38,755 - INFO - tqdm - f1: 0.8175, accuracy: 0.8754, batch_loss: 0.2838, loss: 0.3454 ||: 100%|#########9| 11203/11253 [12:43<00:03, 15.02it/s]
2022-03-21 06:11:38,894 - INFO - tqdm - f1: 0.8175, accuracy: 0.8754, batch_loss: 0.4572, loss: 0.3454 ||: 100%|#########9| 11205/11253 [12:43<00:03, 14.81it/s]
2022-03-21 06:11:39,025 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.2941, loss: 0.3454 ||: 100%|#########9| 11207/11253 [12:43<00:03, 14.95it/s]
2022-03-21 06:11:39,156 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.2559, loss: 0.3454 ||: 100%|#########9| 11209/11253 [12:43<00:02, 15.05it/s]
2022-03-21 06:11:39,280 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.3028, loss: 0.3454 ||: 100%|#########9| 11211/11253 [12:43<00:02, 15.36it/s]
2022-03-21 06:11:39,415 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.3821, loss: 0.3454 ||: 100%|#########9| 11213/11253 [12:43<00:02, 15.18it/s]
2022-03-21 06:11:39,538 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.3167, loss: 0.3454 ||: 100%|#########9| 11215/11253 [12:44<00:02, 15.49it/s]
2022-03-21 06:11:39,688 - INFO - tqdm - f1: 0.8176, accuracy: 0.8755, batch_loss: 0.1284, loss: 0.3453 ||: 100%|#########9| 11217/11253 [12:44<00:02, 14.78it/s]
2022-03-21 06:11:39,830 - INFO - tqdm - f1: 0.8176, accuracy: 0.8755, batch_loss: 0.0971, loss: 0.3453 ||: 100%|#########9| 11219/11253 [12:44<00:02, 14.55it/s]
2022-03-21 06:11:39,971 - INFO - tqdm - f1: 0.8176, accuracy: 0.8755, batch_loss: 0.6386, loss: 0.3453 ||: 100%|#########9| 11221/11253 [12:44<00:02, 14.45it/s]
2022-03-21 06:11:40,099 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.3839, loss: 0.3453 ||: 100%|#########9| 11223/11253 [12:44<00:02, 14.78it/s]
2022-03-21 06:11:40,242 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.4015, loss: 0.3453 ||: 100%|#########9| 11225/11253 [12:44<00:01, 14.55it/s]
2022-03-21 06:11:40,367 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.4122, loss: 0.3454 ||: 100%|#########9| 11227/11253 [12:44<00:01, 14.93it/s]
2022-03-21 06:11:40,507 - INFO - tqdm - f1: 0.8175, accuracy: 0.8754, batch_loss: 0.5360, loss: 0.3454 ||: 100%|#########9| 11229/11253 [12:44<00:01, 14.75it/s]
2022-03-21 06:11:40,659 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.2633, loss: 0.3454 ||: 100%|#########9| 11231/11253 [12:45<00:01, 14.25it/s]
2022-03-21 06:11:40,811 - INFO - tqdm - f1: 0.8175, accuracy: 0.8754, batch_loss: 0.2775, loss: 0.3454 ||: 100%|#########9| 11233/11253 [12:45<00:01, 13.89it/s]
2022-03-21 06:11:40,953 - INFO - tqdm - f1: 0.8175, accuracy: 0.8754, batch_loss: 0.5135, loss: 0.3454 ||: 100%|#########9| 11235/11253 [12:45<00:01, 13.94it/s]
2022-03-21 06:11:41,100 - INFO - tqdm - f1: 0.8175, accuracy: 0.8754, batch_loss: 0.3290, loss: 0.3454 ||: 100%|#########9| 11237/11253 [12:45<00:01, 13.84it/s]
2022-03-21 06:11:41,237 - INFO - tqdm - f1: 0.8175, accuracy: 0.8754, batch_loss: 0.1088, loss: 0.3453 ||: 100%|#########9| 11239/11253 [12:45<00:00, 14.07it/s]
2022-03-21 06:11:41,375 - INFO - tqdm - f1: 0.8175, accuracy: 0.8754, batch_loss: 0.2111, loss: 0.3453 ||: 100%|#########9| 11241/11253 [12:45<00:00, 14.18it/s]
2022-03-21 06:11:41,502 - INFO - tqdm - f1: 0.8175, accuracy: 0.8754, batch_loss: 0.3797, loss: 0.3453 ||: 100%|#########9| 11243/11253 [12:45<00:00, 14.61it/s]
2022-03-21 06:11:41,641 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.2901, loss: 0.3453 ||: 100%|#########9| 11245/11253 [12:46<00:00, 14.56it/s]
2022-03-21 06:11:41,786 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.2574, loss: 0.3453 ||: 100%|#########9| 11247/11253 [12:46<00:00, 14.32it/s]
2022-03-21 06:11:41,916 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.4165, loss: 0.3453 ||: 100%|#########9| 11249/11253 [12:46<00:00, 14.61it/s]
2022-03-21 06:11:42,040 - INFO - tqdm - f1: 0.8175, accuracy: 0.8755, batch_loss: 0.2312, loss: 0.3453 ||: 100%|#########9| 11251/11253 [12:46<00:00, 15.04it/s]
2022-03-21 06:11:42,164 - INFO - tqdm - f1: 0.8176, accuracy: 0.8755, batch_loss: 0.0842, loss: 0.3453 ||: 100%|##########| 11253/11253 [12:46<00:00, 15.35it/s]
2022-03-21 06:11:42,227 - INFO - tqdm - f1: 0.8176, accuracy: 0.8755, batch_loss: 0.0842, loss: 0.3453 ||: 100%|##########| 11253/11253 [12:46<00:00, 14.68it/s]
2022-03-21 06:11:42,233 - INFO - allennlp.training.trainer - Validating
2022-03-21 06:11:42,236 - INFO - tqdm - 0%|          | 0/1889 [00:00<?, ?it/s]
2022-03-21 06:11:52,341 - INFO - tqdm - f1: 0.8239, accuracy: 0.8781, batch_loss: 0.2238, loss: 0.3485 ||:  23%|##2       | 429/1889 [00:10<00:34, 42.18it/s]
2022-03-21 06:12:02,405 - INFO - tqdm - f1: 0.8210, accuracy: 0.8762, batch_loss: 0.2928, loss: 0.3525 ||:  45%|####5     | 854/1889 [00:20<00:23, 44.97it/s]
2022-03-21 06:12:12,498 - INFO - tqdm - f1: 0.8214, accuracy: 0.8758, batch_loss: 0.3256, loss: 0.3579 ||:  68%|######8   | 1292/1889 [00:30<00:12, 48.60it/s]
2022-03-21 06:12:22,543 - INFO - tqdm - f1: 0.8192, accuracy: 0.8750, batch_loss: 0.4823, loss: 0.3573 ||:  92%|#########2| 1742/1889 [00:40<00:02, 51.97it/s]
2022-03-21 06:12:25,580 - INFO - tqdm - f1: 0.8193, accuracy: 0.8753, batch_loss: 0.3019, loss: 0.3582 ||: 100%|#########9| 1881/1889 [00:43<00:00, 52.49it/s]
2022-03-21 06:12:25,756 - INFO - tqdm - f1: 0.8195, accuracy: 0.8753, batch_loss: 0.8028, loss: 0.3582 ||: 100%|#########9| 1887/1889 [00:43<00:00, 45.00it/s]
2022-03-21 06:12:25,826 - INFO - tqdm - f1: 0.8195, accuracy: 0.8753, batch_loss: 0.5594, loss: 0.3582 ||: 100%|##########| 1889/1889 [00:43<00:00, 43.34it/s]
2022-03-21 06:12:25,841 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/rct-20k_base_hyper_small_seed_314/best.th'.
2022-03-21 06:12:28,421 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 06:12:28,423 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.875  |     0.875
2022-03-21 06:12:28,424 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.818  |     0.820
2022-03-21 06:12:28,425 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 06:12:28,427 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.345  |     0.358
2022-03-21 06:12:28,431 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8641.559  |       N/A
2022-03-21 06:12:28,433 - INFO - allennlp.training.trainer - Epoch duration: 0:13:32.925487
2022-03-21 06:12:28,435 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:48:26
2022-03-21 06:12:28,436 - INFO - allennlp.training.trainer - Epoch 2/9
2022-03-21 06:12:28,437 - INFO - allennlp.training.trainer - Worker 0 memory usage: 8.4G
2022-03-21 06:12:28,439 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 06:12:28,442 - INFO - allennlp.training.trainer - Training
2022-03-21 06:12:28,444 - INFO - tqdm - 0%|          | 0/11253 [00:00<?, ?it/s]
2022-03-21 06:12:38,519 - INFO - tqdm - f1: 0.8551, accuracy: 0.9050, batch_loss: 0.4528, loss: 0.2975 ||:   1%|          | 73/11253 [00:10<12:42, 14.67it/s]
2022-03-21 06:12:48,530 - INFO - tqdm - f1: 0.8502, accuracy: 0.9019, batch_loss: 0.1200, loss: 0.2839 ||:   2%|1         | 223/11253 [00:20<11:42, 15.69it/s]
2022-03-21 06:12:58,624 - INFO - tqdm - f1: 0.8447, accuracy: 0.8956, batch_loss: 0.1007, loss: 0.3062 ||:   3%|3         | 371/11253 [00:30<12:03, 15.05it/s]
2022-03-21 06:13:08,761 - INFO - tqdm - f1: 0.8417, accuracy: 0.8944, batch_loss: 0.0211, loss: 0.3039 ||:   5%|4         | 521/11253 [00:40<12:21, 14.48it/s]
2022-03-21 06:13:18,789 - INFO - tqdm - f1: 0.8406, accuracy: 0.8937, batch_loss: 0.3215, loss: 0.3034 ||:   6%|5         | 669/11253 [00:50<11:48, 14.93it/s]
2022-03-21 06:13:28,920 - INFO - tqdm - f1: 0.8377, accuracy: 0.8920, batch_loss: 0.1165, loss: 0.3050 ||:   7%|7         | 815/11253 [01:00<12:23, 14.04it/s]
2022-03-21 06:13:39,004 - INFO - tqdm - f1: 0.8389, accuracy: 0.8928, batch_loss: 0.1412, loss: 0.3019 ||:   9%|8         | 963/11253 [01:10<12:06, 14.16it/s]
2022-03-21 06:13:49,070 - INFO - tqdm - f1: 0.8362, accuracy: 0.8912, batch_loss: 0.1443, loss: 0.3075 ||:  10%|9         | 1109/11253 [01:20<11:40, 14.48it/s]
2022-03-21 06:13:59,097 - INFO - tqdm - f1: 0.8362, accuracy: 0.8913, batch_loss: 0.6864, loss: 0.3063 ||:  11%|#1        | 1257/11253 [01:30<11:12, 14.87it/s]
2022-03-21 06:14:09,125 - INFO - tqdm - f1: 0.8357, accuracy: 0.8901, batch_loss: 0.2758, loss: 0.3091 ||:  13%|#2        | 1407/11253 [01:40<11:55, 13.76it/s]
2022-03-21 06:14:19,145 - INFO - tqdm - f1: 0.8352, accuracy: 0.8899, batch_loss: 0.2067, loss: 0.3107 ||:  14%|#3        | 1555/11253 [01:50<10:48, 14.95it/s]
2022-03-21 06:14:29,266 - INFO - tqdm - f1: 0.8345, accuracy: 0.8896, batch_loss: 0.1343, loss: 0.3103 ||:  15%|#5        | 1705/11253 [02:00<10:48, 14.73it/s]
2022-03-21 06:14:39,380 - INFO - tqdm - f1: 0.8337, accuracy: 0.8891, batch_loss: 0.7271, loss: 0.3104 ||:  17%|#6        | 1857/11253 [02:10<10:00, 15.64it/s]
2022-03-21 06:14:49,453 - INFO - tqdm - f1: 0.8338, accuracy: 0.8888, batch_loss: 0.2183, loss: 0.3108 ||:  18%|#7        | 2009/11253 [02:21<10:16, 14.99it/s]
2022-03-21 06:14:59,520 - INFO - tqdm - f1: 0.8347, accuracy: 0.8891, batch_loss: 0.3800, loss: 0.3094 ||:  19%|#9        | 2157/11253 [02:31<10:14, 14.80it/s]
2022-03-21 06:15:10,623 - INFO - tqdm - f1: 0.8351, accuracy: 0.8896, batch_loss: 0.2406, loss: 0.3086 ||:  21%|##        | 2307/11253 [02:42<33:04,  4.51it/s]
2022-03-21 06:15:20,697 - INFO - tqdm - f1: 0.8349, accuracy: 0.8895, batch_loss: 0.4532, loss: 0.3088 ||:  22%|##1       | 2439/11253 [02:52<09:43, 15.10it/s]
2022-03-21 06:15:30,800 - INFO - tqdm - f1: 0.8355, accuracy: 0.8901, batch_loss: 0.4429, loss: 0.3083 ||:  23%|##3       | 2589/11253 [03:02<10:18, 14.02it/s]
2022-03-21 06:15:40,844 - INFO - tqdm - f1: 0.8350, accuracy: 0.8899, batch_loss: 0.3523, loss: 0.3091 ||:  24%|##4       | 2741/11253 [03:12<09:25, 15.05it/s]
2022-03-21 06:15:50,886 - INFO - tqdm - f1: 0.8352, accuracy: 0.8899, batch_loss: 0.1204, loss: 0.3090 ||:  26%|##5       | 2885/11253 [03:22<09:33, 14.59it/s]
2022-03-21 06:16:01,016 - INFO - tqdm - f1: 0.8344, accuracy: 0.8895, batch_loss: 0.2076, loss: 0.3094 ||:  27%|##6       | 3035/11253 [03:32<09:22, 14.62it/s]
2022-03-21 06:16:11,077 - INFO - tqdm - f1: 0.8349, accuracy: 0.8895, batch_loss: 0.3459, loss: 0.3096 ||:  28%|##8       | 3183/11253 [03:42<08:52, 15.15it/s]
2022-03-21 06:16:21,137 - INFO - tqdm - f1: 0.8342, accuracy: 0.8890, batch_loss: 0.2676, loss: 0.3103 ||:  30%|##9       | 3333/11253 [03:52<08:43, 15.12it/s]
2022-03-21 06:16:31,140 - INFO - tqdm - f1: 0.8348, accuracy: 0.8894, batch_loss: 0.2537, loss: 0.3093 ||:  31%|###       | 3479/11253 [04:02<08:53, 14.58it/s]
2022-03-21 06:16:41,215 - INFO - tqdm - f1: 0.8350, accuracy: 0.8893, batch_loss: 0.3225, loss: 0.3092 ||:  32%|###2      | 3631/11253 [04:12<07:54, 16.07it/s]
2022-03-21 06:16:51,300 - INFO - tqdm - f1: 0.8342, accuracy: 0.8889, batch_loss: 0.4776, loss: 0.3104 ||:  34%|###3      | 3785/11253 [04:22<08:02, 15.48it/s]
2022-03-21 06:17:01,302 - INFO - tqdm - f1: 0.8343, accuracy: 0.8889, batch_loss: 0.2187, loss: 0.3103 ||:  35%|###4      | 3931/11253 [04:32<07:51, 15.54it/s]
2022-03-21 06:17:11,319 - INFO - tqdm - f1: 0.8339, accuracy: 0.8887, batch_loss: 0.5918, loss: 0.3108 ||:  36%|###6      | 4079/11253 [04:42<08:37, 13.86it/s]
2022-03-21 06:17:21,361 - INFO - tqdm - f1: 0.8337, accuracy: 0.8890, batch_loss: 0.3298, loss: 0.3102 ||:  38%|###7      | 4225/11253 [04:52<07:28, 15.66it/s]
2022-03-21 06:17:31,468 - INFO - tqdm - f1: 0.8333, accuracy: 0.8888, batch_loss: 0.2379, loss: 0.3102 ||:  39%|###8      | 4379/11253 [05:03<07:20, 15.60it/s]
2022-03-21 06:17:41,494 - INFO - tqdm - f1: 0.8331, accuracy: 0.8886, batch_loss: 0.3590, loss: 0.3109 ||:  40%|####      | 4527/11253 [05:13<07:31, 14.89it/s]
2022-03-21 06:17:51,536 - INFO - tqdm - f1: 0.8330, accuracy: 0.8885, batch_loss: 0.2709, loss: 0.3111 ||:  42%|####1     | 4679/11253 [05:23<07:20, 14.91it/s]
2022-03-21 06:18:01,628 - INFO - tqdm - f1: 0.8330, accuracy: 0.8885, batch_loss: 0.6698, loss: 0.3112 ||:  43%|####2     | 4829/11253 [05:33<07:01, 15.24it/s]
2022-03-21 06:18:11,697 - INFO - tqdm - f1: 0.8331, accuracy: 0.8887, batch_loss: 0.2631, loss: 0.3113 ||:  44%|####4     | 4979/11253 [05:43<06:50, 15.27it/s]
2022-03-21 06:18:21,702 - INFO - tqdm - f1: 0.8332, accuracy: 0.8887, batch_loss: 0.3186, loss: 0.3110 ||:  46%|####5     | 5131/11253 [05:53<06:51, 14.89it/s]
2022-03-21 06:18:31,708 - INFO - tqdm - f1: 0.8332, accuracy: 0.8886, batch_loss: 0.3601, loss: 0.3109 ||:  47%|####6     | 5277/11253 [06:03<06:35, 15.12it/s]
2022-03-21 06:18:41,790 - INFO - tqdm - f1: 0.8335, accuracy: 0.8888, batch_loss: 0.2175, loss: 0.3107 ||:  48%|####8     | 5429/11253 [06:13<06:40, 14.55it/s]
2022-03-21 06:18:51,911 - INFO - tqdm - f1: 0.8340, accuracy: 0.8892, batch_loss: 0.6881, loss: 0.3097 ||:  50%|####9     | 5573/11253 [06:23<06:39, 14.23it/s]
2022-03-21 06:19:02,033 - INFO - tqdm - f1: 0.8339, accuracy: 0.8889, batch_loss: 0.1913, loss: 0.3102 ||:  51%|#####     | 5725/11253 [06:33<05:30, 16.75it/s]
2022-03-21 06:19:12,052 - INFO - tqdm - f1: 0.8339, accuracy: 0.8890, batch_loss: 0.3409, loss: 0.3095 ||:  52%|#####2    | 5875/11253 [06:43<06:03, 14.79it/s]
2022-03-21 06:19:22,069 - INFO - tqdm - f1: 0.8342, accuracy: 0.8892, batch_loss: 0.3537, loss: 0.3092 ||:  54%|#####3    | 6023/11253 [06:53<06:20, 13.76it/s]
2022-03-21 06:19:32,107 - INFO - tqdm - f1: 0.8343, accuracy: 0.8893, batch_loss: 0.3081, loss: 0.3090 ||:  55%|#####4    | 6171/11253 [07:03<05:30, 15.37it/s]
2022-03-21 06:19:42,117 - INFO - tqdm - f1: 0.8339, accuracy: 0.8890, batch_loss: 0.3008, loss: 0.3097 ||:  56%|#####6    | 6319/11253 [07:13<05:26, 15.09it/s]
2022-03-21 06:19:52,208 - INFO - tqdm - f1: 0.8344, accuracy: 0.8892, batch_loss: 0.1553, loss: 0.3094 ||:  58%|#####7    | 6473/11253 [07:23<05:04, 15.70it/s]
2022-03-21 06:20:02,328 - INFO - tqdm - f1: 0.8342, accuracy: 0.8892, batch_loss: 0.4935, loss: 0.3092 ||:  59%|#####8    | 6625/11253 [07:33<05:18, 14.54it/s]
2022-03-21 06:20:12,432 - INFO - tqdm - f1: 0.8343, accuracy: 0.8893, batch_loss: 0.1957, loss: 0.3093 ||:  60%|######    | 6777/11253 [07:43<04:55, 15.15it/s]
2022-03-21 06:20:22,524 - INFO - tqdm - f1: 0.8345, accuracy: 0.8894, batch_loss: 0.2200, loss: 0.3090 ||:  62%|######1   | 6927/11253 [07:54<04:59, 14.44it/s]
2022-03-21 06:20:32,636 - INFO - tqdm - f1: 0.8345, accuracy: 0.8895, batch_loss: 0.5631, loss: 0.3087 ||:  63%|######2   | 7077/11253 [08:04<04:23, 15.86it/s]
2022-03-21 06:20:42,636 - INFO - tqdm - f1: 0.8341, accuracy: 0.8893, batch_loss: 0.6784, loss: 0.3092 ||:  64%|######4   | 7227/11253 [08:14<04:42, 14.25it/s]
2022-03-21 06:20:52,782 - INFO - tqdm - f1: 0.8344, accuracy: 0.8895, batch_loss: 0.1392, loss: 0.3088 ||:  66%|######5   | 7377/11253 [08:24<04:27, 14.51it/s]
2022-03-21 06:21:02,898 - INFO - tqdm - f1: 0.8343, accuracy: 0.8894, batch_loss: 0.2194, loss: 0.3089 ||:  67%|######6   | 7525/11253 [08:34<04:26, 14.00it/s]
2022-03-21 06:21:13,019 - INFO - tqdm - f1: 0.8346, accuracy: 0.8894, batch_loss: 0.3382, loss: 0.3088 ||:  68%|######8   | 7675/11253 [08:44<04:04, 14.65it/s]
2022-03-21 06:21:23,115 - INFO - tqdm - f1: 0.8344, accuracy: 0.8893, batch_loss: 0.1737, loss: 0.3088 ||:  69%|######9   | 7819/11253 [08:54<04:02, 14.17it/s]
2022-03-21 06:21:33,220 - INFO - tqdm - f1: 0.8343, accuracy: 0.8893, batch_loss: 0.3778, loss: 0.3090 ||:  71%|#######   | 7965/11253 [09:04<03:32, 15.48it/s]
2022-03-21 06:21:43,298 - INFO - tqdm - f1: 0.8346, accuracy: 0.8893, batch_loss: 0.4263, loss: 0.3084 ||:  72%|#######2  | 8113/11253 [09:14<03:29, 14.98it/s]
2022-03-21 06:21:53,405 - INFO - tqdm - f1: 0.8347, accuracy: 0.8894, batch_loss: 0.1222, loss: 0.3084 ||:  73%|#######3  | 8263/11253 [09:24<03:12, 15.55it/s]
2022-03-21 06:22:03,498 - INFO - tqdm - f1: 0.8347, accuracy: 0.8893, batch_loss: 0.8993, loss: 0.3088 ||:  75%|#######4  | 8417/11253 [09:35<03:00, 15.75it/s]
2022-03-21 06:22:13,602 - INFO - tqdm - f1: 0.8349, accuracy: 0.8894, batch_loss: 0.5404, loss: 0.3086 ||:  76%|#######6  | 8571/11253 [09:45<02:48, 15.95it/s]
2022-03-21 06:22:23,690 - INFO - tqdm - f1: 0.8346, accuracy: 0.8892, batch_loss: 0.3398, loss: 0.3089 ||:  77%|#######7  | 8721/11253 [09:55<02:46, 15.21it/s]
2022-03-21 06:22:33,804 - INFO - tqdm - f1: 0.8346, accuracy: 0.8892, batch_loss: 0.2238, loss: 0.3088 ||:  79%|#######8  | 8869/11253 [10:05<02:36, 15.19it/s]
2022-03-21 06:22:43,832 - INFO - tqdm - f1: 0.8346, accuracy: 0.8892, batch_loss: 0.2954, loss: 0.3093 ||:  80%|########  | 9021/11253 [10:15<02:27, 15.16it/s]
2022-03-21 06:22:53,861 - INFO - tqdm - f1: 0.8346, accuracy: 0.8892, batch_loss: 0.5281, loss: 0.3095 ||:  82%|########1 | 9173/11253 [10:25<02:22, 14.60it/s]
2022-03-21 06:23:03,978 - INFO - tqdm - f1: 0.8345, accuracy: 0.8891, batch_loss: 0.7590, loss: 0.3096 ||:  83%|########2 | 9325/11253 [10:35<02:07, 15.16it/s]
2022-03-21 06:23:14,112 - INFO - tqdm - f1: 0.8346, accuracy: 0.8892, batch_loss: 0.0300, loss: 0.3092 ||:  84%|########4 | 9475/11253 [10:45<02:15, 13.08it/s]
2022-03-21 06:23:24,152 - INFO - tqdm - f1: 0.8347, accuracy: 0.8893, batch_loss: 0.1220, loss: 0.3089 ||:  85%|########5 | 9619/11253 [10:55<01:49, 14.92it/s]
2022-03-21 06:23:34,173 - INFO - tqdm - f1: 0.8346, accuracy: 0.8892, batch_loss: 0.2735, loss: 0.3090 ||:  87%|########6 | 9765/11253 [11:05<01:40, 14.82it/s]
2022-03-21 06:23:44,211 - INFO - tqdm - f1: 0.8345, accuracy: 0.8891, batch_loss: 0.4410, loss: 0.3093 ||:  88%|########8 | 9911/11253 [11:15<01:34, 14.14it/s]
2022-03-21 06:23:54,279 - INFO - tqdm - f1: 0.8343, accuracy: 0.8889, batch_loss: 0.4588, loss: 0.3096 ||:  89%|########9 | 10057/11253 [11:25<01:20, 14.83it/s]
2022-03-21 06:24:04,345 - INFO - tqdm - f1: 0.8341, accuracy: 0.8888, batch_loss: 0.0928, loss: 0.3098 ||:  91%|######### | 10195/11253 [11:35<01:22, 12.84it/s]
2022-03-21 06:24:14,460 - INFO - tqdm - f1: 0.8340, accuracy: 0.8888, batch_loss: 0.3571, loss: 0.3097 ||:  92%|#########1| 10339/11253 [11:46<01:00, 15.19it/s]
2022-03-21 06:24:24,510 - INFO - tqdm - f1: 0.8341, accuracy: 0.8888, batch_loss: 0.2081, loss: 0.3097 ||:  93%|#########3| 10485/11253 [11:56<00:52, 14.73it/s]
2022-03-21 06:24:34,557 - INFO - tqdm - f1: 0.8340, accuracy: 0.8887, batch_loss: 0.3419, loss: 0.3098 ||:  94%|#########4| 10633/11253 [12:06<00:40, 15.40it/s]
2022-03-21 06:24:44,739 - INFO - tqdm - f1: 0.8340, accuracy: 0.8886, batch_loss: 0.3023, loss: 0.3099 ||:  96%|#########5| 10779/11253 [12:16<00:51,  9.29it/s]
2022-03-21 06:24:54,823 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.8324, loss: 0.3097 ||:  97%|#########7| 10921/11253 [12:26<00:24, 13.72it/s]
2022-03-21 06:25:04,829 - INFO - tqdm - f1: 0.8337, accuracy: 0.8886, batch_loss: 0.2928, loss: 0.3096 ||:  98%|#########8| 11067/11253 [12:36<00:12, 14.66it/s]
2022-03-21 06:25:13,709 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.2209, loss: 0.3093 ||: 100%|#########9| 11197/11253 [12:45<00:03, 15.49it/s]
2022-03-21 06:25:13,853 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.3286, loss: 0.3093 ||: 100%|#########9| 11199/11253 [12:45<00:03, 14.96it/s]
2022-03-21 06:25:13,985 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.2591, loss: 0.3093 ||: 100%|#########9| 11201/11253 [12:45<00:03, 15.03it/s]
2022-03-21 06:25:14,113 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.0963, loss: 0.3093 ||: 100%|#########9| 11203/11253 [12:45<00:03, 15.20it/s]
2022-03-21 06:25:14,237 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.8079, loss: 0.3093 ||: 100%|#########9| 11205/11253 [12:45<00:03, 15.46it/s]
2022-03-21 06:25:14,379 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.0479, loss: 0.3093 ||: 100%|#########9| 11207/11253 [12:45<00:03, 15.03it/s]
2022-03-21 06:25:14,513 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.1548, loss: 0.3093 ||: 100%|#########9| 11209/11253 [12:46<00:02, 15.00it/s]
2022-03-21 06:25:14,642 - INFO - tqdm - f1: 0.8338, accuracy: 0.8886, batch_loss: 0.8523, loss: 0.3094 ||: 100%|#########9| 11211/11253 [12:46<00:02, 15.12it/s]
2022-03-21 06:25:14,782 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.5524, loss: 0.3094 ||: 100%|#########9| 11213/11253 [12:46<00:02, 14.87it/s]
2022-03-21 06:25:14,937 - INFO - tqdm - f1: 0.8338, accuracy: 0.8886, batch_loss: 0.2085, loss: 0.3094 ||: 100%|#########9| 11215/11253 [12:46<00:02, 14.21it/s]
2022-03-21 06:25:15,095 - INFO - tqdm - f1: 0.8338, accuracy: 0.8886, batch_loss: 0.2707, loss: 0.3094 ||: 100%|#########9| 11217/11253 [12:46<00:02, 13.71it/s]
2022-03-21 06:25:15,251 - INFO - tqdm - f1: 0.8338, accuracy: 0.8886, batch_loss: 0.1819, loss: 0.3094 ||: 100%|#########9| 11219/11253 [12:46<00:02, 13.44it/s]
2022-03-21 06:25:15,382 - INFO - tqdm - f1: 0.8338, accuracy: 0.8886, batch_loss: 0.2460, loss: 0.3094 ||: 100%|#########9| 11221/11253 [12:46<00:02, 13.95it/s]
2022-03-21 06:25:15,520 - INFO - tqdm - f1: 0.8338, accuracy: 0.8886, batch_loss: 0.0472, loss: 0.3094 ||: 100%|#########9| 11223/11253 [12:47<00:02, 14.10it/s]
2022-03-21 06:25:15,643 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.1652, loss: 0.3094 ||: 100%|#########9| 11225/11253 [12:47<00:01, 14.69it/s]
2022-03-21 06:25:15,786 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.5122, loss: 0.3094 ||: 100%|#########9| 11227/11253 [12:47<00:01, 14.47it/s]
2022-03-21 06:25:15,924 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.4838, loss: 0.3094 ||: 100%|#########9| 11229/11253 [12:47<00:01, 14.48it/s]
2022-03-21 06:25:16,065 - INFO - tqdm - f1: 0.8339, accuracy: 0.8887, batch_loss: 0.0734, loss: 0.3093 ||: 100%|#########9| 11231/11253 [12:47<00:01, 14.39it/s]
2022-03-21 06:25:16,202 - INFO - tqdm - f1: 0.8339, accuracy: 0.8887, batch_loss: 0.4008, loss: 0.3094 ||: 100%|#########9| 11233/11253 [12:47<00:01, 14.45it/s]
2022-03-21 06:25:16,337 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.3283, loss: 0.3094 ||: 100%|#########9| 11235/11253 [12:47<00:01, 14.54it/s]
2022-03-21 06:25:16,482 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.3549, loss: 0.3094 ||: 100%|#########9| 11237/11253 [12:48<00:01, 14.31it/s]
2022-03-21 06:25:16,614 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.0652, loss: 0.3094 ||: 100%|#########9| 11239/11253 [12:48<00:00, 14.58it/s]
2022-03-21 06:25:16,741 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.0892, loss: 0.3094 ||: 100%|#########9| 11241/11253 [12:48<00:00, 14.91it/s]
2022-03-21 06:25:16,873 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.3198, loss: 0.3094 ||: 100%|#########9| 11243/11253 [12:48<00:00, 14.97it/s]
2022-03-21 06:25:17,026 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.2103, loss: 0.3093 ||: 100%|#########9| 11245/11253 [12:48<00:00, 14.34it/s]
2022-03-21 06:25:17,158 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.3375, loss: 0.3093 ||: 100%|#########9| 11247/11253 [12:48<00:00, 14.56it/s]
2022-03-21 06:25:17,278 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.0850, loss: 0.3093 ||: 100%|#########9| 11249/11253 [12:48<00:00, 15.15it/s]
2022-03-21 06:25:17,403 - INFO - tqdm - f1: 0.8338, accuracy: 0.8886, batch_loss: 0.6165, loss: 0.3093 ||: 100%|#########9| 11251/11253 [12:48<00:00, 15.41it/s]
2022-03-21 06:25:17,542 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.2689, loss: 0.3093 ||: 100%|##########| 11253/11253 [12:49<00:00, 15.06it/s]
2022-03-21 06:25:17,607 - INFO - tqdm - f1: 0.8338, accuracy: 0.8887, batch_loss: 0.2689, loss: 0.3093 ||: 100%|##########| 11253/11253 [12:49<00:00, 14.63it/s]
2022-03-21 06:25:17,615 - INFO - allennlp.training.trainer - Validating
2022-03-21 06:25:17,618 - INFO - tqdm - 0%|          | 0/1889 [00:00<?, ?it/s]
2022-03-21 06:25:27,716 - INFO - tqdm - f1: 0.8151, accuracy: 0.8721, batch_loss: 0.0446, loss: 0.3585 ||:  23%|##3       | 438/1889 [00:10<00:32, 44.04it/s]
2022-03-21 06:25:37,762 - INFO - tqdm - f1: 0.8111, accuracy: 0.8679, batch_loss: 0.2534, loss: 0.3745 ||:  45%|####5     | 855/1889 [00:20<00:21, 47.67it/s]
2022-03-21 06:25:47,918 - INFO - tqdm - f1: 0.8126, accuracy: 0.8686, batch_loss: 0.2546, loss: 0.3742 ||:  68%|######8   | 1290/1889 [00:30<00:14, 42.56it/s]
2022-03-21 06:25:57,937 - INFO - tqdm - f1: 0.8152, accuracy: 0.8718, batch_loss: 0.8821, loss: 0.3665 ||:  91%|######### | 1717/1889 [00:40<00:04, 41.40it/s]
2022-03-21 06:26:01,692 - INFO - tqdm - f1: 0.8157, accuracy: 0.8722, batch_loss: 0.0485, loss: 0.3662 ||: 100%|#########9| 1881/1889 [00:44<00:00, 44.33it/s]
2022-03-21 06:26:01,823 - INFO - tqdm - f1: 0.8157, accuracy: 0.8722, batch_loss: 0.2720, loss: 0.3662 ||: 100%|#########9| 1886/1889 [00:44<00:00, 42.31it/s]
2022-03-21 06:26:01,919 - INFO - tqdm - f1: 0.8158, accuracy: 0.8722, batch_loss: 0.1914, loss: 0.3662 ||: 100%|##########| 1889/1889 [00:44<00:00, 42.64it/s]
2022-03-21 06:26:01,933 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 06:26:01,934 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.889  |     0.872
2022-03-21 06:26:01,936 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.834  |     0.816
2022-03-21 06:26:01,937 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 06:26:01,939 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.309  |     0.366
2022-03-21 06:26:01,940 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8641.785  |       N/A
2022-03-21 06:26:01,941 - INFO - allennlp.training.trainer - Epoch duration: 0:13:33.505379
2022-03-21 06:26:01,943 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:34:53
2022-03-21 06:26:01,944 - INFO - allennlp.training.trainer - Epoch 3/9
2022-03-21 06:26:01,946 - INFO - allennlp.training.trainer - Worker 0 memory usage: 8.4G
2022-03-21 06:26:01,948 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 06:26:01,950 - INFO - allennlp.training.trainer - Training
2022-03-21 06:26:01,952 - INFO - tqdm - 0%|          | 0/11253 [00:00<?, ?it/s]
2022-03-21 06:26:12,039 - INFO - tqdm - f1: 0.8692, accuracy: 0.9106, batch_loss: 0.2061, loss: 0.2556 ||:   1%|1         | 137/11253 [00:10<12:38, 14.66it/s]
2022-03-21 06:26:22,201 - INFO - tqdm - f1: 0.8613, accuracy: 0.9090, batch_loss: 0.2549, loss: 0.2563 ||:   3%|2         | 285/11253 [00:20<13:30, 13.53it/s]
2022-03-21 06:26:32,238 - INFO - tqdm - f1: 0.8587, accuracy: 0.9066, batch_loss: 0.2282, loss: 0.2653 ||:   4%|3         | 431/11253 [00:30<12:56, 13.94it/s]
2022-03-21 06:26:42,245 - INFO - tqdm - f1: 0.8580, accuracy: 0.9076, batch_loss: 0.0598, loss: 0.2623 ||:   5%|5         | 579/11253 [00:40<12:32, 14.19it/s]
2022-03-21 06:26:52,299 - INFO - tqdm - f1: 0.8558, accuracy: 0.9057, batch_loss: 0.4183, loss: 0.2629 ||:   6%|6         | 729/11253 [00:50<11:32, 15.20it/s]
2022-03-21 06:27:02,309 - INFO - tqdm - f1: 0.8561, accuracy: 0.9057, batch_loss: 0.1990, loss: 0.2661 ||:   8%|7         | 877/11253 [01:00<12:01, 14.38it/s]
2022-03-21 06:27:12,367 - INFO - tqdm - f1: 0.8555, accuracy: 0.9054, batch_loss: 0.1918, loss: 0.2646 ||:   9%|9         | 1021/11253 [01:10<12:12, 13.96it/s]
2022-03-21 06:27:22,422 - INFO - tqdm - f1: 0.8551, accuracy: 0.9047, batch_loss: 0.1582, loss: 0.2669 ||:  10%|#         | 1169/11253 [01:20<11:18, 14.87it/s]
2022-03-21 06:27:32,510 - INFO - tqdm - f1: 0.8547, accuracy: 0.9045, batch_loss: 0.4610, loss: 0.2668 ||:  12%|#1        | 1319/11253 [01:30<10:59, 15.07it/s]
2022-03-21 06:27:42,592 - INFO - tqdm - f1: 0.8528, accuracy: 0.9030, batch_loss: 0.3522, loss: 0.2674 ||:  13%|#3        | 1471/11253 [01:40<10:59, 14.84it/s]
2022-03-21 06:27:52,605 - INFO - tqdm - f1: 0.8510, accuracy: 0.9023, batch_loss: 0.3378, loss: 0.2682 ||:  14%|#4        | 1617/11253 [01:50<11:03, 14.52it/s]
2022-03-21 06:28:02,642 - INFO - tqdm - f1: 0.8520, accuracy: 0.9033, batch_loss: 0.1808, loss: 0.2661 ||:  16%|#5        | 1763/11253 [02:00<10:13, 15.47it/s]
2022-03-21 06:28:12,743 - INFO - tqdm - f1: 0.8524, accuracy: 0.9033, batch_loss: 0.1515, loss: 0.2674 ||:  17%|#7        | 1915/11253 [02:10<10:23, 14.99it/s]
2022-03-21 06:28:22,775 - INFO - tqdm - f1: 0.8518, accuracy: 0.9035, batch_loss: 0.3581, loss: 0.2672 ||:  18%|#8        | 2063/11253 [02:20<10:27, 14.65it/s]
2022-03-21 06:28:32,845 - INFO - tqdm - f1: 0.8525, accuracy: 0.9038, batch_loss: 0.3426, loss: 0.2663 ||:  20%|#9        | 2211/11253 [02:30<09:44, 15.47it/s]
2022-03-21 06:28:42,994 - INFO - tqdm - f1: 0.8524, accuracy: 0.9039, batch_loss: 0.0374, loss: 0.2656 ||:  21%|##        | 2355/11253 [02:41<10:38, 13.95it/s]
2022-03-21 06:28:53,119 - INFO - tqdm - f1: 0.8521, accuracy: 0.9036, batch_loss: 0.5035, loss: 0.2654 ||:  22%|##2       | 2503/11253 [02:51<09:38, 15.12it/s]
2022-03-21 06:29:03,162 - INFO - tqdm - f1: 0.8517, accuracy: 0.9033, batch_loss: 0.3020, loss: 0.2660 ||:  24%|##3       | 2653/11253 [03:01<09:36, 14.91it/s]
2022-03-21 06:29:13,275 - INFO - tqdm - f1: 0.8510, accuracy: 0.9028, batch_loss: 0.8536, loss: 0.2663 ||:  25%|##4       | 2807/11253 [03:11<09:41, 14.54it/s]
2022-03-21 06:29:23,399 - INFO - tqdm - f1: 0.8507, accuracy: 0.9020, batch_loss: 0.6659, loss: 0.2674 ||:  26%|##6       | 2957/11253 [03:21<08:54, 15.52it/s]
2022-03-21 06:29:33,492 - INFO - tqdm - f1: 0.8503, accuracy: 0.9020, batch_loss: 0.3293, loss: 0.2675 ||:  28%|##7       | 3099/11253 [03:31<09:02, 15.02it/s]
2022-03-21 06:29:43,509 - INFO - tqdm - f1: 0.8503, accuracy: 0.9019, batch_loss: 0.3386, loss: 0.2683 ||:  29%|##8       | 3247/11253 [03:41<09:20, 14.28it/s]
2022-03-21 06:29:53,631 - INFO - tqdm - f1: 0.8505, accuracy: 0.9020, batch_loss: 0.2451, loss: 0.2684 ||:  30%|###       | 3399/11253 [03:51<08:39, 15.12it/s]
2022-03-21 06:30:03,698 - INFO - tqdm - f1: 0.8508, accuracy: 0.9022, batch_loss: 0.1262, loss: 0.2681 ||:  32%|###1      | 3551/11253 [04:01<08:18, 15.45it/s]
2022-03-21 06:30:13,809 - INFO - tqdm - f1: 0.8510, accuracy: 0.9026, batch_loss: 0.3458, loss: 0.2672 ||:  33%|###2      | 3701/11253 [04:11<08:51, 14.21it/s]
2022-03-21 06:30:23,918 - INFO - tqdm - f1: 0.8502, accuracy: 0.9021, batch_loss: 0.3373, loss: 0.2681 ||:  34%|###4      | 3843/11253 [04:21<08:18, 14.87it/s]
2022-03-21 06:30:33,948 - INFO - tqdm - f1: 0.8501, accuracy: 0.9021, batch_loss: 0.0981, loss: 0.2679 ||:  35%|###5      | 3991/11253 [04:31<07:54, 15.29it/s]
2022-03-21 06:30:43,958 - INFO - tqdm - f1: 0.8501, accuracy: 0.9020, batch_loss: 0.2978, loss: 0.2682 ||:  37%|###6      | 4139/11253 [04:42<07:44, 15.33it/s]
2022-03-21 06:30:54,042 - INFO - tqdm - f1: 0.8500, accuracy: 0.9020, batch_loss: 0.4190, loss: 0.2684 ||:  38%|###8      | 4287/11253 [04:52<07:41, 15.08it/s]
2022-03-21 06:31:04,130 - INFO - tqdm - f1: 0.8498, accuracy: 0.9017, batch_loss: 0.6618, loss: 0.2689 ||:  39%|###9      | 4429/11253 [05:02<07:59, 14.23it/s]
2022-03-21 06:31:14,133 - INFO - tqdm - f1: 0.8495, accuracy: 0.9016, batch_loss: 0.4236, loss: 0.2689 ||:  41%|####      | 4571/11253 [05:12<07:23, 15.08it/s]
2022-03-21 06:31:24,163 - INFO - tqdm - f1: 0.8493, accuracy: 0.9016, batch_loss: 0.0569, loss: 0.2686 ||:  42%|####1     | 4723/11253 [05:22<07:16, 14.96it/s]
2022-03-21 06:31:34,181 - INFO - tqdm - f1: 0.8494, accuracy: 0.9017, batch_loss: 0.4279, loss: 0.2688 ||:  43%|####3     | 4871/11253 [05:32<07:19, 14.52it/s]
2022-03-21 06:31:44,286 - INFO - tqdm - f1: 0.8495, accuracy: 0.9016, batch_loss: 0.2003, loss: 0.2693 ||:  44%|####4     | 5005/11253 [05:42<08:16, 12.58it/s]
2022-03-21 06:31:54,406 - INFO - tqdm - f1: 0.8497, accuracy: 0.9017, batch_loss: 0.0354, loss: 0.2693 ||:  46%|####5     | 5139/11253 [05:52<07:00, 14.53it/s]
2022-03-21 06:32:04,533 - INFO - tqdm - f1: 0.8497, accuracy: 0.9019, batch_loss: 0.1081, loss: 0.2688 ||:  47%|####6     | 5285/11253 [06:02<06:34, 15.11it/s]
2022-03-21 06:32:14,541 - INFO - tqdm - f1: 0.8496, accuracy: 0.9019, batch_loss: 0.3158, loss: 0.2684 ||:  48%|####8     | 5431/11253 [06:12<07:05, 13.69it/s]
2022-03-21 06:32:24,602 - INFO - tqdm - f1: 0.8495, accuracy: 0.9019, batch_loss: 0.5737, loss: 0.2686 ||:  50%|####9     | 5577/11253 [06:22<06:41, 14.14it/s]
2022-03-21 06:32:34,704 - INFO - tqdm - f1: 0.8494, accuracy: 0.9018, batch_loss: 0.0716, loss: 0.2687 ||:  51%|#####     | 5725/11253 [06:32<06:23, 14.43it/s]
2022-03-21 06:32:44,735 - INFO - tqdm - f1: 0.8488, accuracy: 0.9014, batch_loss: 0.0787, loss: 0.2690 ||:  52%|#####2    | 5867/11253 [06:42<06:09, 14.58it/s]
2022-03-21 06:32:54,798 - INFO - tqdm - f1: 0.8489, accuracy: 0.9014, batch_loss: 0.2631, loss: 0.2690 ||:  53%|#####3    | 6017/11253 [06:52<05:48, 15.02it/s]
2022-03-21 06:33:04,862 - INFO - tqdm - f1: 0.8487, accuracy: 0.9014, batch_loss: 0.0651, loss: 0.2693 ||:  55%|#####4    | 6167/11253 [07:02<05:35, 15.15it/s]
2022-03-21 06:33:14,866 - INFO - tqdm - f1: 0.8487, accuracy: 0.9014, batch_loss: 0.3926, loss: 0.2700 ||:  56%|#####6    | 6309/11253 [07:12<06:16, 13.15it/s]
2022-03-21 06:33:24,881 - INFO - tqdm - f1: 0.8489, accuracy: 0.9015, batch_loss: 0.1485, loss: 0.2699 ||:  57%|#####7    | 6455/11253 [07:22<05:19, 15.02it/s]
2022-03-21 06:33:34,893 - INFO - tqdm - f1: 0.8485, accuracy: 0.9012, batch_loss: 0.0626, loss: 0.2707 ||:  59%|#####8    | 6599/11253 [07:32<04:56, 15.69it/s]
2022-03-21 06:33:44,897 - INFO - tqdm - f1: 0.8484, accuracy: 0.9012, batch_loss: 0.4711, loss: 0.2707 ||:  60%|#####9    | 6749/11253 [07:42<05:03, 14.83it/s]
2022-03-21 06:33:54,934 - INFO - tqdm - f1: 0.8482, accuracy: 0.9010, batch_loss: 0.0496, loss: 0.2709 ||:  61%|######1   | 6897/11253 [07:52<05:12, 13.92it/s]
2022-03-21 06:34:05,040 - INFO - tqdm - f1: 0.8479, accuracy: 0.9008, batch_loss: 0.4960, loss: 0.2715 ||:  63%|######2   | 7039/11253 [08:03<04:42, 14.94it/s]
2022-03-21 06:34:15,087 - INFO - tqdm - f1: 0.8477, accuracy: 0.9006, batch_loss: 0.6314, loss: 0.2719 ||:  64%|######3   | 7187/11253 [08:13<04:42, 14.40it/s]
2022-03-21 06:34:25,127 - INFO - tqdm - f1: 0.8479, accuracy: 0.9007, batch_loss: 0.0052, loss: 0.2721 ||:  65%|######5   | 7333/11253 [08:23<04:22, 14.95it/s]
2022-03-21 06:34:35,160 - INFO - tqdm - f1: 0.8480, accuracy: 0.9008, batch_loss: 0.4082, loss: 0.2719 ||:  66%|######6   | 7483/11253 [08:33<04:13, 14.86it/s]
2022-03-21 06:34:45,166 - INFO - tqdm - f1: 0.8480, accuracy: 0.9007, batch_loss: 0.2657, loss: 0.2721 ||:  68%|######7   | 7631/11253 [08:43<03:54, 15.43it/s]
2022-03-21 06:34:55,235 - INFO - tqdm - f1: 0.8477, accuracy: 0.9004, batch_loss: 0.3532, loss: 0.2727 ||:  69%|######9   | 7775/11253 [08:53<03:49, 15.15it/s]
2022-03-21 06:35:05,251 - INFO - tqdm - f1: 0.8475, accuracy: 0.9002, batch_loss: 0.5855, loss: 0.2730 ||:  70%|#######   | 7921/11253 [09:03<03:50, 14.44it/s]
2022-03-21 06:35:15,296 - INFO - tqdm - f1: 0.8473, accuracy: 0.9002, batch_loss: 0.4713, loss: 0.2730 ||:  72%|#######1  | 8071/11253 [09:13<03:25, 15.46it/s]
2022-03-21 06:35:25,424 - INFO - tqdm - f1: 0.8470, accuracy: 0.9001, batch_loss: 0.2791, loss: 0.2730 ||:  73%|#######3  | 8225/11253 [09:23<03:18, 15.23it/s]
2022-03-21 06:35:35,538 - INFO - tqdm - f1: 0.8470, accuracy: 0.9001, batch_loss: 0.1945, loss: 0.2730 ||:  74%|#######4  | 8375/11253 [09:33<03:22, 14.24it/s]
2022-03-21 06:35:45,656 - INFO - tqdm - f1: 0.8468, accuracy: 0.9000, batch_loss: 0.2818, loss: 0.2733 ||:  76%|#######5  | 8527/11253 [09:43<02:53, 15.68it/s]
2022-03-21 06:35:55,751 - INFO - tqdm - f1: 0.8465, accuracy: 0.8997, batch_loss: 0.7873, loss: 0.2741 ||:  77%|#######7  | 8673/11253 [09:53<03:05, 13.92it/s]
2022-03-21 06:36:05,812 - INFO - tqdm - f1: 0.8463, accuracy: 0.8996, batch_loss: 0.2366, loss: 0.2743 ||:  78%|#######8  | 8819/11253 [10:03<02:50, 14.31it/s]
2022-03-21 06:36:15,860 - INFO - tqdm - f1: 0.8461, accuracy: 0.8995, batch_loss: 0.2807, loss: 0.2744 ||:  80%|#######9  | 8965/11253 [10:13<02:40, 14.29it/s]
2022-03-21 06:36:25,905 - INFO - tqdm - f1: 0.8459, accuracy: 0.8995, batch_loss: 0.0396, loss: 0.2746 ||:  81%|########  | 9113/11253 [10:23<02:22, 15.01it/s]
2022-03-21 06:36:36,018 - INFO - tqdm - f1: 0.8456, accuracy: 0.8993, batch_loss: 0.6520, loss: 0.2751 ||:  82%|########2 | 9265/11253 [10:34<02:12, 15.00it/s]
2022-03-21 06:36:46,064 - INFO - tqdm - f1: 0.8458, accuracy: 0.8993, batch_loss: 0.5404, loss: 0.2751 ||:  84%|########3 | 9411/11253 [10:44<02:02, 15.01it/s]
2022-03-21 06:36:56,158 - INFO - tqdm - f1: 0.8458, accuracy: 0.8994, batch_loss: 0.0105, loss: 0.2751 ||:  85%|########4 | 9561/11253 [10:54<01:48, 15.56it/s]
2022-03-21 06:37:06,273 - INFO - tqdm - f1: 0.8460, accuracy: 0.8994, batch_loss: 0.2223, loss: 0.2751 ||:  86%|########6 | 9709/11253 [11:04<01:42, 15.08it/s]
2022-03-21 06:37:16,364 - INFO - tqdm - f1: 0.8459, accuracy: 0.8993, batch_loss: 0.4260, loss: 0.2754 ||:  88%|########7 | 9859/11253 [11:14<01:32, 15.09it/s]
2022-03-21 06:37:26,479 - INFO - tqdm - f1: 0.8459, accuracy: 0.8993, batch_loss: 0.1908, loss: 0.2752 ||:  89%|########8 | 10007/11253 [11:24<01:26, 14.36it/s]
2022-03-21 06:37:36,533 - INFO - tqdm - f1: 0.8459, accuracy: 0.8992, batch_loss: 0.7442, loss: 0.2755 ||:  90%|######### | 10155/11253 [11:34<01:14, 14.71it/s]
2022-03-21 06:37:46,623 - INFO - tqdm - f1: 0.8461, accuracy: 0.8993, batch_loss: 0.1700, loss: 0.2757 ||:  92%|#########1| 10301/11253 [11:44<01:11, 13.34it/s]
2022-03-21 06:37:56,625 - INFO - tqdm - f1: 0.8460, accuracy: 0.8992, batch_loss: 0.1046, loss: 0.2758 ||:  93%|#########2| 10443/11253 [11:54<00:59, 13.68it/s]
2022-03-21 06:38:06,685 - INFO - tqdm - f1: 0.8461, accuracy: 0.8992, batch_loss: 0.1076, loss: 0.2760 ||:  94%|#########4| 10591/11253 [12:04<00:45, 14.46it/s]
2022-03-21 06:38:16,782 - INFO - tqdm - f1: 0.8460, accuracy: 0.8991, batch_loss: 0.2086, loss: 0.2761 ||:  95%|#########5| 10743/11253 [12:14<00:34, 14.83it/s]
2022-03-21 06:38:26,876 - INFO - tqdm - f1: 0.8463, accuracy: 0.8993, batch_loss: 0.2929, loss: 0.2760 ||:  97%|#########6| 10891/11253 [12:24<00:24, 14.71it/s]
2022-03-21 06:38:36,946 - INFO - tqdm - f1: 0.8464, accuracy: 0.8993, batch_loss: 0.1878, loss: 0.2762 ||:  98%|#########8| 11035/11253 [12:34<00:15, 14.20it/s]
2022-03-21 06:38:47,078 - INFO - tqdm - f1: 0.8463, accuracy: 0.8992, batch_loss: 0.1846, loss: 0.2766 ||:  99%|#########9| 11181/11253 [12:45<00:04, 14.66it/s]
2022-03-21 06:38:48,197 - INFO - tqdm - f1: 0.8463, accuracy: 0.8992, batch_loss: 0.6107, loss: 0.2767 ||: 100%|#########9| 11197/11253 [12:46<00:03, 14.71it/s]
2022-03-21 06:38:48,362 - INFO - tqdm - f1: 0.8462, accuracy: 0.8992, batch_loss: 0.3644, loss: 0.2767 ||: 100%|#########9| 11199/11253 [12:46<00:03, 13.85it/s]
2022-03-21 06:38:48,524 - INFO - tqdm - f1: 0.8462, accuracy: 0.8992, batch_loss: 0.0911, loss: 0.2767 ||: 100%|#########9| 11201/11253 [12:46<00:03, 13.34it/s]
2022-03-21 06:38:48,690 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.3807, loss: 0.2767 ||: 100%|#########9| 11203/11253 [12:46<00:03, 12.92it/s]
2022-03-21 06:38:48,844 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.1912, loss: 0.2767 ||: 100%|#########9| 11205/11253 [12:46<00:03, 12.96it/s]
2022-03-21 06:38:48,986 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.7433, loss: 0.2768 ||: 100%|#########9| 11207/11253 [12:47<00:03, 13.26it/s]
2022-03-21 06:38:49,141 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.2409, loss: 0.2768 ||: 100%|#########9| 11209/11253 [12:47<00:03, 13.17it/s]
2022-03-21 06:38:49,296 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.4915, loss: 0.2768 ||: 100%|#########9| 11211/11253 [12:47<00:03, 13.09it/s]
2022-03-21 06:38:49,460 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.0486, loss: 0.2768 ||: 100%|#########9| 11213/11253 [12:47<00:03, 12.80it/s]
2022-03-21 06:38:49,627 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.5856, loss: 0.2768 ||: 100%|#########9| 11215/11253 [12:47<00:03, 12.53it/s]
2022-03-21 06:38:49,790 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.3545, loss: 0.2768 ||: 100%|#########9| 11217/11253 [12:47<00:02, 12.45it/s]
2022-03-21 06:38:49,929 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.2306, loss: 0.2768 ||: 100%|#########9| 11219/11253 [12:47<00:02, 12.99it/s]
2022-03-21 06:38:50,085 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.0769, loss: 0.2768 ||: 100%|#########9| 11221/11253 [12:48<00:02, 12.92it/s]
2022-03-21 06:38:50,248 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.1643, loss: 0.2767 ||: 100%|#########9| 11223/11253 [12:48<00:02, 12.72it/s]
2022-03-21 06:38:50,401 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.4828, loss: 0.2768 ||: 100%|#########9| 11225/11253 [12:48<00:02, 12.84it/s]
2022-03-21 06:38:50,563 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.3935, loss: 0.2768 ||: 100%|#########9| 11227/11253 [12:48<00:02, 12.68it/s]
2022-03-21 06:38:50,708 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.1921, loss: 0.2767 ||: 100%|#########9| 11229/11253 [12:48<00:01, 13.00it/s]
2022-03-21 06:38:50,859 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.1653, loss: 0.2767 ||: 100%|#########9| 11231/11253 [12:48<00:01, 13.06it/s]
2022-03-21 06:38:51,015 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.2143, loss: 0.2767 ||: 100%|#########9| 11233/11253 [12:49<00:01, 13.01it/s]
2022-03-21 06:38:51,158 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.2738, loss: 0.2767 ||: 100%|#########9| 11235/11253 [12:49<00:01, 13.27it/s]
2022-03-21 06:38:51,304 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.1758, loss: 0.2767 ||: 100%|#########9| 11237/11253 [12:49<00:01, 13.41it/s]
2022-03-21 06:38:51,475 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.2657, loss: 0.2767 ||: 100%|#########9| 11239/11253 [12:49<00:01, 12.83it/s]
2022-03-21 06:38:51,616 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.6209, loss: 0.2767 ||: 100%|#########9| 11241/11253 [12:49<00:00, 13.22it/s]
2022-03-21 06:38:51,767 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.1229, loss: 0.2767 ||: 100%|#########9| 11243/11253 [12:49<00:00, 13.22it/s]
2022-03-21 06:38:51,926 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.7480, loss: 0.2768 ||: 100%|#########9| 11245/11253 [12:49<00:00, 13.01it/s]
2022-03-21 06:38:52,084 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.3934, loss: 0.2768 ||: 100%|#########9| 11247/11253 [12:50<00:00, 12.91it/s]
2022-03-21 06:38:52,258 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.6798, loss: 0.2768 ||: 100%|#########9| 11249/11253 [12:50<00:00, 12.45it/s]
2022-03-21 06:38:52,405 - INFO - tqdm - f1: 0.8462, accuracy: 0.8991, batch_loss: 0.4305, loss: 0.2768 ||: 100%|#########9| 11251/11253 [12:50<00:00, 12.79it/s]
2022-03-21 06:38:52,570 - INFO - tqdm - f1: 0.8461, accuracy: 0.8991, batch_loss: 0.0106, loss: 0.2768 ||: 100%|##########| 11253/11253 [12:50<00:00, 12.58it/s]
2022-03-21 06:38:52,658 - INFO - tqdm - f1: 0.8461, accuracy: 0.8991, batch_loss: 0.0106, loss: 0.2768 ||: 100%|##########| 11253/11253 [12:50<00:00, 14.60it/s]
2022-03-21 06:38:52,692 - INFO - allennlp.training.trainer - Validating
2022-03-21 06:38:52,714 - INFO - tqdm - 0%|          | 0/1889 [00:00<?, ?it/s]
2022-03-21 06:39:02,811 - INFO - tqdm - f1: 0.8173, accuracy: 0.8760, batch_loss: 0.4198, loss: 0.3569 ||:  23%|##3       | 439/1889 [00:10<00:35, 40.60it/s]
2022-03-21 06:39:12,824 - INFO - tqdm - f1: 0.8128, accuracy: 0.8705, batch_loss: 0.4280, loss: 0.3722 ||:  46%|####6     | 873/1889 [00:20<00:24, 41.81it/s]
2022-03-21 06:39:22,843 - INFO - tqdm - f1: 0.8159, accuracy: 0.8727, batch_loss: 0.1100, loss: 0.3652 ||:  69%|######8   | 1303/1889 [00:30<00:14, 39.34it/s]
2022-03-21 06:39:32,848 - INFO - tqdm - f1: 0.8152, accuracy: 0.8718, batch_loss: 0.0590, loss: 0.3650 ||:  91%|#########1| 1728/1889 [00:40<00:03, 46.93it/s]
2022-03-21 06:39:36,252 - INFO - tqdm - f1: 0.8150, accuracy: 0.8722, batch_loss: 1.2021, loss: 0.3655 ||: 100%|#########9| 1880/1889 [00:43<00:00, 48.99it/s]
2022-03-21 06:39:36,385 - INFO - tqdm - f1: 0.8150, accuracy: 0.8722, batch_loss: 0.3305, loss: 0.3654 ||: 100%|#########9| 1885/1889 [00:43<00:00, 45.29it/s]
2022-03-21 06:39:36,529 - INFO - tqdm - f1: 0.8151, accuracy: 0.8722, batch_loss: 0.4566, loss: 0.3652 ||: 100%|##########| 1889/1889 [00:43<00:00, 43.13it/s]
2022-03-21 06:39:36,544 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 06:39:36,546 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.899  |     0.872
2022-03-21 06:39:36,547 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.846  |     0.815
2022-03-21 06:39:36,549 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 06:39:36,550 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.277  |     0.365
2022-03-21 06:39:36,551 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8641.785  |       N/A
2022-03-21 06:39:36,566 - INFO - allennlp.training.trainer - Epoch duration: 0:13:34.621660
2022-03-21 06:39:36,577 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:21:22
2022-03-21 06:39:36,589 - INFO - allennlp.training.trainer - Epoch 4/9
2022-03-21 06:39:36,602 - INFO - allennlp.training.trainer - Worker 0 memory usage: 8.4G
2022-03-21 06:39:36,614 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 06:39:36,638 - INFO - allennlp.training.trainer - Training
2022-03-21 06:39:36,660 - INFO - tqdm - 0%|          | 0/11253 [00:00<?, ?it/s]
2022-03-21 06:39:46,786 - INFO - tqdm - f1: 0.8669, accuracy: 0.9159, batch_loss: 0.1900, loss: 0.2184 ||:   1%|1         | 139/11253 [00:10<12:29, 14.83it/s]
2022-03-21 06:39:56,898 - INFO - tqdm - f1: 0.8688, accuracy: 0.9182, batch_loss: 0.0792, loss: 0.2239 ||:   3%|2         | 285/11253 [00:20<12:16, 14.90it/s]
2022-03-21 06:40:06,912 - INFO - tqdm - f1: 0.8666, accuracy: 0.9164, batch_loss: 0.1188, loss: 0.2311 ||:   4%|3         | 433/11253 [00:30<12:27, 14.47it/s]
2022-03-21 06:40:16,958 - INFO - tqdm - f1: 0.8628, accuracy: 0.9140, batch_loss: 0.0760, loss: 0.2349 ||:   5%|5         | 581/11253 [00:40<11:33, 15.39it/s]
2022-03-21 06:40:26,961 - INFO - tqdm - f1: 0.8613, accuracy: 0.9133, batch_loss: 0.2564, loss: 0.2393 ||:   7%|6         | 733/11253 [00:50<11:02, 15.87it/s]
2022-03-21 06:40:36,964 - INFO - tqdm - f1: 0.8596, accuracy: 0.9129, batch_loss: 0.0109, loss: 0.2386 ||:   8%|7         | 881/11253 [01:00<12:04, 14.31it/s]
2022-03-21 06:40:47,039 - INFO - tqdm - f1: 0.8610, accuracy: 0.9138, batch_loss: 0.3774, loss: 0.2371 ||:   9%|9         | 1029/11253 [01:10<11:28, 14.85it/s]
2022-03-21 06:40:57,055 - INFO - tqdm - f1: 0.8611, accuracy: 0.9136, batch_loss: 0.1198, loss: 0.2371 ||:  10%|#         | 1175/11253 [01:20<10:54, 15.41it/s]
2022-03-21 06:41:07,072 - INFO - tqdm - f1: 0.8616, accuracy: 0.9137, batch_loss: 0.1453, loss: 0.2353 ||:  12%|#1        | 1327/11253 [01:30<10:34, 15.65it/s]
2022-03-21 06:41:17,118 - INFO - tqdm - f1: 0.8615, accuracy: 0.9138, batch_loss: 0.0471, loss: 0.2348 ||:  13%|#3        | 1479/11253 [01:40<10:44, 15.17it/s]
2022-03-21 06:41:27,233 - INFO - tqdm - f1: 0.8608, accuracy: 0.9131, batch_loss: 0.1530, loss: 0.2381 ||:  14%|#4        | 1627/11253 [01:50<10:51, 14.78it/s]
2022-03-21 06:41:37,265 - INFO - tqdm - f1: 0.8606, accuracy: 0.9128, batch_loss: 0.5808, loss: 0.2396 ||:  16%|#5        | 1769/11253 [02:00<11:04, 14.27it/s]
2022-03-21 06:41:47,343 - INFO - tqdm - f1: 0.8614, accuracy: 0.9133, batch_loss: 0.1574, loss: 0.2385 ||:  17%|#6        | 1911/11253 [02:10<10:21, 15.04it/s]
2022-03-21 06:41:57,497 - INFO - tqdm - f1: 0.8616, accuracy: 0.9132, batch_loss: 0.0890, loss: 0.2389 ||:  18%|#8        | 2057/11253 [02:20<11:00, 13.91it/s]
2022-03-21 06:42:07,596 - INFO - tqdm - f1: 0.8606, accuracy: 0.9123, batch_loss: 0.6364, loss: 0.2407 ||:  20%|#9        | 2205/11253 [02:30<10:16, 14.68it/s]
2022-03-21 06:42:17,651 - INFO - tqdm - f1: 0.8609, accuracy: 0.9121, batch_loss: 0.2665, loss: 0.2409 ||:  21%|##        | 2351/11253 [02:40<10:27, 14.18it/s]
2022-03-21 06:42:27,748 - INFO - tqdm - f1: 0.8604, accuracy: 0.9119, batch_loss: 0.2189, loss: 0.2419 ||:  22%|##2       | 2491/11253 [02:51<10:48, 13.51it/s]
2022-03-21 06:42:37,785 - INFO - tqdm - f1: 0.8604, accuracy: 0.9117, batch_loss: 0.1031, loss: 0.2419 ||:  23%|##3       | 2635/11253 [03:01<10:14, 14.04it/s]
2022-03-21 06:42:47,848 - INFO - tqdm - f1: 0.8597, accuracy: 0.9113, batch_loss: 0.1261, loss: 0.2419 ||:  25%|##4       | 2783/11253 [03:11<09:51, 14.33it/s]
2022-03-21 06:42:57,855 - INFO - tqdm - f1: 0.8606, accuracy: 0.9120, batch_loss: 0.2525, loss: 0.2410 ||:  26%|##6       | 2931/11253 [03:21<10:50, 12.79it/s]
2022-03-21 06:43:07,967 - INFO - tqdm - f1: 0.8607, accuracy: 0.9122, batch_loss: 0.0673, loss: 0.2404 ||:  27%|##7       | 3071/11253 [03:31<10:03, 13.55it/s]
2022-03-21 06:43:18,047 - INFO - tqdm - f1: 0.8613, accuracy: 0.9124, batch_loss: 0.1277, loss: 0.2396 ||:  29%|##8       | 3221/11253 [03:41<08:38, 15.49it/s]
2022-03-21 06:43:28,080 - INFO - tqdm - f1: 0.8606, accuracy: 0.9120, batch_loss: 0.2936, loss: 0.2401 ||:  30%|##9       | 3369/11253 [03:51<09:12, 14.26it/s]
2022-03-21 06:43:38,206 - INFO - tqdm - f1: 0.8605, accuracy: 0.9119, batch_loss: 1.0755, loss: 0.2408 ||:  31%|###1      | 3519/11253 [04:01<08:32, 15.09it/s]
2022-03-21 06:43:48,301 - INFO - tqdm - f1: 0.8605, accuracy: 0.9116, batch_loss: 0.2847, loss: 0.2415 ||:  33%|###2      | 3669/11253 [04:11<08:13, 15.37it/s]
2022-03-21 06:43:58,340 - INFO - tqdm - f1: 0.8607, accuracy: 0.9120, batch_loss: 0.0602, loss: 0.2404 ||:  34%|###3      | 3805/11253 [04:21<08:48, 14.08it/s]
2022-03-21 06:44:08,421 - INFO - tqdm - f1: 0.8605, accuracy: 0.9121, batch_loss: 0.1008, loss: 0.2400 ||:  35%|###5      | 3951/11253 [04:31<08:14, 14.76it/s]
2022-03-21 06:44:18,501 - INFO - tqdm - f1: 0.8612, accuracy: 0.9122, batch_loss: 0.0831, loss: 0.2399 ||:  36%|###6      | 4099/11253 [04:41<07:56, 15.00it/s]
2022-03-21 06:44:28,590 - INFO - tqdm - f1: 0.8609, accuracy: 0.9120, batch_loss: 0.0576, loss: 0.2403 ||:  38%|###7      | 4249/11253 [04:51<07:25, 15.73it/s]
2022-03-21 06:44:38,659 - INFO - tqdm - f1: 0.8607, accuracy: 0.9119, batch_loss: 0.0990, loss: 0.2406 ||:  39%|###9      | 4391/11253 [05:01<08:00, 14.27it/s]
2022-03-21 06:44:48,745 - INFO - tqdm - f1: 0.8604, accuracy: 0.9119, batch_loss: 0.2281, loss: 0.2407 ||:  40%|####      | 4533/11253 [05:12<07:33, 14.83it/s]
2022-03-21 06:44:58,817 - INFO - tqdm - f1: 0.8605, accuracy: 0.9117, batch_loss: 0.5568, loss: 0.2410 ||:  42%|####1     | 4687/11253 [05:22<07:07, 15.37it/s]
2022-03-21 06:45:08,848 - INFO - tqdm - f1: 0.8603, accuracy: 0.9115, batch_loss: 0.2421, loss: 0.2417 ||:  43%|####3     | 4841/11253 [05:32<07:07, 14.99it/s]
2022-03-21 06:45:18,900 - INFO - tqdm - f1: 0.8604, accuracy: 0.9117, batch_loss: 0.1753, loss: 0.2414 ||:  44%|####4     | 4993/11253 [05:42<07:18, 14.26it/s]
2022-03-21 06:45:29,044 - INFO - tqdm - f1: 0.8604, accuracy: 0.9117, batch_loss: 1.0393, loss: 0.2419 ||:  46%|####5     | 5131/11253 [05:52<07:21, 13.88it/s]
2022-03-21 06:45:39,162 - INFO - tqdm - f1: 0.8602, accuracy: 0.9116, batch_loss: 0.0929, loss: 0.2421 ||:  47%|####6     | 5279/11253 [06:02<06:47, 14.66it/s]
2022-03-21 06:45:49,178 - INFO - tqdm - f1: 0.8604, accuracy: 0.9115, batch_loss: 0.1478, loss: 0.2421 ||:  48%|####8     | 5429/11253 [06:12<06:39, 14.56it/s]
2022-03-21 06:45:59,250 - INFO - tqdm - f1: 0.8600, accuracy: 0.9111, batch_loss: 0.3902, loss: 0.2425 ||:  50%|####9     | 5581/11253 [06:22<06:09, 15.35it/s]
2022-03-21 06:46:09,291 - INFO - tqdm - f1: 0.8600, accuracy: 0.9111, batch_loss: 0.0591, loss: 0.2424 ||:  51%|#####     | 5729/11253 [06:32<06:24, 14.38it/s]
2022-03-21 06:46:19,379 - INFO - tqdm - f1: 0.8600, accuracy: 0.9110, batch_loss: 0.2680, loss: 0.2426 ||:  52%|#####2    | 5871/11253 [06:42<06:26, 13.91it/s]
2022-03-21 06:46:29,500 - INFO - tqdm - f1: 0.8600, accuracy: 0.9111, batch_loss: 0.3342, loss: 0.2421 ||:  54%|#####3    | 6021/11253 [06:52<05:36, 15.54it/s]
2022-03-21 06:46:39,591 - INFO - tqdm - f1: 0.8602, accuracy: 0.9112, batch_loss: 0.2622, loss: 0.2422 ||:  55%|#####4    | 6175/11253 [07:02<05:23, 15.68it/s]
2022-03-21 06:46:49,661 - INFO - tqdm - f1: 0.8604, accuracy: 0.9113, batch_loss: 0.1562, loss: 0.2421 ||:  56%|#####6    | 6331/11253 [07:12<05:22, 15.28it/s]
2022-03-21 06:46:59,712 - INFO - tqdm - f1: 0.8602, accuracy: 0.9111, batch_loss: 0.2494, loss: 0.2428 ||:  58%|#####7    | 6477/11253 [07:23<05:45, 13.83it/s]
2022-03-21 06:47:09,777 - INFO - tqdm - f1: 0.8599, accuracy: 0.9108, batch_loss: 0.2050, loss: 0.2433 ||:  59%|#####8    | 6619/11253 [07:33<05:26, 14.20it/s]
2022-03-21 06:47:19,881 - INFO - tqdm - f1: 0.8598, accuracy: 0.9107, batch_loss: 0.6242, loss: 0.2436 ||:  60%|######    | 6767/11253 [07:43<05:01, 14.86it/s]
2022-03-21 06:47:29,973 - INFO - tqdm - f1: 0.8599, accuracy: 0.9107, batch_loss: 0.2181, loss: 0.2437 ||:  61%|######1   | 6915/11253 [07:53<05:27, 13.23it/s]
2022-03-21 06:47:40,069 - INFO - tqdm - f1: 0.8598, accuracy: 0.9105, batch_loss: 0.1019, loss: 0.2440 ||:  62%|######2   | 7028/11253 [08:03<08:20,  8.44it/s]
2022-03-21 06:47:50,076 - INFO - tqdm - f1: 0.8595, accuracy: 0.9104, batch_loss: 0.2160, loss: 0.2444 ||:  63%|######3   | 7108/11253 [08:13<08:48,  7.84it/s]
2022-03-21 06:48:00,153 - INFO - tqdm - f1: 0.8595, accuracy: 0.9104, batch_loss: 0.1185, loss: 0.2442 ||:  64%|######4   | 7234/11253 [08:23<04:39, 14.40it/s]
2022-03-21 06:48:10,159 - INFO - tqdm - f1: 0.8594, accuracy: 0.9103, batch_loss: 0.2459, loss: 0.2442 ||:  66%|######5   | 7380/11253 [08:33<04:41, 13.76it/s]
2022-03-21 06:48:20,274 - INFO - tqdm - f1: 0.8592, accuracy: 0.9102, batch_loss: 0.4338, loss: 0.2445 ||:  67%|######6   | 7530/11253 [08:43<04:11, 14.83it/s]
2022-03-21 06:48:30,318 - INFO - tqdm - f1: 0.8592, accuracy: 0.9101, batch_loss: 0.1728, loss: 0.2447 ||:  68%|######8   | 7678/11253 [08:53<04:21, 13.68it/s]
2022-03-21 06:48:40,397 - INFO - tqdm - f1: 0.8593, accuracy: 0.9102, batch_loss: 0.0457, loss: 0.2445 ||:  69%|######9   | 7800/11253 [09:03<04:37, 12.46it/s]
2022-03-21 06:48:50,479 - INFO - tqdm - f1: 0.8591, accuracy: 0.9100, batch_loss: 0.3143, loss: 0.2449 ||:  71%|#######   | 7944/11253 [09:13<03:40, 15.00it/s]
2022-03-21 06:49:00,571 - INFO - tqdm - f1: 0.8592, accuracy: 0.9101, batch_loss: 0.0283, loss: 0.2447 ||:  72%|#######1  | 8094/11253 [09:23<03:34, 14.74it/s]
2022-03-21 06:49:10,610 - INFO - tqdm - f1: 0.8593, accuracy: 0.9102, batch_loss: 0.0656, loss: 0.2445 ||:  73%|#######3  | 8244/11253 [09:33<03:28, 14.44it/s]
2022-03-21 06:49:20,702 - INFO - tqdm - f1: 0.8591, accuracy: 0.9100, batch_loss: 0.4974, loss: 0.2451 ||:  75%|#######4  | 8392/11253 [09:44<03:24, 14.01it/s]
2022-03-21 06:49:30,744 - INFO - tqdm - f1: 0.8590, accuracy: 0.9100, batch_loss: 0.3278, loss: 0.2451 ||:  76%|#######5  | 8530/11253 [09:54<03:12, 14.18it/s]
2022-03-21 06:49:40,774 - INFO - tqdm - f1: 0.8591, accuracy: 0.9101, batch_loss: 0.2405, loss: 0.2447 ||:  77%|#######6  | 8662/11253 [10:04<03:16, 13.17it/s]
2022-03-21 06:49:50,859 - INFO - tqdm - f1: 0.8589, accuracy: 0.9099, batch_loss: 0.1884, loss: 0.2451 ||:  78%|#######8  | 8806/11253 [10:14<02:45, 14.81it/s]
2022-03-21 06:50:00,997 - INFO - tqdm - f1: 0.8588, accuracy: 0.9100, batch_loss: 0.2984, loss: 0.2451 ||:  80%|#######9  | 8948/11253 [10:24<02:55, 13.12it/s]
2022-03-21 06:50:11,101 - INFO - tqdm - f1: 0.8588, accuracy: 0.9100, batch_loss: 0.0711, loss: 0.2452 ||:  81%|########  | 9090/11253 [10:34<02:32, 14.22it/s]
2022-03-21 06:50:21,163 - INFO - tqdm - f1: 0.8587, accuracy: 0.9099, batch_loss: 0.1950, loss: 0.2456 ||:  82%|########2 | 9236/11253 [10:44<02:18, 14.54it/s]
2022-03-21 06:50:31,300 - INFO - tqdm - f1: 0.8586, accuracy: 0.9099, batch_loss: 0.0879, loss: 0.2455 ||:  83%|########3 | 9386/11253 [10:54<02:15, 13.82it/s]
2022-03-21 06:50:41,314 - INFO - tqdm - f1: 0.8588, accuracy: 0.9099, batch_loss: 0.0464, loss: 0.2456 ||:  85%|########4 | 9536/11253 [11:04<01:57, 14.59it/s]
2022-03-21 06:50:51,426 - INFO - tqdm - f1: 0.8586, accuracy: 0.9098, batch_loss: 0.1838, loss: 0.2454 ||:  86%|########6 | 9682/11253 [11:14<01:51, 14.04it/s]
2022-03-21 06:51:01,457 - INFO - tqdm - f1: 0.8585, accuracy: 0.9098, batch_loss: 0.0962, loss: 0.2455 ||:  87%|########7 | 9822/11253 [11:24<01:50, 12.95it/s]
2022-03-21 06:51:11,552 - INFO - tqdm - f1: 0.8584, accuracy: 0.9096, batch_loss: 0.2892, loss: 0.2457 ||:  88%|########7 | 9898/11253 [11:34<03:12,  7.05it/s]
2022-03-21 06:51:21,637 - INFO - tqdm - f1: 0.8582, accuracy: 0.9095, batch_loss: 0.1716, loss: 0.2458 ||:  89%|########8 | 9994/11253 [11:44<02:05, 10.06it/s]
2022-03-21 06:51:31,659 - INFO - tqdm - f1: 0.8580, accuracy: 0.9093, batch_loss: 0.0718, loss: 0.2462 ||:  90%|######### | 10144/11253 [11:54<01:12, 15.28it/s]
2022-03-21 06:51:41,703 - INFO - tqdm - f1: 0.8582, accuracy: 0.9093, batch_loss: 0.0950, loss: 0.2463 ||:  91%|#########1| 10294/11253 [12:05<01:09, 13.88it/s]
2022-03-21 06:51:51,759 - INFO - tqdm - f1: 0.8582, accuracy: 0.9093, batch_loss: 0.1972, loss: 0.2463 ||:  93%|#########2| 10438/11253 [12:15<00:56, 14.33it/s]
2022-03-21 06:52:01,793 - INFO - tqdm - f1: 0.8581, accuracy: 0.9093, batch_loss: 0.3492, loss: 0.2466 ||:  94%|#########4| 10586/11253 [12:25<00:43, 15.30it/s]
2022-03-21 06:52:11,814 - INFO - tqdm - f1: 0.8581, accuracy: 0.9093, batch_loss: 0.1506, loss: 0.2467 ||:  95%|#########5| 10728/11253 [12:35<00:36, 14.42it/s]
2022-03-21 06:52:21,867 - INFO - tqdm - f1: 0.8579, accuracy: 0.9090, batch_loss: 0.2301, loss: 0.2472 ||:  97%|#########6| 10876/11253 [12:45<00:24, 15.24it/s]
2022-03-21 06:52:31,988 - INFO - tqdm - f1: 0.8578, accuracy: 0.9089, batch_loss: 0.2048, loss: 0.2476 ||:  98%|#########7| 11020/11253 [12:55<00:16, 14.34it/s]
2022-03-21 06:52:42,096 - INFO - tqdm - f1: 0.8578, accuracy: 0.9088, batch_loss: 0.5983, loss: 0.2477 ||:  99%|#########9| 11164/11253 [13:05<00:06, 14.14it/s]
2022-03-21 06:52:44,508 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.1708, loss: 0.2478 ||: 100%|#########9| 11198/11253 [13:07<00:04, 13.72it/s]
2022-03-21 06:52:44,662 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.3052, loss: 0.2478 ||: 100%|#########9| 11200/11253 [13:08<00:03, 13.49it/s]
2022-03-21 06:52:44,830 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.2928, loss: 0.2478 ||: 100%|#########9| 11202/11253 [13:08<00:03, 12.97it/s]
2022-03-21 06:52:44,993 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.4072, loss: 0.2479 ||: 100%|#########9| 11204/11253 [13:08<00:03, 12.75it/s]
2022-03-21 06:52:45,193 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.0508, loss: 0.2479 ||: 100%|#########9| 11206/11253 [13:08<00:03, 11.79it/s]
2022-03-21 06:52:45,336 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.1336, loss: 0.2478 ||: 100%|#########9| 11208/11253 [13:08<00:03, 12.36it/s]
2022-03-21 06:52:45,516 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.2440, loss: 0.2478 ||: 100%|#########9| 11210/11253 [13:08<00:03, 11.96it/s]
2022-03-21 06:52:45,676 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.1065, loss: 0.2478 ||: 100%|#########9| 11212/11253 [13:09<00:03, 12.11it/s]
2022-03-21 06:52:45,845 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.1443, loss: 0.2478 ||: 100%|#########9| 11214/11253 [13:09<00:03, 12.03it/s]
2022-03-21 06:52:46,027 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.6510, loss: 0.2478 ||: 100%|#########9| 11216/11253 [13:09<00:03, 11.70it/s]
2022-03-21 06:52:46,169 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.3167, loss: 0.2478 ||: 100%|#########9| 11218/11253 [13:09<00:02, 12.32it/s]
2022-03-21 06:52:46,321 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.6276, loss: 0.2478 ||: 100%|#########9| 11220/11253 [13:09<00:02, 12.58it/s]
2022-03-21 06:52:46,481 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.4146, loss: 0.2479 ||: 100%|#########9| 11222/11253 [13:09<00:02, 12.54it/s]
2022-03-21 06:52:46,691 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.1717, loss: 0.2479 ||: 100%|#########9| 11224/11253 [13:10<00:02, 11.46it/s]
2022-03-21 06:52:46,855 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.0987, loss: 0.2479 ||: 100%|#########9| 11226/11253 [13:10<00:02, 11.68it/s]
2022-03-21 06:52:47,011 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.4990, loss: 0.2479 ||: 100%|#########9| 11228/11253 [13:10<00:02, 12.00it/s]
2022-03-21 06:52:47,142 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.0653, loss: 0.2479 ||: 100%|#########9| 11230/11253 [13:10<00:01, 12.80it/s]
2022-03-21 06:52:47,313 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.1563, loss: 0.2479 ||: 100%|#########9| 11232/11253 [13:10<00:01, 12.45it/s]
2022-03-21 06:52:47,470 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.3613, loss: 0.2479 ||: 100%|#########9| 11234/11253 [13:10<00:01, 12.54it/s]
2022-03-21 06:52:47,626 - INFO - tqdm - f1: 0.8576, accuracy: 0.9087, batch_loss: 0.4417, loss: 0.2479 ||: 100%|#########9| 11236/11253 [13:10<00:01, 12.62it/s]
2022-03-21 06:52:47,789 - INFO - tqdm - f1: 0.8576, accuracy: 0.9087, batch_loss: 0.3216, loss: 0.2479 ||: 100%|#########9| 11238/11253 [13:11<00:01, 12.52it/s]
2022-03-21 06:52:47,942 - INFO - tqdm - f1: 0.8576, accuracy: 0.9087, batch_loss: 0.1402, loss: 0.2479 ||: 100%|#########9| 11240/11253 [13:11<00:01, 12.68it/s]
2022-03-21 06:52:48,099 - INFO - tqdm - f1: 0.8576, accuracy: 0.9087, batch_loss: 0.2034, loss: 0.2479 ||: 100%|#########9| 11242/11253 [13:11<00:00, 12.70it/s]
2022-03-21 06:52:48,266 - INFO - tqdm - f1: 0.8576, accuracy: 0.9087, batch_loss: 0.9160, loss: 0.2479 ||: 100%|#########9| 11244/11253 [13:11<00:00, 12.46it/s]
2022-03-21 06:52:48,440 - INFO - tqdm - f1: 0.8576, accuracy: 0.9087, batch_loss: 0.6127, loss: 0.2480 ||: 100%|#########9| 11246/11253 [13:11<00:00, 12.17it/s]
2022-03-21 06:52:48,613 - INFO - tqdm - f1: 0.8576, accuracy: 0.9087, batch_loss: 0.7661, loss: 0.2480 ||: 100%|#########9| 11248/11253 [13:11<00:00, 11.98it/s]
2022-03-21 06:52:48,764 - INFO - tqdm - f1: 0.8576, accuracy: 0.9087, batch_loss: 0.2248, loss: 0.2480 ||: 100%|#########9| 11250/11253 [13:12<00:00, 12.32it/s]
2022-03-21 06:52:48,924 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.1144, loss: 0.2480 ||: 100%|#########9| 11252/11253 [13:12<00:00, 12.39it/s]
2022-03-21 06:52:49,089 - INFO - tqdm - f1: 0.8577, accuracy: 0.9087, batch_loss: 0.0554, loss: 0.2480 ||: 100%|##########| 11253/11253 [13:12<00:00, 14.20it/s]
2022-03-21 06:52:49,100 - INFO - allennlp.training.trainer - Validating
2022-03-21 06:52:49,103 - INFO - tqdm - 0%|          | 0/1889 [00:00<?, ?it/s]
2022-03-21 06:52:59,133 - INFO - tqdm - f1: 0.8082, accuracy: 0.8669, batch_loss: 0.1548, loss: 0.3971 ||:  23%|##3       | 442/1889 [00:10<00:29, 49.71it/s]
2022-03-21 06:53:09,267 - INFO - tqdm - f1: 0.8102, accuracy: 0.8678, batch_loss: 0.0525, loss: 0.3963 ||:  46%|####5     | 868/1889 [00:20<00:25, 39.55it/s]
2022-03-21 06:53:19,280 - INFO - tqdm - f1: 0.8088, accuracy: 0.8663, batch_loss: 0.2728, loss: 0.4028 ||:  68%|######8   | 1292/1889 [00:30<00:14, 42.59it/s]
2022-03-21 06:53:29,438 - INFO - tqdm - f1: 0.8097, accuracy: 0.8671, batch_loss: 0.1889, loss: 0.3962 ||:  91%|#########1| 1723/1889 [00:40<00:04, 39.21it/s]
2022-03-21 06:53:33,086 - INFO - tqdm - f1: 0.8098, accuracy: 0.8671, batch_loss: 0.4235, loss: 0.3980 ||: 100%|#########9| 1881/1889 [00:43<00:00, 46.28it/s]
2022-03-21 06:53:33,240 - INFO - tqdm - f1: 0.8097, accuracy: 0.8671, batch_loss: 0.1032, loss: 0.3977 ||: 100%|#########9| 1886/1889 [00:44<00:00, 41.31it/s]
2022-03-21 06:53:33,369 - INFO - tqdm - f1: 0.8098, accuracy: 0.8672, batch_loss: 0.1293, loss: 0.3975 ||: 100%|##########| 1889/1889 [00:44<00:00, 42.69it/s]
2022-03-21 06:53:33,401 - INFO - allennlp.training.trainer - Ran out of patience.  Stopping training.
2022-03-21 06:53:33,426 - INFO - dont_stop_pretraining.training.roberta_checkpointer - loading best weights
2022-03-21 06:53:35,512 - INFO - allennlp.commands.train - The model will be evaluated using the best epoch weights.
2022-03-21 06:53:35,531 - INFO - allennlp.training.util - Iterating over dataset
2022-03-21 06:53:35,561 - INFO - tqdm - 0it [00:00, ?it/s]
2022-03-21 06:53:35,623 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-21 06:53:35,651 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-21 06:53:45,588 - INFO - tqdm - f1: 0.80, accuracy: 0.86, loss: 0.40 ||: : 438it [00:09, 36.82it/s]
2022-03-21 06:53:55,662 - INFO - tqdm - f1: 0.81, accuracy: 0.86, loss: 0.40 ||: : 879it [00:20, 32.73it/s]
2022-03-21 06:54:05,662 - INFO - tqdm - f1: 0.81, accuracy: 0.86, loss: 0.39 ||: : 1298it [00:30, 45.48it/s]
2022-03-21 06:54:15,709 - INFO - tqdm - f1: 0.81, accuracy: 0.87, loss: 0.39 ||: : 1720it [00:40, 37.75it/s]
2022-03-21 06:54:19,707 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 1,
  "peak_worker_0_memory_MB": 8641.78515625,
  "peak_gpu_0_memory_MB": 0,
  "training_duration": "0:54:14.842003",
  "training_start_epoch": 0,
  "training_epochs": 3,
  "epoch": 3,
  "training_f1": 0.8461454391479493,
  "training_accuracy": 0.8991001999555654,
  "training_loss": 0.27678674278483456,
  "training_worker_0_memory_MB": 8641.78515625,
  "training_gpu_0_memory_MB": 0.0,
  "validation_f1": 0.8150901317596435,
  "validation_accuracy": 0.8722030981067126,
  "validation_loss": 0.36520357432520306,
  "best_validation_f1": 0.8195441246032715,
  "best_validation_accuracy": 0.8753475440222428,
  "best_validation_loss": 0.3581674373558293,
  "test_f1": 0.8110930442810058,
  "test_accuracy": 0.8664675626348101,
  "test_loss": 0.38591716558061495
}
2022-03-21 06:54:19,943 - INFO - allennlp.models.archival - archiving weights and vocabulary to model_logs/rct-20k_base_hyper_small_seed_314/model.tar.gz
