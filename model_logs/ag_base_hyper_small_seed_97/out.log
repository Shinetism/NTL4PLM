2022-03-21 11:17:02,501 - INFO - allennlp.common.params - random_seed = 97
2022-03-21 11:17:02,536 - INFO - allennlp.common.params - numpy_seed = 97
2022-03-21 11:17:02,560 - INFO - allennlp.common.params - pytorch_seed = 97
2022-03-21 11:17:02,589 - INFO - allennlp.common.checks - Pytorch version: 1.8.0+cu111
2022-03-21 11:17:02,617 - INFO - allennlp.common.params - type = default
2022-03-21 11:17:02,646 - INFO - allennlp.common.params - dataset_reader.type = text_classification_json_with_sampling
2022-03-21 11:17:02,674 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer
2022-03-21 11:17:02,703 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2022-03-21 11:17:02,730 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = roberta-base
2022-03-21 11:17:02,757 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags
2022-03-21 11:17:02,783 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512
2022-03-21 11:17:02,809 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None
2022-03-21 11:17:26,324 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer
2022-03-21 11:17:26,327 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = roberta-base
2022-03-21 11:17:26,345 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True
2022-03-21 11:17:26,364 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = 512
2022-03-21 11:17:26,386 - INFO - allennlp.common.params - dataset_reader.tokenizer.stride = 0
2022-03-21 11:17:26,401 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None
2022-03-21 11:17:26,420 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = 512
2022-03-21 11:17:26,438 - INFO - allennlp.common.params - dataset_reader.sample = None
2022-03-21 11:17:26,456 - INFO - allennlp.common.params - dataset_reader.skip_label_indexing = False
2022-03-21 11:17:26,475 - INFO - allennlp.common.params - train_data_path = datasets/ag/train.jsonl
2022-03-21 11:17:26,498 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f1867c2f110>
2022-03-21 11:17:26,516 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-03-21 11:17:26,535 - INFO - allennlp.common.params - validation_dataset_reader.type = text_classification_json_with_sampling
2022-03-21 11:17:26,554 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer
2022-03-21 11:17:26,573 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2022-03-21 11:17:26,591 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = roberta-base
2022-03-21 11:17:26,610 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags
2022-03-21 11:17:26,628 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512
2022-03-21 11:17:26,630 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None
2022-03-21 11:17:26,633 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.type = pretrained_transformer
2022-03-21 11:17:26,635 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.model_name = roberta-base
2022-03-21 11:17:26,637 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.add_special_tokens = True
2022-03-21 11:17:26,638 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.max_length = 512
2022-03-21 11:17:26,640 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.stride = 0
2022-03-21 11:17:26,663 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.tokenizer_kwargs = None
2022-03-21 11:17:26,682 - INFO - allennlp.common.params - validation_dataset_reader.max_sequence_length = 512
2022-03-21 11:17:26,701 - INFO - allennlp.common.params - validation_dataset_reader.sample = None
2022-03-21 11:17:26,719 - INFO - allennlp.common.params - validation_dataset_reader.skip_label_indexing = False
2022-03-21 11:17:26,738 - INFO - allennlp.common.params - validation_data_path = datasets/ag/dev.jsonl
2022-03-21 11:17:26,756 - INFO - allennlp.common.params - validation_data_loader = None
2022-03-21 11:17:26,775 - INFO - allennlp.common.params - test_data_path = datasets/ag/test.jsonl
2022-03-21 11:17:26,794 - INFO - allennlp.common.params - evaluate_on_test = True
2022-03-21 11:17:26,812 - INFO - allennlp.common.params - batch_weight_key = 
2022-03-21 11:17:26,847 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-21 11:17:26,865 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-21 11:17:26,884 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-21 11:17:26,902 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-21 11:17:26,920 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-21 11:17:26,939 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-21 11:17:26,957 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-21 11:17:26,976 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-21 11:17:26,995 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-21 11:17:27,013 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-21 11:17:27,031 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-21 11:17:27,050 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-21 11:17:27,052 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-21 11:17:27,053 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-21 11:17:27,056 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-21 11:17:37,091 - INFO - tqdm - loading instances: 31671it [00:10, 3600.45it/s]
2022-03-21 11:17:47,128 - INFO - tqdm - loading instances: 62499it [00:20, 3779.25it/s]
2022-03-21 11:17:57,148 - INFO - tqdm - loading instances: 93057it [00:30, 3351.50it/s]
2022-03-21 11:18:04,283 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-21 11:18:04,307 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-21 11:18:04,321 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-21 11:18:04,340 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-21 11:18:04,342 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-21 11:18:04,344 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-21 11:18:04,345 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-21 11:18:04,347 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-21 11:18:04,366 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-21 11:18:04,384 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-21 11:18:04,403 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-21 11:18:04,425 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-21 11:18:04,444 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-21 11:18:04,463 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-21 11:18:04,486 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-21 11:18:05,849 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-21 11:18:05,856 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-21 11:18:05,873 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-21 11:18:05,892 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-21 11:18:05,912 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-21 11:18:05,929 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-21 11:18:05,948 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-21 11:18:05,966 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-21 11:18:05,985 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-21 11:18:06,004 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-21 11:18:06,006 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-21 11:18:06,007 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-21 11:18:06,009 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-21 11:18:06,010 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-21 11:18:06,012 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-21 11:18:08,093 - INFO - allennlp.common.params - type = from_instances
2022-03-21 11:18:08,104 - INFO - allennlp.common.params - min_count = None
2022-03-21 11:18:08,122 - INFO - allennlp.common.params - max_vocab_size = None
2022-03-21 11:18:08,141 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-03-21 11:18:08,160 - INFO - allennlp.common.params - pretrained_files = None
2022-03-21 11:18:08,178 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-03-21 11:18:08,197 - INFO - allennlp.common.params - tokens_to_add = None
2022-03-21 11:18:08,216 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-03-21 11:18:08,234 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-03-21 11:18:08,254 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-03-21 11:18:08,273 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-03-21 11:18:08,292 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-03-21 11:18:09,113 - INFO - allennlp.common.params - model.type = basic_classifier_with_f1
2022-03-21 11:18:09,129 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2022-03-21 11:18:09,147 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer
2022-03-21 11:18:09,166 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = roberta-base
2022-03-21 11:18:09,184 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512
2022-03-21 11:18:09,203 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_module = None
2022-03-21 11:18:09,222 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True
2022-03-21 11:18:09,241 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True
2022-03-21 11:18:09,260 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None
2022-03-21 11:18:09,279 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None
2022-03-21 11:18:09,301 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None
2022-03-21 11:18:09,320 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None
2022-03-21 11:18:09,347 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None
2022-03-21 11:18:14,995 - INFO - allennlp.common.params - model.seq2vec_encoder.type = cls_pooler
2022-03-21 11:18:15,009 - INFO - allennlp.common.params - model.seq2vec_encoder.embedding_dim = 768
2022-03-21 11:18:15,023 - INFO - allennlp.common.params - model.seq2vec_encoder.cls_is_last_token = False
2022-03-21 11:18:15,025 - INFO - allennlp.common.params - model.feedforward_layer.input_dim = 768
2022-03-21 11:18:15,027 - INFO - allennlp.common.params - model.feedforward_layer.num_layers = 1
2022-03-21 11:18:15,028 - INFO - allennlp.common.params - model.feedforward_layer.hidden_dims = 768
2022-03-21 11:18:15,030 - INFO - allennlp.common.params - model.feedforward_layer.activations = tanh
2022-03-21 11:18:15,048 - INFO - allennlp.common.params - type = tanh
2022-03-21 11:18:15,067 - INFO - allennlp.common.params - model.feedforward_layer.dropout = 0.0
2022-03-21 11:18:15,092 - INFO - allennlp.common.params - model.seq2seq_encoder = None
2022-03-21 11:18:15,106 - INFO - allennlp.common.params - model.dropout = 0.1
2022-03-21 11:18:15,107 - INFO - allennlp.common.params - model.num_labels = None
2022-03-21 11:18:15,131 - INFO - allennlp.common.params - model.label_namespace = labels
2022-03-21 11:18:15,150 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f19d9c89210>
2022-03-21 11:18:15,172 - INFO - allennlp.common.params - model.regularizer = None
2022-03-21 11:18:15,174 - INFO - allennlp.common.params - model.track_weights = False
2022-03-21 11:18:15,175 - INFO - allennlp.common.params - model.disable_layers = []
2022-03-21 11:18:15,196 - INFO - allennlp.nn.initializers - Initializing parameters
2022-03-21 11:18:15,215 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-03-21 11:18:15,232 - INFO - allennlp.nn.initializers -    _classification_layer.bias
2022-03-21 11:18:15,251 - INFO - allennlp.nn.initializers -    _classification_layer.weight
2022-03-21 11:18:15,270 - INFO - allennlp.nn.initializers -    _feedforward_layer._linear_layers.0.bias
2022-03-21 11:18:15,288 - INFO - allennlp.nn.initializers -    _feedforward_layer._linear_layers.0.weight
2022-03-21 11:18:15,307 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias
2022-03-21 11:18:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight
2022-03-21 11:18:15,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight
2022-03-21 11:18:15,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight
2022-03-21 11:18:15,386 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight
2022-03-21 11:18:15,403 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2022-03-21 11:18:15,422 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2022-03-21 11:18:15,440 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias
2022-03-21 11:18:15,459 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight
2022-03-21 11:18:15,477 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias
2022-03-21 11:18:15,496 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight
2022-03-21 11:18:15,515 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias
2022-03-21 11:18:15,533 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight
2022-03-21 11:18:15,551 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias
2022-03-21 11:18:15,572 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight
2022-03-21 11:18:15,589 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias
2022-03-21 11:18:15,608 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight
2022-03-21 11:18:15,626 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias
2022-03-21 11:18:15,645 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight
2022-03-21 11:18:15,664 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias
2022-03-21 11:18:15,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight
2022-03-21 11:18:15,701 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2022-03-21 11:18:15,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2022-03-21 11:18:15,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias
2022-03-21 11:18:15,723 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight
2022-03-21 11:18:15,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias
2022-03-21 11:18:15,764 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight
2022-03-21 11:18:15,783 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias
2022-03-21 11:18:15,801 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight
2022-03-21 11:18:15,824 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias
2022-03-21 11:18:15,826 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight
2022-03-21 11:18:15,828 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias
2022-03-21 11:18:15,829 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight
2022-03-21 11:18:15,831 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias
2022-03-21 11:18:15,832 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight
2022-03-21 11:18:15,833 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias
2022-03-21 11:18:15,834 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight
2022-03-21 11:18:15,857 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2022-03-21 11:18:15,876 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2022-03-21 11:18:15,895 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias
2022-03-21 11:18:15,913 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight
2022-03-21 11:18:15,932 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias
2022-03-21 11:18:15,950 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight
2022-03-21 11:18:15,970 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias
2022-03-21 11:18:15,972 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight
2022-03-21 11:18:15,974 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias
2022-03-21 11:18:15,975 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight
2022-03-21 11:18:15,977 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias
2022-03-21 11:18:15,978 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight
2022-03-21 11:18:15,980 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias
2022-03-21 11:18:15,981 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight
2022-03-21 11:18:15,983 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias
2022-03-21 11:18:15,984 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight
2022-03-21 11:18:15,986 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2022-03-21 11:18:15,987 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2022-03-21 11:18:15,989 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias
2022-03-21 11:18:16,011 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight
2022-03-21 11:18:16,013 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias
2022-03-21 11:18:16,033 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight
2022-03-21 11:18:16,034 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias
2022-03-21 11:18:16,053 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight
2022-03-21 11:18:16,072 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias
2022-03-21 11:18:16,091 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight
2022-03-21 11:18:16,110 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias
2022-03-21 11:18:16,112 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight
2022-03-21 11:18:16,114 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias
2022-03-21 11:18:16,115 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight
2022-03-21 11:18:16,117 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias
2022-03-21 11:18:16,118 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight
2022-03-21 11:18:16,119 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2022-03-21 11:18:16,121 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2022-03-21 11:18:16,122 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias
2022-03-21 11:18:16,124 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight
2022-03-21 11:18:16,125 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias
2022-03-21 11:18:16,126 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight
2022-03-21 11:18:16,128 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias
2022-03-21 11:18:16,129 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight
2022-03-21 11:18:16,130 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias
2022-03-21 11:18:16,132 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight
2022-03-21 11:18:16,133 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias
2022-03-21 11:18:16,135 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight
2022-03-21 11:18:16,136 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias
2022-03-21 11:18:16,137 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight
2022-03-21 11:18:16,139 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias
2022-03-21 11:18:16,140 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight
2022-03-21 11:18:16,141 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2022-03-21 11:18:16,143 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2022-03-21 11:18:16,144 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias
2022-03-21 11:18:16,146 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight
2022-03-21 11:18:16,147 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias
2022-03-21 11:18:16,149 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight
2022-03-21 11:18:16,150 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias
2022-03-21 11:18:16,151 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight
2022-03-21 11:18:16,153 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias
2022-03-21 11:18:16,154 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight
2022-03-21 11:18:16,156 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias
2022-03-21 11:18:16,157 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight
2022-03-21 11:18:16,158 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias
2022-03-21 11:18:16,160 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight
2022-03-21 11:18:16,161 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias
2022-03-21 11:18:16,162 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight
2022-03-21 11:18:16,164 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2022-03-21 11:18:16,165 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2022-03-21 11:18:16,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias
2022-03-21 11:18:16,171 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight
2022-03-21 11:18:16,172 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias
2022-03-21 11:18:16,174 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight
2022-03-21 11:18:16,175 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias
2022-03-21 11:18:16,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight
2022-03-21 11:18:16,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias
2022-03-21 11:18:16,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight
2022-03-21 11:18:16,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias
2022-03-21 11:18:16,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight
2022-03-21 11:18:16,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias
2022-03-21 11:18:16,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight
2022-03-21 11:18:16,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias
2022-03-21 11:18:16,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight
2022-03-21 11:18:16,190 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2022-03-21 11:18:16,191 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2022-03-21 11:18:16,192 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias
2022-03-21 11:18:16,194 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight
2022-03-21 11:18:16,195 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias
2022-03-21 11:18:16,197 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight
2022-03-21 11:18:16,198 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias
2022-03-21 11:18:16,199 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight
2022-03-21 11:18:16,201 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias
2022-03-21 11:18:16,202 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight
2022-03-21 11:18:16,205 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias
2022-03-21 11:18:16,206 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight
2022-03-21 11:18:16,208 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias
2022-03-21 11:18:16,235 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight
2022-03-21 11:18:16,249 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias
2022-03-21 11:18:16,267 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight
2022-03-21 11:18:16,286 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2022-03-21 11:18:16,304 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2022-03-21 11:18:16,323 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias
2022-03-21 11:18:16,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight
2022-03-21 11:18:16,361 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias
2022-03-21 11:18:16,379 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight
2022-03-21 11:18:16,398 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias
2022-03-21 11:18:16,416 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight
2022-03-21 11:18:16,435 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias
2022-03-21 11:18:16,457 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight
2022-03-21 11:18:16,458 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias
2022-03-21 11:18:16,478 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight
2022-03-21 11:18:16,497 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias
2022-03-21 11:18:16,516 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight
2022-03-21 11:18:16,534 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias
2022-03-21 11:18:16,552 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight
2022-03-21 11:18:16,571 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2022-03-21 11:18:16,590 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2022-03-21 11:18:16,608 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias
2022-03-21 11:18:16,627 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight
2022-03-21 11:18:16,645 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias
2022-03-21 11:18:16,664 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight
2022-03-21 11:18:16,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias
2022-03-21 11:18:16,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight
2022-03-21 11:18:16,686 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias
2022-03-21 11:18:16,687 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight
2022-03-21 11:18:16,690 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias
2022-03-21 11:18:16,691 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight
2022-03-21 11:18:16,692 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias
2022-03-21 11:18:16,695 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight
2022-03-21 11:18:16,696 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias
2022-03-21 11:18:16,697 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight
2022-03-21 11:18:16,699 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2022-03-21 11:18:16,700 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2022-03-21 11:18:16,701 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias
2022-03-21 11:18:16,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight
2022-03-21 11:18:16,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias
2022-03-21 11:18:16,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight
2022-03-21 11:18:16,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias
2022-03-21 11:18:16,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight
2022-03-21 11:18:16,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias
2022-03-21 11:18:16,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight
2022-03-21 11:18:16,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias
2022-03-21 11:18:16,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight
2022-03-21 11:18:16,735 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias
2022-03-21 11:18:16,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight
2022-03-21 11:18:16,773 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias
2022-03-21 11:18:16,791 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight
2022-03-21 11:18:16,810 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2022-03-21 11:18:16,828 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2022-03-21 11:18:16,847 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias
2022-03-21 11:18:16,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight
2022-03-21 11:18:16,884 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias
2022-03-21 11:18:16,903 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight
2022-03-21 11:18:16,921 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias
2022-03-21 11:18:16,940 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight
2022-03-21 11:18:16,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias
2022-03-21 11:18:16,976 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight
2022-03-21 11:18:16,995 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias
2022-03-21 11:18:17,013 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight
2022-03-21 11:18:17,032 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias
2022-03-21 11:18:17,051 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight
2022-03-21 11:18:17,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias
2022-03-21 11:18:17,088 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight
2022-03-21 11:18:17,111 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias
2022-03-21 11:18:17,125 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight
2022-03-21 11:18:23,085 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-03-21 11:18:23,094 - INFO - allennlp.common.params - trainer.patience = 3
2022-03-21 11:18:23,109 - INFO - allennlp.common.params - trainer.validation_metric = +f1
2022-03-21 11:18:23,128 - INFO - allennlp.common.params - trainer.num_epochs = 10
2022-03-21 11:18:23,153 - INFO - allennlp.common.params - trainer.cuda_device = 2
2022-03-21 11:18:23,169 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-03-21 11:18:23,185 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-03-21 11:18:23,204 - INFO - allennlp.common.params - trainer.distributed = False
2022-03-21 11:18:23,223 - INFO - allennlp.common.params - trainer.world_size = 1
2022-03-21 11:18:23,244 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1
2022-03-21 11:18:23,262 - INFO - allennlp.common.params - trainer.use_amp = False
2022-03-21 11:18:23,281 - INFO - allennlp.common.params - trainer.no_grad = None
2022-03-21 11:18:23,300 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2022-03-21 11:18:23,318 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-03-21 11:18:23,336 - INFO - allennlp.common.params - trainer.moving_average = None
2022-03-21 11:18:23,354 - INFO - allennlp.common.params - trainer.callbacks = None
2022-03-21 11:18:23,374 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True
2022-03-21 11:18:30,216 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw_str_lr
2022-03-21 11:18:30,225 - INFO - allennlp.common.params - trainer.optimizer.lr = 2e-05
2022-03-21 11:18:30,239 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-03-21 11:18:30,257 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-03-21 11:18:30,275 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-03-21 11:18:30,295 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = False
2022-03-21 11:18:30,315 - INFO - allennlp.training.optimizers - Done constructing parameter groups.
2022-03-21 11:18:30,331 - INFO - allennlp.training.optimizers - Group 0: ['_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias', '_classification_layer.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias', '_feedforward_layer._linear_layers.0.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias'], {'weight_decay': 0}
2022-03-21 11:18:30,350 - INFO - allennlp.training.optimizers - Group 1: ['_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight', '_feedforward_layer._linear_layers.0.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight', '_classification_layer.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight'], {}
2022-03-21 11:18:30,369 - WARNING - allennlp.training.optimizers - When constructing parameter groups, layer_norm.weight does not match any parameter name
2022-03-21 11:18:30,387 - INFO - allennlp.training.optimizers - Number of trainable parameters: 125239300
2022-03-21 11:18:30,406 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-03-21 11:18:30,425 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-03-21 11:18:30,443 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight
2022-03-21 11:18:30,462 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight
2022-03-21 11:18:30,481 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight
2022-03-21 11:18:30,500 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight
2022-03-21 11:18:30,519 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias
2022-03-21 11:18:30,541 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight
2022-03-21 11:18:30,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias
2022-03-21 11:18:30,578 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight
2022-03-21 11:18:30,600 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias
2022-03-21 11:18:30,616 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight
2022-03-21 11:18:30,634 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias
2022-03-21 11:18:30,653 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight
2022-03-21 11:18:30,672 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias
2022-03-21 11:18:30,691 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2022-03-21 11:18:30,710 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2022-03-21 11:18:30,728 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight
2022-03-21 11:18:30,748 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias
2022-03-21 11:18:30,767 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight
2022-03-21 11:18:30,792 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias
2022-03-21 11:18:30,807 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight
2022-03-21 11:18:30,826 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias
2022-03-21 11:18:30,844 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight
2022-03-21 11:18:30,863 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias
2022-03-21 11:18:30,881 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight
2022-03-21 11:18:30,900 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias
2022-03-21 11:18:30,919 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight
2022-03-21 11:18:30,938 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias
2022-03-21 11:18:30,956 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight
2022-03-21 11:18:30,975 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias
2022-03-21 11:18:30,994 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2022-03-21 11:18:31,013 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2022-03-21 11:18:31,031 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight
2022-03-21 11:18:31,050 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias
2022-03-21 11:18:31,069 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight
2022-03-21 11:18:31,087 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias
2022-03-21 11:18:31,106 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight
2022-03-21 11:18:31,108 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias
2022-03-21 11:18:31,110 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight
2022-03-21 11:18:31,111 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias
2022-03-21 11:18:31,113 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight
2022-03-21 11:18:31,136 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias
2022-03-21 11:18:31,155 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight
2022-03-21 11:18:31,178 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias
2022-03-21 11:18:31,180 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight
2022-03-21 11:18:31,181 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias
2022-03-21 11:18:31,183 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2022-03-21 11:18:31,206 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2022-03-21 11:18:31,226 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight
2022-03-21 11:18:31,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias
2022-03-21 11:18:31,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight
2022-03-21 11:18:31,266 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias
2022-03-21 11:18:31,285 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight
2022-03-21 11:18:31,303 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias
2022-03-21 11:18:31,322 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight
2022-03-21 11:18:31,341 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias
2022-03-21 11:18:31,360 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight
2022-03-21 11:18:31,378 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias
2022-03-21 11:18:31,397 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight
2022-03-21 11:18:31,416 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias
2022-03-21 11:18:31,418 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight
2022-03-21 11:18:31,419 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias
2022-03-21 11:18:31,421 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2022-03-21 11:18:31,422 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2022-03-21 11:18:31,424 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight
2022-03-21 11:18:31,425 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias
2022-03-21 11:18:31,427 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight
2022-03-21 11:18:31,449 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias
2022-03-21 11:18:31,468 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight
2022-03-21 11:18:31,488 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias
2022-03-21 11:18:31,505 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight
2022-03-21 11:18:31,523 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias
2022-03-21 11:18:31,541 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight
2022-03-21 11:18:31,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias
2022-03-21 11:18:31,577 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight
2022-03-21 11:18:31,596 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias
2022-03-21 11:18:31,600 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight
2022-03-21 11:18:31,601 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias
2022-03-21 11:18:31,603 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2022-03-21 11:18:31,604 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2022-03-21 11:18:31,605 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight
2022-03-21 11:18:31,606 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias
2022-03-21 11:18:31,628 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight
2022-03-21 11:18:31,646 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias
2022-03-21 11:18:31,664 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight
2022-03-21 11:18:31,665 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias
2022-03-21 11:18:31,667 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight
2022-03-21 11:18:31,668 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias
2022-03-21 11:18:31,669 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight
2022-03-21 11:18:31,670 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias
2022-03-21 11:18:31,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight
2022-03-21 11:18:31,708 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias
2022-03-21 11:18:31,726 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight
2022-03-21 11:18:31,727 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias
2022-03-21 11:18:31,729 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2022-03-21 11:18:31,731 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2022-03-21 11:18:31,732 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight
2022-03-21 11:18:31,733 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias
2022-03-21 11:18:31,735 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight
2022-03-21 11:18:31,736 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias
2022-03-21 11:18:31,762 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight
2022-03-21 11:18:31,776 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias
2022-03-21 11:18:31,794 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight
2022-03-21 11:18:31,812 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias
2022-03-21 11:18:31,830 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight
2022-03-21 11:18:31,832 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias
2022-03-21 11:18:31,833 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight
2022-03-21 11:18:31,834 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias
2022-03-21 11:18:31,836 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight
2022-03-21 11:18:31,837 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias
2022-03-21 11:18:31,839 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2022-03-21 11:18:31,840 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2022-03-21 11:18:31,841 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight
2022-03-21 11:18:31,843 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias
2022-03-21 11:18:31,844 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight
2022-03-21 11:18:31,845 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias
2022-03-21 11:18:31,864 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight
2022-03-21 11:18:31,883 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias
2022-03-21 11:18:31,884 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight
2022-03-21 11:18:31,906 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias
2022-03-21 11:18:31,925 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight
2022-03-21 11:18:31,927 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias
2022-03-21 11:18:31,928 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight
2022-03-21 11:18:31,930 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias
2022-03-21 11:18:31,950 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight
2022-03-21 11:18:31,968 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias
2022-03-21 11:18:31,987 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2022-03-21 11:18:32,006 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2022-03-21 11:18:32,024 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight
2022-03-21 11:18:32,026 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias
2022-03-21 11:18:32,027 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight
2022-03-21 11:18:32,029 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias
2022-03-21 11:18:32,030 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight
2022-03-21 11:18:32,032 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias
2022-03-21 11:18:32,033 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight
2022-03-21 11:18:32,034 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias
2022-03-21 11:18:32,036 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight
2022-03-21 11:18:32,037 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias
2022-03-21 11:18:32,039 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight
2022-03-21 11:18:32,040 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias
2022-03-21 11:18:32,041 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight
2022-03-21 11:18:32,042 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias
2022-03-21 11:18:32,044 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2022-03-21 11:18:32,045 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2022-03-21 11:18:32,046 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight
2022-03-21 11:18:32,048 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias
2022-03-21 11:18:32,050 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight
2022-03-21 11:18:32,051 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias
2022-03-21 11:18:32,052 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight
2022-03-21 11:18:32,053 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias
2022-03-21 11:18:32,054 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight
2022-03-21 11:18:32,055 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias
2022-03-21 11:18:32,057 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight
2022-03-21 11:18:32,058 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias
2022-03-21 11:18:32,060 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight
2022-03-21 11:18:32,061 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias
2022-03-21 11:18:32,062 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight
2022-03-21 11:18:32,064 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias
2022-03-21 11:18:32,065 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2022-03-21 11:18:32,066 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2022-03-21 11:18:32,068 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight
2022-03-21 11:18:32,069 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias
2022-03-21 11:18:32,071 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight
2022-03-21 11:18:32,072 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias
2022-03-21 11:18:32,073 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight
2022-03-21 11:18:32,075 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias
2022-03-21 11:18:32,076 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight
2022-03-21 11:18:32,078 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias
2022-03-21 11:18:32,079 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight
2022-03-21 11:18:32,080 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias
2022-03-21 11:18:32,082 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight
2022-03-21 11:18:32,083 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias
2022-03-21 11:18:32,086 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight
2022-03-21 11:18:32,108 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias
2022-03-21 11:18:32,127 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2022-03-21 11:18:32,145 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2022-03-21 11:18:32,165 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight
2022-03-21 11:18:32,183 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias
2022-03-21 11:18:32,205 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight
2022-03-21 11:18:32,207 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias
2022-03-21 11:18:32,208 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight
2022-03-21 11:18:32,210 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias
2022-03-21 11:18:32,211 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight
2022-03-21 11:18:32,212 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias
2022-03-21 11:18:32,214 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight
2022-03-21 11:18:32,215 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias
2022-03-21 11:18:32,238 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight
2022-03-21 11:18:32,257 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias
2022-03-21 11:18:32,276 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight
2022-03-21 11:18:32,295 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias
2022-03-21 11:18:32,313 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2022-03-21 11:18:32,315 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2022-03-21 11:18:32,316 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight
2022-03-21 11:18:32,335 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias
2022-03-21 11:18:32,354 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight
2022-03-21 11:18:32,379 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias
2022-03-21 11:18:32,392 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight
2022-03-21 11:18:32,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias
2022-03-21 11:18:32,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight
2022-03-21 11:18:32,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias
2022-03-21 11:18:32,416 - INFO - allennlp.common.util - _feedforward_layer._linear_layers.0.weight
2022-03-21 11:18:32,417 - INFO - allennlp.common.util - _feedforward_layer._linear_layers.0.bias
2022-03-21 11:18:32,440 - INFO - allennlp.common.util - _classification_layer.weight
2022-03-21 11:18:32,459 - INFO - allennlp.common.util - _classification_layer.bias
2022-03-21 11:18:32,478 - INFO - allennlp.common.params - trainer.checkpointer.type = roberta_default
2022-03-21 11:18:32,497 - INFO - allennlp.common.params - trainer.checkpointer.keep_serialized_model_every_num_seconds = None
2022-03-21 11:18:32,515 - INFO - allennlp.common.params - trainer.checkpointer.num_serialized_models_to_keep = 0
2022-03-21 11:18:32,556 - INFO - allennlp.common.params - trainer.checkpointer.model_save_interval = None
2022-03-21 11:18:32,593 - INFO - allennlp.common.params - trainer.checkpointer.num_epochs = 10
2022-03-21 11:18:32,626 - INFO - allennlp.common.params - trainer.checkpointer.skip_early_stopping = False
2022-03-21 11:18:32,692 - INFO - allennlp.training.trainer - Beginning training.
2022-03-21 11:18:32,727 - INFO - allennlp.training.trainer - Epoch 0/9
2022-03-21 11:18:32,759 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.2G
2022-03-21 11:18:32,791 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 11:18:32,824 - INFO - allennlp.training.trainer - Training
2022-03-21 11:18:32,855 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 11:18:33,019 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-21 11:18:33,029 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-21 11:18:42,879 - INFO - tqdm - f1: 0.6982, accuracy: 0.6958, batch_loss: 0.4076, loss: 0.7428 ||:   1%|1         | 75/7188 [00:09<14:06,  8.40it/s]
2022-03-21 11:18:52,994 - INFO - tqdm - f1: 0.8081, accuracy: 0.8069, batch_loss: 0.2281, loss: 0.5132 ||:   2%|2         | 179/7188 [00:20<12:00,  9.73it/s]
2022-03-21 11:19:03,102 - INFO - tqdm - f1: 0.8430, accuracy: 0.8428, batch_loss: 0.2933, loss: 0.4385 ||:   4%|3         | 285/7188 [00:30<15:03,  7.64it/s]
2022-03-21 11:19:13,163 - INFO - tqdm - f1: 0.8560, accuracy: 0.8564, batch_loss: 0.3489, loss: 0.4051 ||:   5%|5         | 393/7188 [00:40<16:10,  7.00it/s]
2022-03-21 11:19:23,257 - INFO - tqdm - f1: 0.8628, accuracy: 0.8630, batch_loss: 0.4315, loss: 0.3936 ||:   7%|6         | 499/7188 [00:50<19:10,  5.81it/s]
2022-03-21 11:19:33,372 - INFO - tqdm - f1: 0.8687, accuracy: 0.8690, batch_loss: 0.0498, loss: 0.3757 ||:   8%|8         | 607/7188 [01:00<15:27,  7.10it/s]
2022-03-21 11:19:43,398 - INFO - tqdm - f1: 0.8729, accuracy: 0.8726, batch_loss: 0.1511, loss: 0.3641 ||:  10%|9         | 713/7188 [01:10<18:30,  5.83it/s]
2022-03-21 11:19:53,577 - INFO - tqdm - f1: 0.8761, accuracy: 0.8760, batch_loss: 0.3222, loss: 0.3559 ||:  11%|#1        | 823/7188 [01:20<23:08,  4.58it/s]
2022-03-21 11:20:03,648 - INFO - tqdm - f1: 0.8797, accuracy: 0.8798, batch_loss: 0.0464, loss: 0.3460 ||:  13%|#2        | 929/7188 [01:30<22:34,  4.62it/s]
2022-03-21 11:20:13,681 - INFO - tqdm - f1: 0.8818, accuracy: 0.8818, batch_loss: 0.2124, loss: 0.3394 ||:  14%|#4        | 1033/7188 [01:40<18:04,  5.68it/s]
2022-03-21 11:20:23,780 - INFO - tqdm - f1: 0.8837, accuracy: 0.8838, batch_loss: 0.5212, loss: 0.3341 ||:  16%|#5        | 1139/7188 [01:50<17:22,  5.80it/s]
2022-03-21 11:20:33,831 - INFO - tqdm - f1: 0.8859, accuracy: 0.8859, batch_loss: 0.2834, loss: 0.3274 ||:  17%|#7        | 1247/7188 [02:00<16:52,  5.87it/s]
2022-03-21 11:20:44,111 - INFO - tqdm - f1: 0.8870, accuracy: 0.8869, batch_loss: 0.2181, loss: 0.3247 ||:  19%|#8        | 1355/7188 [02:11<21:29,  4.53it/s]
2022-03-21 11:20:54,220 - INFO - tqdm - f1: 0.8898, accuracy: 0.8898, batch_loss: 0.2746, loss: 0.3180 ||:  20%|##        | 1465/7188 [02:21<16:19,  5.84it/s]
2022-03-21 11:21:04,228 - INFO - tqdm - f1: 0.8913, accuracy: 0.8912, batch_loss: 0.2160, loss: 0.3153 ||:  22%|##1       | 1571/7188 [02:31<13:24,  6.98it/s]
2022-03-21 11:21:14,268 - INFO - tqdm - f1: 0.8916, accuracy: 0.8915, batch_loss: 0.9390, loss: 0.3144 ||:  23%|##3       | 1679/7188 [02:41<13:03,  7.03it/s]
2022-03-21 11:21:24,362 - INFO - tqdm - f1: 0.8935, accuracy: 0.8934, batch_loss: 0.1868, loss: 0.3089 ||:  25%|##4       | 1785/7188 [02:51<11:08,  8.08it/s]
2022-03-21 11:21:34,468 - INFO - tqdm - f1: 0.8944, accuracy: 0.8943, batch_loss: 0.4134, loss: 0.3056 ||:  26%|##6       | 1891/7188 [03:01<19:42,  4.48it/s]
2022-03-21 11:21:44,563 - INFO - tqdm - f1: 0.8958, accuracy: 0.8957, batch_loss: 0.7966, loss: 0.3017 ||:  28%|##7       | 1999/7188 [03:11<18:39,  4.63it/s]
2022-03-21 11:21:54,630 - INFO - tqdm - f1: 0.8977, accuracy: 0.8977, batch_loss: 0.3939, loss: 0.2971 ||:  29%|##9       | 2107/7188 [03:21<14:24,  5.88it/s]
2022-03-21 11:22:04,667 - INFO - tqdm - f1: 0.8982, accuracy: 0.8982, batch_loss: 0.8675, loss: 0.2949 ||:  31%|###       | 2213/7188 [03:31<11:39,  7.11it/s]
2022-03-21 11:22:14,717 - INFO - tqdm - f1: 0.8993, accuracy: 0.8993, batch_loss: 0.1369, loss: 0.2927 ||:  32%|###2      | 2321/7188 [03:41<09:48,  8.26it/s]
2022-03-21 11:22:24,739 - INFO - tqdm - f1: 0.9001, accuracy: 0.9001, batch_loss: 0.4273, loss: 0.2907 ||:  34%|###3      | 2425/7188 [03:51<13:52,  5.72it/s]
2022-03-21 11:22:34,859 - INFO - tqdm - f1: 0.9011, accuracy: 0.9011, batch_loss: 0.2651, loss: 0.2879 ||:  35%|###5      | 2533/7188 [04:01<13:31,  5.74it/s]
2022-03-21 11:22:45,142 - INFO - tqdm - f1: 0.9015, accuracy: 0.9015, batch_loss: 0.0457, loss: 0.2864 ||:  37%|###6      | 2643/7188 [04:12<16:35,  4.56it/s]
2022-03-21 11:22:55,199 - INFO - tqdm - f1: 0.9020, accuracy: 0.9019, batch_loss: 0.1834, loss: 0.2852 ||:  38%|###8      | 2753/7188 [04:22<15:56,  4.64it/s]
2022-03-21 11:23:05,652 - INFO - tqdm - f1: 0.9029, accuracy: 0.9028, batch_loss: 0.0396, loss: 0.2831 ||:  40%|###9      | 2865/7188 [04:32<16:27,  4.38it/s]
2022-03-21 11:23:16,109 - INFO - tqdm - f1: 0.9036, accuracy: 0.9035, batch_loss: 0.3545, loss: 0.2810 ||:  41%|####1     | 2975/7188 [04:43<15:40,  4.48it/s]
2022-03-21 11:23:26,114 - INFO - tqdm - f1: 0.9037, accuracy: 0.9036, batch_loss: 0.0468, loss: 0.2798 ||:  43%|####2     | 3081/7188 [04:53<14:58,  4.57it/s]
2022-03-21 11:23:36,243 - INFO - tqdm - f1: 0.9040, accuracy: 0.9038, batch_loss: 0.2855, loss: 0.2788 ||:  44%|####4     | 3187/7188 [05:03<11:37,  5.74it/s]
2022-03-21 11:23:46,251 - INFO - tqdm - f1: 0.9048, accuracy: 0.9047, batch_loss: 0.3603, loss: 0.2764 ||:  46%|####5     | 3289/7188 [05:13<14:40,  4.43it/s]
2022-03-21 11:23:56,507 - INFO - tqdm - f1: 0.9054, accuracy: 0.9053, batch_loss: 0.1786, loss: 0.2749 ||:  47%|####7     | 3397/7188 [05:23<13:53,  4.55it/s]
2022-03-21 11:24:06,609 - INFO - tqdm - f1: 0.9060, accuracy: 0.9060, batch_loss: 0.1559, loss: 0.2732 ||:  49%|####8     | 3505/7188 [05:33<13:38,  4.50it/s]
2022-03-21 11:24:16,660 - INFO - tqdm - f1: 0.9065, accuracy: 0.9064, batch_loss: 0.1325, loss: 0.2720 ||:  50%|#####     | 3609/7188 [05:43<13:05,  4.56it/s]
2022-03-21 11:24:26,778 - INFO - tqdm - f1: 0.9068, accuracy: 0.9068, batch_loss: 0.1113, loss: 0.2705 ||:  52%|#####1    | 3715/7188 [05:53<12:44,  4.54it/s]
2022-03-21 11:24:36,888 - INFO - tqdm - f1: 0.9069, accuracy: 0.9069, batch_loss: 0.2829, loss: 0.2695 ||:  53%|#####3    | 3819/7188 [06:04<09:59,  5.62it/s]
2022-03-21 11:24:47,051 - INFO - tqdm - f1: 0.9072, accuracy: 0.9071, batch_loss: 0.2085, loss: 0.2689 ||:  55%|#####4    | 3929/7188 [06:14<11:56,  4.55it/s]
2022-03-21 11:24:57,135 - INFO - tqdm - f1: 0.9074, accuracy: 0.9074, batch_loss: 0.0771, loss: 0.2680 ||:  56%|#####6    | 4035/7188 [06:24<11:35,  4.53it/s]
2022-03-21 11:25:07,265 - INFO - tqdm - f1: 0.9078, accuracy: 0.9078, batch_loss: 0.3403, loss: 0.2674 ||:  58%|#####7    | 4143/7188 [06:34<10:59,  4.61it/s]
2022-03-21 11:25:17,344 - INFO - tqdm - f1: 0.9082, accuracy: 0.9082, batch_loss: 0.1094, loss: 0.2659 ||:  59%|#####9    | 4249/7188 [06:44<08:19,  5.88it/s]
2022-03-21 11:25:27,392 - INFO - tqdm - f1: 0.9086, accuracy: 0.9086, batch_loss: 0.0310, loss: 0.2648 ||:  61%|######    | 4355/7188 [06:54<10:13,  4.62it/s]
2022-03-21 11:25:37,416 - INFO - tqdm - f1: 0.9089, accuracy: 0.9089, batch_loss: 0.1704, loss: 0.2638 ||:  62%|######2   | 4461/7188 [07:04<10:13,  4.45it/s]
2022-03-21 11:25:47,625 - INFO - tqdm - f1: 0.9093, accuracy: 0.9093, batch_loss: 0.4273, loss: 0.2625 ||:  64%|######3   | 4569/7188 [07:14<09:42,  4.50it/s]
2022-03-21 11:25:57,851 - INFO - tqdm - f1: 0.9093, accuracy: 0.9093, batch_loss: 0.2566, loss: 0.2624 ||:  65%|######5   | 4679/7188 [07:24<09:11,  4.55it/s]
2022-03-21 11:26:07,947 - INFO - tqdm - f1: 0.9095, accuracy: 0.9095, batch_loss: 0.2932, loss: 0.2617 ||:  67%|######6   | 4785/7188 [07:35<08:58,  4.46it/s]
2022-03-21 11:26:18,261 - INFO - tqdm - f1: 0.9099, accuracy: 0.9099, batch_loss: 0.6012, loss: 0.2602 ||:  68%|######8   | 4895/7188 [07:45<08:29,  4.50it/s]
2022-03-21 11:26:28,553 - INFO - tqdm - f1: 0.9102, accuracy: 0.9102, batch_loss: 0.9161, loss: 0.2591 ||:  70%|######9   | 5005/7188 [07:55<08:01,  4.53it/s]
2022-03-21 11:26:38,957 - INFO - tqdm - f1: 0.9107, accuracy: 0.9107, batch_loss: 0.2077, loss: 0.2579 ||:  71%|#######1  | 5115/7188 [08:06<07:34,  4.56it/s]
2022-03-21 11:26:49,048 - INFO - tqdm - f1: 0.9114, accuracy: 0.9114, batch_loss: 0.1822, loss: 0.2561 ||:  73%|#######2  | 5221/7188 [08:16<07:22,  4.45it/s]
2022-03-21 11:27:00,215 - INFO - tqdm - f1: 0.9117, accuracy: 0.9117, batch_loss: 0.1090, loss: 0.2550 ||:  74%|#######4  | 5339/7188 [08:27<07:17,  4.23it/s]
2022-03-21 11:27:10,894 - INFO - tqdm - f1: 0.9119, accuracy: 0.9119, batch_loss: 0.3760, loss: 0.2544 ||:  76%|#######5  | 5453/7188 [08:38<06:13,  4.65it/s]
2022-03-21 11:27:21,222 - INFO - tqdm - f1: 0.9122, accuracy: 0.9122, batch_loss: 0.1430, loss: 0.2534 ||:  77%|#######7  | 5563/7188 [08:48<05:53,  4.59it/s]
2022-03-21 11:27:31,341 - INFO - tqdm - f1: 0.9125, accuracy: 0.9125, batch_loss: 0.1120, loss: 0.2528 ||:  79%|#######8  | 5669/7188 [08:58<05:33,  4.55it/s]
2022-03-21 11:27:41,389 - INFO - tqdm - f1: 0.9129, accuracy: 0.9129, batch_loss: 0.3373, loss: 0.2517 ||:  80%|########  | 5773/7188 [09:08<04:11,  5.62it/s]
2022-03-21 11:27:51,441 - INFO - tqdm - f1: 0.9132, accuracy: 0.9133, batch_loss: 0.1865, loss: 0.2510 ||:  82%|########1 | 5877/7188 [09:18<03:52,  5.63it/s]
2022-03-21 11:28:01,566 - INFO - tqdm - f1: 0.9135, accuracy: 0.9135, batch_loss: 0.3561, loss: 0.2501 ||:  83%|########3 | 5985/7188 [09:28<04:22,  4.58it/s]
2022-03-21 11:28:11,647 - INFO - tqdm - f1: 0.9136, accuracy: 0.9136, batch_loss: 0.5089, loss: 0.2493 ||:  85%|########4 | 6089/7188 [09:38<02:39,  6.88it/s]
2022-03-21 11:28:21,701 - INFO - tqdm - f1: 0.9139, accuracy: 0.9139, batch_loss: 0.0835, loss: 0.2484 ||:  86%|########6 | 6193/7188 [09:48<02:59,  5.54it/s]
2022-03-21 11:28:31,949 - INFO - tqdm - f1: 0.9140, accuracy: 0.9140, batch_loss: 0.0474, loss: 0.2481 ||:  88%|########7 | 6301/7188 [09:59<03:19,  4.44it/s]
2022-03-21 11:28:42,040 - INFO - tqdm - f1: 0.9142, accuracy: 0.9143, batch_loss: 0.2868, loss: 0.2477 ||:  89%|########9 | 6407/7188 [10:09<02:50,  4.59it/s]
2022-03-21 11:28:52,110 - INFO - tqdm - f1: 0.9145, accuracy: 0.9145, batch_loss: 0.1047, loss: 0.2468 ||:  91%|######### | 6511/7188 [10:19<01:57,  5.77it/s]
2022-03-21 11:29:02,167 - INFO - tqdm - f1: 0.9148, accuracy: 0.9148, batch_loss: 0.5608, loss: 0.2459 ||:  92%|#########2| 6615/7188 [10:29<02:09,  4.43it/s]
2022-03-21 11:29:12,273 - INFO - tqdm - f1: 0.9150, accuracy: 0.9151, batch_loss: 0.0291, loss: 0.2452 ||:  94%|#########3| 6721/7188 [10:39<01:04,  7.19it/s]
2022-03-21 11:29:22,316 - INFO - tqdm - f1: 0.9153, accuracy: 0.9153, batch_loss: 0.1023, loss: 0.2443 ||:  95%|#########4| 6825/7188 [10:49<00:43,  8.33it/s]
2022-03-21 11:29:32,407 - INFO - tqdm - f1: 0.9155, accuracy: 0.9155, batch_loss: 0.2906, loss: 0.2437 ||:  96%|#########6| 6931/7188 [10:59<00:36,  7.02it/s]
2022-03-21 11:29:42,526 - INFO - tqdm - f1: 0.9156, accuracy: 0.9157, batch_loss: 0.1957, loss: 0.2432 ||:  98%|#########7| 7037/7188 [11:09<00:18,  8.30it/s]
2022-03-21 11:29:52,651 - INFO - tqdm - f1: 0.9158, accuracy: 0.9159, batch_loss: 0.1193, loss: 0.2424 ||:  99%|#########9| 7143/7188 [11:19<00:07,  6.31it/s]
2022-03-21 11:29:53,324 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.2798, loss: 0.2425 ||: 100%|#########9| 7153/7188 [11:20<00:02, 12.38it/s]
2022-03-21 11:29:53,457 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.3676, loss: 0.2425 ||: 100%|#########9| 7155/7188 [11:20<00:02, 13.01it/s]
2022-03-21 11:29:53,589 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.4027, loss: 0.2425 ||: 100%|#########9| 7157/7188 [11:20<00:02, 13.59it/s]
2022-03-21 11:29:53,734 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.0742, loss: 0.2425 ||: 100%|#########9| 7159/7188 [11:20<00:02, 13.64it/s]
2022-03-21 11:29:53,897 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.5760, loss: 0.2426 ||: 100%|#########9| 7161/7188 [11:21<00:02, 13.20it/s]
2022-03-21 11:29:54,056 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.0803, loss: 0.2425 ||: 100%|#########9| 7163/7188 [11:21<00:01, 13.00it/s]
2022-03-21 11:29:54,201 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.1441, loss: 0.2425 ||: 100%|#########9| 7165/7188 [11:21<00:01, 13.23it/s]
2022-03-21 11:29:54,338 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.2791, loss: 0.2425 ||: 100%|#########9| 7167/7188 [11:21<00:01, 13.62it/s]
2022-03-21 11:29:54,469 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.2065, loss: 0.2425 ||: 100%|#########9| 7169/7188 [11:21<00:01, 14.07it/s]
2022-03-21 11:29:54,620 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.4562, loss: 0.2425 ||: 100%|#########9| 7171/7188 [11:21<00:01, 13.81it/s]
2022-03-21 11:29:55,831 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.1334, loss: 0.2424 ||: 100%|#########9| 7173/7188 [11:22<00:03,  4.31it/s]
2022-03-21 11:29:55,984 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.1956, loss: 0.2424 ||: 100%|#########9| 7175/7188 [11:23<00:02,  5.39it/s]
2022-03-21 11:29:56,116 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.0645, loss: 0.2424 ||: 100%|#########9| 7177/7188 [11:23<00:01,  6.68it/s]
2022-03-21 11:29:56,246 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.2053, loss: 0.2423 ||: 100%|#########9| 7179/7188 [11:23<00:01,  8.04it/s]
2022-03-21 11:29:56,386 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.5684, loss: 0.2424 ||: 100%|#########9| 7181/7188 [11:23<00:00,  9.26it/s]
2022-03-21 11:29:56,523 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.2703, loss: 0.2423 ||: 100%|#########9| 7183/7188 [11:23<00:00, 10.41it/s]
2022-03-21 11:29:56,656 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.0095, loss: 0.2423 ||: 100%|#########9| 7185/7188 [11:23<00:00, 11.45it/s]
2022-03-21 11:29:56,794 - INFO - tqdm - f1: 0.9159, accuracy: 0.9158, batch_loss: 0.3636, loss: 0.2423 ||: 100%|#########9| 7187/7188 [11:23<00:00, 12.24it/s]
2022-03-21 11:29:56,931 - INFO - tqdm - f1: 0.9158, accuracy: 0.9158, batch_loss: 0.2419, loss: 0.2423 ||: 100%|##########| 7188/7188 [11:24<00:00, 10.51it/s]
2022-03-21 11:29:56,950 - INFO - allennlp.training.trainer - Validating
2022-03-21 11:29:56,969 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 11:29:56,997 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-21 11:29:57,009 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-21 11:30:06,994 - INFO - tqdm - f1: 0.9321, accuracy: 0.9322, batch_loss: 0.1129, loss: 0.2060 ||: 100%|##########| 313/313 [00:10<00:00, 31.28it/s]
2022-03-21 11:30:07,059 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/ag_base_hyper_small_seed_97/best.th'.
2022-03-21 11:30:12,050 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 11:30:12,053 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.916  |     0.932
2022-03-21 11:30:12,054 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.916  |     0.932
2022-03-21 11:30:12,075 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 11:30:12,094 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.242  |     0.206
2022-03-21 11:30:12,115 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7389.504  |       N/A
2022-03-21 11:30:12,137 - INFO - allennlp.training.trainer - Epoch duration: 0:11:39.410271
2022-03-21 11:30:12,157 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:44:54
2022-03-21 11:30:12,178 - INFO - allennlp.training.trainer - Epoch 1/9
2022-03-21 11:30:12,179 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-21 11:30:12,200 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 11:30:12,222 - INFO - allennlp.training.trainer - Training
2022-03-21 11:30:12,242 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 11:30:22,855 - INFO - tqdm - f1: 0.9408, accuracy: 0.9418, batch_loss: 0.1673, loss: 0.1781 ||:   1%|1         | 101/7188 [00:10<25:59,  4.54it/s]
2022-03-21 11:30:33,159 - INFO - tqdm - f1: 0.9388, accuracy: 0.9393, batch_loss: 0.0146, loss: 0.1667 ||:   3%|2         | 209/7188 [00:20<25:22,  4.58it/s]
2022-03-21 11:30:43,411 - INFO - tqdm - f1: 0.9398, accuracy: 0.9400, batch_loss: 0.0359, loss: 0.1666 ||:   4%|4         | 315/7188 [00:31<25:40,  4.46it/s]
2022-03-21 11:30:53,851 - INFO - tqdm - f1: 0.9376, accuracy: 0.9377, batch_loss: 0.7145, loss: 0.1733 ||:   6%|5         | 423/7188 [00:41<24:50,  4.54it/s]
2022-03-21 11:31:03,940 - INFO - tqdm - f1: 0.9378, accuracy: 0.9382, batch_loss: 0.2448, loss: 0.1740 ||:   7%|7         | 527/7188 [00:51<23:52,  4.65it/s]
2022-03-21 11:31:14,044 - INFO - tqdm - f1: 0.9371, accuracy: 0.9373, batch_loss: 0.1306, loss: 0.1781 ||:   9%|8         | 633/7188 [01:01<23:42,  4.61it/s]
2022-03-21 11:31:24,078 - INFO - tqdm - f1: 0.9362, accuracy: 0.9363, batch_loss: 0.7664, loss: 0.1793 ||:  10%|#         | 737/7188 [01:11<23:57,  4.49it/s]
2022-03-21 11:31:34,377 - INFO - tqdm - f1: 0.9371, accuracy: 0.9371, batch_loss: 0.1136, loss: 0.1762 ||:  12%|#1        | 845/7188 [01:22<23:23,  4.52it/s]
2022-03-21 11:31:44,412 - INFO - tqdm - f1: 0.9382, accuracy: 0.9382, batch_loss: 0.1467, loss: 0.1739 ||:  13%|#3        | 947/7188 [01:32<23:15,  4.47it/s]
2022-03-21 11:31:54,765 - INFO - tqdm - f1: 0.9396, accuracy: 0.9394, batch_loss: 0.0508, loss: 0.1722 ||:  15%|#4        | 1055/7188 [01:42<23:06,  4.42it/s]
2022-03-21 11:32:04,942 - INFO - tqdm - f1: 0.9397, accuracy: 0.9396, batch_loss: 0.1085, loss: 0.1717 ||:  16%|#6        | 1161/7188 [01:52<21:58,  4.57it/s]
2022-03-21 11:32:15,167 - INFO - tqdm - f1: 0.9392, accuracy: 0.9392, batch_loss: 0.2869, loss: 0.1728 ||:  18%|#7        | 1269/7188 [02:02<21:50,  4.52it/s]
2022-03-21 11:32:25,250 - INFO - tqdm - f1: 0.9398, accuracy: 0.9397, batch_loss: 0.0162, loss: 0.1719 ||:  19%|#9        | 1373/7188 [02:12<20:53,  4.64it/s]
2022-03-21 11:32:35,590 - INFO - tqdm - f1: 0.9390, accuracy: 0.9388, batch_loss: 0.0934, loss: 0.1745 ||:  21%|##        | 1481/7188 [02:23<21:25,  4.44it/s]
2022-03-21 11:32:45,975 - INFO - tqdm - f1: 0.9391, accuracy: 0.9390, batch_loss: 0.0291, loss: 0.1744 ||:  22%|##2       | 1589/7188 [02:33<21:00,  4.44it/s]
2022-03-21 11:32:56,326 - INFO - tqdm - f1: 0.9394, accuracy: 0.9394, batch_loss: 0.0197, loss: 0.1735 ||:  24%|##3       | 1697/7188 [02:44<20:09,  4.54it/s]
2022-03-21 11:33:06,535 - INFO - tqdm - f1: 0.9389, accuracy: 0.9388, batch_loss: 0.1991, loss: 0.1751 ||:  25%|##5       | 1805/7188 [02:54<19:31,  4.59it/s]
2022-03-21 11:33:16,696 - INFO - tqdm - f1: 0.9388, accuracy: 0.9387, batch_loss: 0.0182, loss: 0.1753 ||:  27%|##6       | 1911/7188 [03:04<19:30,  4.51it/s]
2022-03-21 11:33:26,786 - INFO - tqdm - f1: 0.9389, accuracy: 0.9389, batch_loss: 0.1095, loss: 0.1752 ||:  28%|##8       | 2017/7188 [03:14<18:43,  4.60it/s]
2022-03-21 11:33:36,891 - INFO - tqdm - f1: 0.9389, accuracy: 0.9390, batch_loss: 0.2575, loss: 0.1753 ||:  30%|##9       | 2123/7188 [03:24<14:33,  5.80it/s]
2022-03-21 11:33:47,024 - INFO - tqdm - f1: 0.9392, accuracy: 0.9393, batch_loss: 0.1715, loss: 0.1753 ||:  31%|###1      | 2229/7188 [03:34<18:39,  4.43it/s]
2022-03-21 11:33:57,279 - INFO - tqdm - f1: 0.9392, accuracy: 0.9392, batch_loss: 0.0491, loss: 0.1756 ||:  32%|###2      | 2335/7188 [03:45<18:02,  4.48it/s]
2022-03-21 11:34:07,541 - INFO - tqdm - f1: 0.9393, accuracy: 0.9394, batch_loss: 0.0816, loss: 0.1755 ||:  34%|###3      | 2441/7188 [03:55<17:12,  4.60it/s]
2022-03-21 11:34:17,674 - INFO - tqdm - f1: 0.9391, accuracy: 0.9391, batch_loss: 0.0255, loss: 0.1758 ||:  35%|###5      | 2545/7188 [04:05<13:41,  5.65it/s]
2022-03-21 11:34:27,712 - INFO - tqdm - f1: 0.9388, accuracy: 0.9388, batch_loss: 0.0470, loss: 0.1761 ||:  37%|###6      | 2649/7188 [04:15<13:35,  5.57it/s]
2022-03-21 11:34:37,810 - INFO - tqdm - f1: 0.9392, accuracy: 0.9392, batch_loss: 0.0240, loss: 0.1754 ||:  38%|###8      | 2751/7188 [04:25<16:26,  4.50it/s]
2022-03-21 11:34:47,964 - INFO - tqdm - f1: 0.9391, accuracy: 0.9392, batch_loss: 0.1100, loss: 0.1757 ||:  40%|###9      | 2853/7188 [04:35<15:32,  4.65it/s]
2022-03-21 11:34:58,059 - INFO - tqdm - f1: 0.9390, accuracy: 0.9390, batch_loss: 0.5734, loss: 0.1764 ||:  41%|####1     | 2957/7188 [04:45<15:29,  4.55it/s]
2022-03-21 11:35:08,279 - INFO - tqdm - f1: 0.9394, accuracy: 0.9394, batch_loss: 0.2070, loss: 0.1759 ||:  43%|####2     | 3065/7188 [04:56<15:01,  4.57it/s]
2022-03-21 11:35:18,357 - INFO - tqdm - f1: 0.9391, accuracy: 0.9391, batch_loss: 0.2056, loss: 0.1762 ||:  44%|####4     | 3169/7188 [05:06<11:39,  5.75it/s]
2022-03-21 11:35:28,388 - INFO - tqdm - f1: 0.9392, accuracy: 0.9392, batch_loss: 0.4176, loss: 0.1761 ||:  46%|####5     | 3271/7188 [05:16<11:41,  5.58it/s]
2022-03-21 11:35:38,469 - INFO - tqdm - f1: 0.9391, accuracy: 0.9391, batch_loss: 0.0203, loss: 0.1763 ||:  47%|####6     | 3375/7188 [05:26<11:08,  5.70it/s]
2022-03-21 11:35:48,585 - INFO - tqdm - f1: 0.9391, accuracy: 0.9391, batch_loss: 0.1484, loss: 0.1762 ||:  48%|####8     | 3479/7188 [05:36<09:00,  6.87it/s]
2022-03-21 11:35:58,701 - INFO - tqdm - f1: 0.9391, accuracy: 0.9391, batch_loss: 0.4982, loss: 0.1762 ||:  50%|####9     | 3583/7188 [05:46<08:35,  6.99it/s]
2022-03-21 11:36:08,793 - INFO - tqdm - f1: 0.9391, accuracy: 0.9391, batch_loss: 0.1778, loss: 0.1762 ||:  51%|#####1    | 3687/7188 [05:56<08:20,  7.00it/s]
2022-03-21 11:36:18,933 - INFO - tqdm - f1: 0.9390, accuracy: 0.9390, batch_loss: 0.0129, loss: 0.1764 ||:  53%|#####2    | 3793/7188 [06:06<10:04,  5.62it/s]
2022-03-21 11:36:29,184 - INFO - tqdm - f1: 0.9390, accuracy: 0.9390, batch_loss: 0.0333, loss: 0.1762 ||:  54%|#####4    | 3901/7188 [06:16<12:15,  4.47it/s]
2022-03-21 11:36:39,215 - INFO - tqdm - f1: 0.9390, accuracy: 0.9391, batch_loss: 0.0515, loss: 0.1764 ||:  56%|#####5    | 4003/7188 [06:26<11:42,  4.54it/s]
2022-03-21 11:36:49,544 - INFO - tqdm - f1: 0.9391, accuracy: 0.9391, batch_loss: 0.0755, loss: 0.1762 ||:  57%|#####7    | 4109/7188 [06:37<11:10,  4.59it/s]
2022-03-21 11:36:59,605 - INFO - tqdm - f1: 0.9392, accuracy: 0.9392, batch_loss: 0.3388, loss: 0.1759 ||:  59%|#####8    | 4215/7188 [06:47<08:35,  5.77it/s]
2022-03-21 11:37:09,628 - INFO - tqdm - f1: 0.9391, accuracy: 0.9391, batch_loss: 0.0499, loss: 0.1758 ||:  60%|######    | 4317/7188 [06:57<07:00,  6.82it/s]
2022-03-21 11:37:19,757 - INFO - tqdm - f1: 0.9393, accuracy: 0.9392, batch_loss: 0.5350, loss: 0.1755 ||:  62%|######1   | 4421/7188 [07:07<08:07,  5.67it/s]
2022-03-21 11:37:29,801 - INFO - tqdm - f1: 0.9392, accuracy: 0.9392, batch_loss: 0.2870, loss: 0.1756 ||:  63%|######2   | 4527/7188 [07:17<06:19,  7.01it/s]
2022-03-21 11:37:39,897 - INFO - tqdm - f1: 0.9392, accuracy: 0.9391, batch_loss: 0.2625, loss: 0.1755 ||:  64%|######4   | 4633/7188 [07:27<06:06,  6.97it/s]
2022-03-21 11:37:49,942 - INFO - tqdm - f1: 0.9396, accuracy: 0.9395, batch_loss: 0.0057, loss: 0.1749 ||:  66%|######5   | 4739/7188 [07:37<05:54,  6.91it/s]
2022-03-21 11:37:59,961 - INFO - tqdm - f1: 0.9398, accuracy: 0.9398, batch_loss: 0.0296, loss: 0.1743 ||:  67%|######7   | 4843/7188 [07:47<08:44,  4.47it/s]
2022-03-21 11:38:10,151 - INFO - tqdm - f1: 0.9398, accuracy: 0.9397, batch_loss: 0.0350, loss: 0.1743 ||:  69%|######8   | 4949/7188 [07:57<08:21,  4.46it/s]
2022-03-21 11:38:20,158 - INFO - tqdm - f1: 0.9398, accuracy: 0.9397, batch_loss: 0.0947, loss: 0.1742 ||:  70%|#######   | 5053/7188 [08:07<06:13,  5.72it/s]
2022-03-21 11:38:30,449 - INFO - tqdm - f1: 0.9398, accuracy: 0.9397, batch_loss: 0.3252, loss: 0.1745 ||:  72%|#######1  | 5161/7188 [08:18<07:25,  4.55it/s]
2022-03-21 11:38:40,738 - INFO - tqdm - f1: 0.9397, accuracy: 0.9396, batch_loss: 0.1908, loss: 0.1746 ||:  73%|#######3  | 5267/7188 [08:28<07:12,  4.45it/s]
2022-03-21 11:38:50,801 - INFO - tqdm - f1: 0.9398, accuracy: 0.9397, batch_loss: 0.1569, loss: 0.1747 ||:  75%|#######4  | 5371/7188 [08:38<04:20,  6.98it/s]
2022-03-21 11:39:00,829 - INFO - tqdm - f1: 0.9399, accuracy: 0.9398, batch_loss: 0.3930, loss: 0.1744 ||:  76%|#######6  | 5473/7188 [08:48<05:10,  5.51it/s]
2022-03-21 11:39:10,860 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.1152, loss: 0.1739 ||:  78%|#######7  | 5575/7188 [08:58<06:02,  4.45it/s]
2022-03-21 11:39:21,088 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.3864, loss: 0.1739 ||:  79%|#######9  | 5681/7188 [09:08<05:29,  4.57it/s]
2022-03-21 11:39:31,151 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.0377, loss: 0.1740 ||:  80%|########  | 5785/7188 [09:18<03:56,  5.92it/s]
2022-03-21 11:39:41,281 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.2015, loss: 0.1740 ||:  82%|########1 | 5889/7188 [09:29<03:02,  7.12it/s]
2022-03-21 11:39:51,402 - INFO - tqdm - f1: 0.9402, accuracy: 0.9401, batch_loss: 0.0189, loss: 0.1739 ||:  83%|########3 | 5991/7188 [09:39<02:25,  8.22it/s]
2022-03-21 11:40:01,456 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.4124, loss: 0.1741 ||:  85%|########4 | 6097/7188 [09:49<01:52,  9.71it/s]
2022-03-21 11:40:11,584 - INFO - tqdm - f1: 0.9400, accuracy: 0.9399, batch_loss: 0.1902, loss: 0.1740 ||:  86%|########6 | 6201/7188 [09:59<02:02,  8.07it/s]
2022-03-21 11:40:21,718 - INFO - tqdm - f1: 0.9400, accuracy: 0.9400, batch_loss: 0.3720, loss: 0.1741 ||:  88%|########7 | 6307/7188 [10:09<01:46,  8.29it/s]
2022-03-21 11:40:31,836 - INFO - tqdm - f1: 0.9400, accuracy: 0.9400, batch_loss: 0.0737, loss: 0.1738 ||:  89%|########9 | 6413/7188 [10:19<01:31,  8.44it/s]
2022-03-21 11:40:41,896 - INFO - tqdm - f1: 0.9400, accuracy: 0.9400, batch_loss: 0.3131, loss: 0.1739 ||:  91%|######### | 6517/7188 [10:29<01:37,  6.90it/s]
2022-03-21 11:40:51,993 - INFO - tqdm - f1: 0.9402, accuracy: 0.9402, batch_loss: 0.1931, loss: 0.1732 ||:  92%|#########2| 6623/7188 [10:39<01:22,  6.84it/s]
2022-03-21 11:41:02,055 - INFO - tqdm - f1: 0.9403, accuracy: 0.9402, batch_loss: 0.0776, loss: 0.1731 ||:  94%|#########3| 6727/7188 [10:49<01:21,  5.62it/s]
2022-03-21 11:41:12,132 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.0980, loss: 0.1727 ||:  95%|#########5| 6831/7188 [10:59<01:02,  5.68it/s]
2022-03-21 11:41:22,251 - INFO - tqdm - f1: 0.9404, accuracy: 0.9403, batch_loss: 0.0064, loss: 0.1727 ||:  97%|#########6| 6937/7188 [11:09<00:54,  4.59it/s]
2022-03-21 11:41:32,507 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.0708, loss: 0.1723 ||:  98%|#########8| 7045/7188 [11:20<00:32,  4.44it/s]
2022-03-21 11:41:42,799 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.5388, loss: 0.1724 ||: 100%|#########9| 7153/7188 [11:30<00:07,  4.45it/s]
2022-03-21 11:41:42,954 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.4700, loss: 0.1725 ||: 100%|#########9| 7155/7188 [11:30<00:05,  5.53it/s]
2022-03-21 11:41:43,116 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0296, loss: 0.1724 ||: 100%|#########9| 7157/7188 [11:30<00:04,  6.63it/s]
2022-03-21 11:41:43,250 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.3901, loss: 0.1725 ||: 100%|#########9| 7159/7188 [11:30<00:03,  7.96it/s]
2022-03-21 11:41:43,387 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.3306, loss: 0.1725 ||: 100%|#########9| 7161/7188 [11:31<00:02,  9.22it/s]
2022-03-21 11:41:43,536 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.2234, loss: 0.1725 ||: 100%|#########9| 7163/7188 [11:31<00:02, 10.18it/s]
2022-03-21 11:41:43,671 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.0255, loss: 0.1724 ||: 100%|#########9| 7165/7188 [11:31<00:02, 11.23it/s]
2022-03-21 11:41:43,805 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.0201, loss: 0.1724 ||: 100%|#########9| 7167/7188 [11:31<00:01, 12.13it/s]
2022-03-21 11:41:43,930 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.0230, loss: 0.1724 ||: 100%|#########9| 7169/7188 [11:31<00:01, 13.07it/s]
2022-03-21 11:41:44,069 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.2071, loss: 0.1724 ||: 100%|#########9| 7171/7188 [11:31<00:01, 13.44it/s]
2022-03-21 11:41:44,211 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.1658, loss: 0.1724 ||: 100%|#########9| 7173/7188 [11:31<00:01, 13.62it/s]
2022-03-21 11:41:44,352 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.0271, loss: 0.1723 ||: 100%|#########9| 7175/7188 [11:32<00:00, 13.78it/s]
2022-03-21 11:41:44,500 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.0324, loss: 0.1723 ||: 100%|#########9| 7177/7188 [11:32<00:00, 13.70it/s]
2022-03-21 11:41:44,644 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.1121, loss: 0.1724 ||: 100%|#########9| 7179/7188 [11:32<00:00, 13.76it/s]
2022-03-21 11:41:44,798 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.0275, loss: 0.1724 ||: 100%|#########9| 7181/7188 [11:32<00:00, 13.53it/s]
2022-03-21 11:41:44,929 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.0190, loss: 0.1724 ||: 100%|#########9| 7183/7188 [11:32<00:00, 13.99it/s]
2022-03-21 11:41:45,069 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.1619, loss: 0.1724 ||: 100%|#########9| 7185/7188 [11:32<00:00, 14.07it/s]
2022-03-21 11:41:46,269 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.1213, loss: 0.1724 ||: 100%|#########9| 7187/7188 [11:34<00:00,  4.35it/s]
2022-03-21 11:41:46,392 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.0856, loss: 0.1724 ||: 100%|##########| 7188/7188 [11:34<00:00, 10.36it/s]
2022-03-21 11:41:46,420 - INFO - allennlp.training.trainer - Validating
2022-03-21 11:41:46,430 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 11:41:56,434 - INFO - tqdm - f1: 0.9345, accuracy: 0.9343, batch_loss: 0.3535, loss: 0.1944 ||:  98%|#########8| 308/313 [00:09<00:00, 12.62it/s]
2022-03-21 11:41:56,549 - INFO - tqdm - f1: 0.9344, accuracy: 0.9342, batch_loss: 0.1349, loss: 0.1938 ||: 100%|#########9| 312/313 [00:10<00:00, 14.83it/s]
2022-03-21 11:41:56,584 - INFO - tqdm - f1: 0.9346, accuracy: 0.9344, batch_loss: 0.0302, loss: 0.1933 ||: 100%|##########| 313/313 [00:10<00:00, 30.84it/s]
2022-03-21 11:41:56,609 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/ag_base_hyper_small_seed_97/best.th'.
2022-03-21 11:42:00,279 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 11:42:00,285 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.940  |     0.934
2022-03-21 11:42:00,291 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.940  |     0.935
2022-03-21 11:42:00,299 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 11:42:00,307 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.172  |     0.193
2022-03-21 11:42:00,313 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7591.871  |       N/A
2022-03-21 11:42:00,320 - INFO - allennlp.training.trainer - Epoch duration: 0:11:48.142420
2022-03-21 11:42:00,327 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:33:50
2022-03-21 11:42:00,335 - INFO - allennlp.training.trainer - Epoch 2/9
2022-03-21 11:42:00,342 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-21 11:42:00,349 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 11:42:00,357 - INFO - allennlp.training.trainer - Training
2022-03-21 11:42:00,364 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 11:42:10,438 - INFO - tqdm - f1: 0.9499, accuracy: 0.9497, batch_loss: 0.0506, loss: 0.1314 ||:   1%|1         | 97/7188 [00:10<12:32,  9.42it/s]
2022-03-21 11:42:20,515 - INFO - tqdm - f1: 0.9493, accuracy: 0.9489, batch_loss: 0.0387, loss: 0.1346 ||:   3%|2         | 203/7188 [00:20<14:12,  8.19it/s]
2022-03-21 11:42:30,635 - INFO - tqdm - f1: 0.9543, accuracy: 0.9539, batch_loss: 0.0805, loss: 0.1312 ||:   4%|4         | 309/7188 [00:30<13:43,  8.35it/s]
2022-03-21 11:42:40,672 - INFO - tqdm - f1: 0.9541, accuracy: 0.9537, batch_loss: 0.2386, loss: 0.1291 ||:   6%|5         | 413/7188 [00:40<13:41,  8.25it/s]
2022-03-21 11:42:50,764 - INFO - tqdm - f1: 0.9538, accuracy: 0.9535, batch_loss: 0.0080, loss: 0.1321 ||:   7%|7         | 517/7188 [00:50<24:34,  4.52it/s]
2022-03-21 11:43:01,115 - INFO - tqdm - f1: 0.9532, accuracy: 0.9529, batch_loss: 0.0306, loss: 0.1342 ||:   9%|8         | 627/7188 [01:00<23:53,  4.58it/s]
2022-03-21 11:43:11,160 - INFO - tqdm - f1: 0.9524, accuracy: 0.9522, batch_loss: 0.0603, loss: 0.1343 ||:  10%|#         | 729/7188 [01:10<23:27,  4.59it/s]
2022-03-21 11:43:21,471 - INFO - tqdm - f1: 0.9523, accuracy: 0.9521, batch_loss: 0.1111, loss: 0.1358 ||:  12%|#1        | 837/7188 [01:21<23:05,  4.58it/s]
2022-03-21 11:43:31,735 - INFO - tqdm - f1: 0.9526, accuracy: 0.9525, batch_loss: 0.3850, loss: 0.1366 ||:  13%|#3        | 943/7188 [01:31<23:07,  4.50it/s]
2022-03-21 11:43:41,961 - INFO - tqdm - f1: 0.9528, accuracy: 0.9528, batch_loss: 0.0245, loss: 0.1363 ||:  15%|#4        | 1051/7188 [01:41<22:18,  4.59it/s]
2022-03-21 11:43:52,238 - INFO - tqdm - f1: 0.9526, accuracy: 0.9526, batch_loss: 0.0152, loss: 0.1370 ||:  16%|#6        | 1159/7188 [01:51<22:23,  4.49it/s]
2022-03-21 11:44:02,526 - INFO - tqdm - f1: 0.9529, accuracy: 0.9530, batch_loss: 0.1179, loss: 0.1371 ||:  18%|#7        | 1265/7188 [02:02<22:32,  4.38it/s]
2022-03-21 11:44:12,552 - INFO - tqdm - f1: 0.9524, accuracy: 0.9524, batch_loss: 0.4275, loss: 0.1382 ||:  19%|#9        | 1369/7188 [02:12<16:49,  5.77it/s]
2022-03-21 11:44:22,662 - INFO - tqdm - f1: 0.9522, accuracy: 0.9522, batch_loss: 0.3602, loss: 0.1386 ||:  21%|##        | 1475/7188 [02:22<13:50,  6.88it/s]
2022-03-21 11:44:32,717 - INFO - tqdm - f1: 0.9524, accuracy: 0.9525, batch_loss: 0.0276, loss: 0.1384 ||:  22%|##1       | 1579/7188 [02:32<13:32,  6.90it/s]
2022-03-21 11:44:42,743 - INFO - tqdm - f1: 0.9528, accuracy: 0.9528, batch_loss: 0.3063, loss: 0.1374 ||:  23%|##3       | 1683/7188 [02:42<15:53,  5.77it/s]
2022-03-21 11:44:52,753 - INFO - tqdm - f1: 0.9528, accuracy: 0.9529, batch_loss: 0.5688, loss: 0.1364 ||:  25%|##4       | 1785/7188 [02:52<19:50,  4.54it/s]
2022-03-21 11:45:02,857 - INFO - tqdm - f1: 0.9533, accuracy: 0.9534, batch_loss: 0.3212, loss: 0.1363 ||:  26%|##6       | 1889/7188 [03:02<19:24,  4.55it/s]
2022-03-21 11:45:13,194 - INFO - tqdm - f1: 0.9530, accuracy: 0.9530, batch_loss: 0.0222, loss: 0.1371 ||:  28%|##7       | 1997/7188 [03:12<18:53,  4.58it/s]
2022-03-21 11:45:23,204 - INFO - tqdm - f1: 0.9528, accuracy: 0.9529, batch_loss: 0.1487, loss: 0.1371 ||:  29%|##9       | 2103/7188 [03:22<18:28,  4.59it/s]
2022-03-21 11:45:33,209 - INFO - tqdm - f1: 0.9526, accuracy: 0.9526, batch_loss: 0.1527, loss: 0.1376 ||:  31%|###       | 2209/7188 [03:32<17:56,  4.63it/s]
2022-03-21 11:45:43,334 - INFO - tqdm - f1: 0.9523, accuracy: 0.9523, batch_loss: 0.4202, loss: 0.1383 ||:  32%|###2      | 2315/7188 [03:42<14:06,  5.76it/s]
2022-03-21 11:45:53,443 - INFO - tqdm - f1: 0.9523, accuracy: 0.9524, batch_loss: 0.0510, loss: 0.1382 ||:  34%|###3      | 2421/7188 [03:53<13:41,  5.80it/s]
2022-03-21 11:46:03,676 - INFO - tqdm - f1: 0.9523, accuracy: 0.9524, batch_loss: 0.0710, loss: 0.1379 ||:  35%|###5      | 2529/7188 [04:03<17:12,  4.51it/s]
2022-03-21 11:46:13,774 - INFO - tqdm - f1: 0.9525, accuracy: 0.9526, batch_loss: 0.0542, loss: 0.1377 ||:  37%|###6      | 2633/7188 [04:13<16:52,  4.50it/s]
2022-03-21 11:46:23,906 - INFO - tqdm - f1: 0.9525, accuracy: 0.9526, batch_loss: 0.0237, loss: 0.1371 ||:  38%|###8      | 2737/7188 [04:23<16:44,  4.43it/s]
2022-03-21 11:46:33,924 - INFO - tqdm - f1: 0.9525, accuracy: 0.9525, batch_loss: 0.1561, loss: 0.1376 ||:  40%|###9      | 2841/7188 [04:33<12:31,  5.79it/s]
2022-03-21 11:46:44,111 - INFO - tqdm - f1: 0.9522, accuracy: 0.9522, batch_loss: 0.0582, loss: 0.1387 ||:  41%|####      | 2947/7188 [04:43<15:40,  4.51it/s]
2022-03-21 11:46:54,185 - INFO - tqdm - f1: 0.9523, accuracy: 0.9524, batch_loss: 0.0686, loss: 0.1386 ||:  42%|####2     | 3051/7188 [04:53<12:12,  5.64it/s]
2022-03-21 11:47:04,207 - INFO - tqdm - f1: 0.9523, accuracy: 0.9524, batch_loss: 0.0647, loss: 0.1382 ||:  44%|####3     | 3155/7188 [05:03<11:44,  5.72it/s]
2022-03-21 11:47:14,211 - INFO - tqdm - f1: 0.9527, accuracy: 0.9528, batch_loss: 0.1983, loss: 0.1375 ||:  45%|####5     | 3259/7188 [05:13<14:20,  4.57it/s]
2022-03-21 11:47:24,263 - INFO - tqdm - f1: 0.9530, accuracy: 0.9530, batch_loss: 0.2367, loss: 0.1371 ||:  47%|####6     | 3365/7188 [05:23<14:12,  4.48it/s]
2022-03-21 11:47:34,292 - INFO - tqdm - f1: 0.9533, accuracy: 0.9533, batch_loss: 0.0083, loss: 0.1365 ||:  48%|####8     | 3467/7188 [05:33<13:40,  4.53it/s]
2022-03-21 11:47:44,333 - INFO - tqdm - f1: 0.9534, accuracy: 0.9534, batch_loss: 0.0663, loss: 0.1360 ||:  50%|####9     | 3569/7188 [05:43<10:53,  5.54it/s]
2022-03-21 11:47:54,437 - INFO - tqdm - f1: 0.9533, accuracy: 0.9533, batch_loss: 0.0022, loss: 0.1361 ||:  51%|#####1    | 3673/7188 [05:54<08:11,  7.15it/s]
2022-03-21 11:48:04,463 - INFO - tqdm - f1: 0.9533, accuracy: 0.9532, batch_loss: 0.0433, loss: 0.1366 ||:  53%|#####2    | 3775/7188 [06:04<06:54,  8.24it/s]
2022-03-21 11:48:14,534 - INFO - tqdm - f1: 0.9532, accuracy: 0.9532, batch_loss: 0.1229, loss: 0.1364 ||:  54%|#####3    | 3881/7188 [06:14<06:34,  8.39it/s]
2022-03-21 11:48:24,607 - INFO - tqdm - f1: 0.9533, accuracy: 0.9533, batch_loss: 0.0503, loss: 0.1364 ||:  55%|#####5    | 3981/7188 [06:24<06:36,  8.09it/s]
2022-03-21 11:48:34,635 - INFO - tqdm - f1: 0.9532, accuracy: 0.9532, batch_loss: 0.0411, loss: 0.1364 ||:  57%|#####6    | 4085/7188 [06:34<07:35,  6.81it/s]
2022-03-21 11:48:44,745 - INFO - tqdm - f1: 0.9531, accuracy: 0.9530, batch_loss: 0.1275, loss: 0.1373 ||:  58%|#####8    | 4189/7188 [06:44<08:52,  5.64it/s]
2022-03-21 11:48:54,879 - INFO - tqdm - f1: 0.9530, accuracy: 0.9530, batch_loss: 0.0764, loss: 0.1372 ||:  60%|#####9    | 4295/7188 [06:54<05:51,  8.24it/s]
2022-03-21 11:49:04,953 - INFO - tqdm - f1: 0.9530, accuracy: 0.9530, batch_loss: 0.0332, loss: 0.1371 ||:  61%|######1   | 4399/7188 [07:04<06:37,  7.02it/s]
2022-03-21 11:49:15,080 - INFO - tqdm - f1: 0.9530, accuracy: 0.9529, batch_loss: 0.0436, loss: 0.1370 ||:  63%|######2   | 4503/7188 [07:14<06:31,  6.86it/s]
2022-03-21 11:49:25,174 - INFO - tqdm - f1: 0.9531, accuracy: 0.9531, batch_loss: 0.3894, loss: 0.1366 ||:  64%|######4   | 4605/7188 [07:24<09:45,  4.41it/s]
2022-03-21 11:49:35,184 - INFO - tqdm - f1: 0.9530, accuracy: 0.9530, batch_loss: 0.0420, loss: 0.1367 ||:  65%|######5   | 4707/7188 [07:34<09:02,  4.57it/s]
2022-03-21 11:49:45,395 - INFO - tqdm - f1: 0.9530, accuracy: 0.9529, batch_loss: 0.4617, loss: 0.1371 ||:  67%|######6   | 4811/7188 [07:45<08:49,  4.49it/s]
2022-03-21 11:49:55,520 - INFO - tqdm - f1: 0.9528, accuracy: 0.9528, batch_loss: 0.0290, loss: 0.1379 ||:  68%|######8   | 4915/7188 [07:55<08:20,  4.54it/s]
2022-03-21 11:50:05,898 - INFO - tqdm - f1: 0.9529, accuracy: 0.9529, batch_loss: 0.5657, loss: 0.1377 ||:  70%|######9   | 5023/7188 [08:05<07:59,  4.51it/s]
2022-03-21 11:50:16,063 - INFO - tqdm - f1: 0.9529, accuracy: 0.9529, batch_loss: 0.0802, loss: 0.1377 ||:  71%|#######1  | 5125/7188 [08:15<07:47,  4.41it/s]
2022-03-21 11:50:26,356 - INFO - tqdm - f1: 0.9530, accuracy: 0.9529, batch_loss: 0.0207, loss: 0.1375 ||:  73%|#######2  | 5229/7188 [08:25<07:21,  4.43it/s]
2022-03-21 11:50:36,464 - INFO - tqdm - f1: 0.9529, accuracy: 0.9528, batch_loss: 0.0216, loss: 0.1378 ||:  74%|#######4  | 5331/7188 [08:36<06:56,  4.45it/s]
2022-03-21 11:50:46,686 - INFO - tqdm - f1: 0.9529, accuracy: 0.9529, batch_loss: 0.0411, loss: 0.1378 ||:  76%|#######5  | 5437/7188 [08:46<06:31,  4.47it/s]
2022-03-21 11:50:56,904 - INFO - tqdm - f1: 0.9530, accuracy: 0.9530, batch_loss: 0.1930, loss: 0.1376 ||:  77%|#######7  | 5543/7188 [08:56<06:00,  4.57it/s]
2022-03-21 11:51:07,026 - INFO - tqdm - f1: 0.9531, accuracy: 0.9531, batch_loss: 0.0313, loss: 0.1372 ||:  79%|#######8  | 5647/7188 [09:06<05:40,  4.52it/s]
2022-03-21 11:51:17,412 - INFO - tqdm - f1: 0.9531, accuracy: 0.9531, batch_loss: 0.0182, loss: 0.1371 ||:  80%|########  | 5755/7188 [09:17<05:23,  4.43it/s]
2022-03-21 11:51:27,707 - INFO - tqdm - f1: 0.9532, accuracy: 0.9532, batch_loss: 0.1903, loss: 0.1368 ||:  82%|########1 | 5863/7188 [09:27<04:51,  4.54it/s]
2022-03-21 11:51:37,930 - INFO - tqdm - f1: 0.9532, accuracy: 0.9532, batch_loss: 0.3892, loss: 0.1366 ||:  83%|########3 | 5971/7188 [09:37<04:22,  4.64it/s]
2022-03-21 11:51:48,353 - INFO - tqdm - f1: 0.9531, accuracy: 0.9531, batch_loss: 0.0862, loss: 0.1368 ||:  85%|########4 | 6079/7188 [09:47<04:06,  4.51it/s]
2022-03-21 11:51:58,700 - INFO - tqdm - f1: 0.9532, accuracy: 0.9532, batch_loss: 0.0206, loss: 0.1366 ||:  86%|########6 | 6187/7188 [09:58<03:42,  4.49it/s]
2022-03-21 11:52:09,043 - INFO - tqdm - f1: 0.9531, accuracy: 0.9531, batch_loss: 0.0137, loss: 0.1368 ||:  88%|########7 | 6297/7188 [10:08<03:12,  4.62it/s]
2022-03-21 11:52:19,166 - INFO - tqdm - f1: 0.9531, accuracy: 0.9530, batch_loss: 0.0326, loss: 0.1371 ||:  89%|########9 | 6403/7188 [10:18<02:18,  5.67it/s]
2022-03-21 11:52:29,265 - INFO - tqdm - f1: 0.9531, accuracy: 0.9530, batch_loss: 0.1367, loss: 0.1370 ||:  91%|######### | 6507/7188 [10:28<02:28,  4.58it/s]
2022-03-21 11:52:39,424 - INFO - tqdm - f1: 0.9531, accuracy: 0.9530, batch_loss: 0.1051, loss: 0.1369 ||:  92%|#########2| 6615/7188 [10:39<02:05,  4.57it/s]
2022-03-21 11:52:49,511 - INFO - tqdm - f1: 0.9531, accuracy: 0.9530, batch_loss: 0.2221, loss: 0.1369 ||:  93%|#########3| 6719/7188 [10:49<01:45,  4.46it/s]
2022-03-21 11:52:59,671 - INFO - tqdm - f1: 0.9530, accuracy: 0.9529, batch_loss: 0.2119, loss: 0.1372 ||:  95%|#########4| 6825/7188 [10:59<01:21,  4.45it/s]
2022-03-21 11:53:09,877 - INFO - tqdm - f1: 0.9529, accuracy: 0.9529, batch_loss: 0.3697, loss: 0.1372 ||:  96%|#########6| 6929/7188 [11:09<00:57,  4.52it/s]
2022-03-21 11:53:20,070 - INFO - tqdm - f1: 0.9528, accuracy: 0.9528, batch_loss: 0.0205, loss: 0.1377 ||:  98%|#########7| 7033/7188 [11:19<00:33,  4.65it/s]
2022-03-21 11:53:30,176 - INFO - tqdm - f1: 0.9527, accuracy: 0.9527, batch_loss: 0.0164, loss: 0.1379 ||:  99%|#########9| 7139/7188 [11:29<00:10,  4.48it/s]
2022-03-21 11:53:31,136 - INFO - tqdm - f1: 0.9527, accuracy: 0.9527, batch_loss: 0.0747, loss: 0.1379 ||: 100%|#########9| 7153/7188 [11:30<00:02, 12.52it/s]
2022-03-21 11:53:31,284 - INFO - tqdm - f1: 0.9527, accuracy: 0.9527, batch_loss: 0.0475, loss: 0.1379 ||: 100%|#########9| 7155/7188 [11:30<00:02, 12.80it/s]
2022-03-21 11:53:31,436 - INFO - tqdm - f1: 0.9527, accuracy: 0.9527, batch_loss: 0.1163, loss: 0.1379 ||: 100%|#########9| 7157/7188 [11:31<00:02, 12.92it/s]
2022-03-21 11:53:31,591 - INFO - tqdm - f1: 0.9527, accuracy: 0.9527, batch_loss: 0.0294, loss: 0.1379 ||: 100%|#########9| 7159/7188 [11:31<00:02, 12.92it/s]
2022-03-21 11:53:31,739 - INFO - tqdm - f1: 0.9527, accuracy: 0.9527, batch_loss: 0.0548, loss: 0.1378 ||: 100%|#########9| 7161/7188 [11:31<00:02, 13.08it/s]
2022-03-21 11:53:31,879 - INFO - tqdm - f1: 0.9528, accuracy: 0.9527, batch_loss: 0.3589, loss: 0.1379 ||: 100%|#########9| 7163/7188 [11:31<00:01, 13.41it/s]
2022-03-21 11:53:32,017 - INFO - tqdm - f1: 0.9527, accuracy: 0.9527, batch_loss: 0.3298, loss: 0.1379 ||: 100%|#########9| 7165/7188 [11:31<00:01, 13.72it/s]
2022-03-21 11:53:32,157 - INFO - tqdm - f1: 0.9528, accuracy: 0.9527, batch_loss: 0.0437, loss: 0.1378 ||: 100%|#########9| 7167/7188 [11:31<00:01, 13.89it/s]
2022-03-21 11:53:32,294 - INFO - tqdm - f1: 0.9528, accuracy: 0.9527, batch_loss: 0.0277, loss: 0.1378 ||: 100%|#########9| 7169/7188 [11:31<00:01, 14.09it/s]
2022-03-21 11:53:32,423 - INFO - tqdm - f1: 0.9528, accuracy: 0.9527, batch_loss: 0.2624, loss: 0.1378 ||: 100%|#########9| 7171/7188 [11:32<00:01, 14.49it/s]
2022-03-21 11:53:32,572 - INFO - tqdm - f1: 0.9528, accuracy: 0.9527, batch_loss: 0.0415, loss: 0.1378 ||: 100%|#########9| 7173/7188 [11:32<00:01, 14.18it/s]
2022-03-21 11:53:33,719 - INFO - tqdm - f1: 0.9528, accuracy: 0.9527, batch_loss: 0.0323, loss: 0.1378 ||: 100%|#########9| 7175/7188 [11:33<00:02,  4.51it/s]
2022-03-21 11:53:33,848 - INFO - tqdm - f1: 0.9528, accuracy: 0.9527, batch_loss: 0.0541, loss: 0.1378 ||: 100%|#########9| 7177/7188 [11:33<00:01,  5.74it/s]
2022-03-21 11:53:33,994 - INFO - tqdm - f1: 0.9528, accuracy: 0.9527, batch_loss: 0.1310, loss: 0.1378 ||: 100%|#########9| 7179/7188 [11:33<00:01,  6.94it/s]
2022-03-21 11:53:34,143 - INFO - tqdm - f1: 0.9528, accuracy: 0.9528, batch_loss: 0.0323, loss: 0.1377 ||: 100%|#########9| 7181/7188 [11:33<00:00,  8.12it/s]
2022-03-21 11:53:34,274 - INFO - tqdm - f1: 0.9528, accuracy: 0.9527, batch_loss: 0.5381, loss: 0.1378 ||: 100%|#########9| 7183/7188 [11:33<00:00,  9.45it/s]
2022-03-21 11:53:34,416 - INFO - tqdm - f1: 0.9528, accuracy: 0.9527, batch_loss: 0.4306, loss: 0.1379 ||: 100%|#########9| 7185/7188 [11:34<00:00, 10.48it/s]
2022-03-21 11:53:34,560 - INFO - tqdm - f1: 0.9528, accuracy: 0.9527, batch_loss: 0.3000, loss: 0.1379 ||: 100%|#########9| 7187/7188 [11:34<00:00, 11.31it/s]
2022-03-21 11:53:34,684 - INFO - tqdm - f1: 0.9527, accuracy: 0.9527, batch_loss: 0.3475, loss: 0.1379 ||: 100%|##########| 7188/7188 [11:34<00:00, 10.35it/s]
2022-03-21 11:53:34,697 - INFO - allennlp.training.trainer - Validating
2022-03-21 11:53:34,712 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 11:53:44,802 - INFO - tqdm - f1: 0.9403, accuracy: 0.9401, batch_loss: 0.0140, loss: 0.1965 ||:  97%|#########7| 304/313 [00:10<00:00, 32.23it/s]
2022-03-21 11:53:44,998 - INFO - tqdm - f1: 0.9405, accuracy: 0.9404, batch_loss: 0.0448, loss: 0.1983 ||: 100%|##########| 313/313 [00:10<00:00, 30.45it/s]
2022-03-21 11:53:45,040 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/ag_base_hyper_small_seed_97/best.th'.
2022-03-21 11:53:49,083 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 11:53:49,097 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.953  |     0.940
2022-03-21 11:53:49,109 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.953  |     0.940
2022-03-21 11:53:49,122 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 11:53:49,124 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.138  |     0.198
2022-03-21 11:53:49,129 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7592.430  |       N/A
2022-03-21 11:53:49,142 - INFO - allennlp.training.trainer - Epoch duration: 0:11:48.807699
2022-03-21 11:53:49,156 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:22:18
2022-03-21 11:53:49,168 - INFO - allennlp.training.trainer - Epoch 3/9
2022-03-21 11:53:49,170 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-21 11:53:49,186 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 11:53:49,199 - INFO - allennlp.training.trainer - Training
2022-03-21 11:53:49,210 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 11:53:59,230 - INFO - tqdm - f1: 0.9720, accuracy: 0.9721, batch_loss: 0.1221, loss: 0.0802 ||:   1%|          | 65/7188 [00:10<11:16, 10.53it/s]
2022-03-21 11:54:09,450 - INFO - tqdm - f1: 0.9615, accuracy: 0.9615, batch_loss: 0.0493, loss: 0.1052 ||:   2%|2         | 167/7188 [00:20<25:49,  4.53it/s]
2022-03-21 11:54:19,959 - INFO - tqdm - f1: 0.9654, accuracy: 0.9659, batch_loss: 0.0694, loss: 0.1028 ||:   4%|3         | 277/7188 [00:30<25:34,  4.50it/s]
2022-03-21 11:54:30,045 - INFO - tqdm - f1: 0.9647, accuracy: 0.9651, batch_loss: 0.2004, loss: 0.1036 ||:   5%|5         | 383/7188 [00:40<24:42,  4.59it/s]
2022-03-21 11:54:40,099 - INFO - tqdm - f1: 0.9659, accuracy: 0.9662, batch_loss: 0.0083, loss: 0.1010 ||:   7%|6         | 487/7188 [00:50<25:12,  4.43it/s]
2022-03-21 11:54:50,187 - INFO - tqdm - f1: 0.9640, accuracy: 0.9643, batch_loss: 0.0593, loss: 0.1050 ||:   8%|8         | 593/7188 [01:00<24:16,  4.53it/s]
2022-03-21 11:55:00,302 - INFO - tqdm - f1: 0.9650, accuracy: 0.9652, batch_loss: 0.1261, loss: 0.1026 ||:  10%|9         | 699/7188 [01:11<18:30,  5.84it/s]
2022-03-21 11:55:10,348 - INFO - tqdm - f1: 0.9647, accuracy: 0.9649, batch_loss: 0.1555, loss: 0.1048 ||:  11%|#1        | 805/7188 [01:21<23:41,  4.49it/s]
2022-03-21 11:55:20,469 - INFO - tqdm - f1: 0.9657, accuracy: 0.9658, batch_loss: 0.2585, loss: 0.1031 ||:  13%|#2        | 909/7188 [01:31<18:53,  5.54it/s]
2022-03-21 11:55:30,517 - INFO - tqdm - f1: 0.9657, accuracy: 0.9658, batch_loss: 0.0380, loss: 0.1035 ||:  14%|#4        | 1015/7188 [01:41<17:52,  5.76it/s]
2022-03-21 11:55:40,589 - INFO - tqdm - f1: 0.9643, accuracy: 0.9644, batch_loss: 0.0378, loss: 0.1058 ||:  16%|#5        | 1121/7188 [01:51<14:27,  6.99it/s]
2022-03-21 11:55:50,840 - INFO - tqdm - f1: 0.9637, accuracy: 0.9639, batch_loss: 0.1310, loss: 0.1078 ||:  17%|#7        | 1229/7188 [02:01<22:20,  4.44it/s]
2022-03-21 11:56:01,121 - INFO - tqdm - f1: 0.9640, accuracy: 0.9642, batch_loss: 0.0039, loss: 0.1063 ||:  19%|#8        | 1337/7188 [02:11<21:09,  4.61it/s]
2022-03-21 11:56:11,458 - INFO - tqdm - f1: 0.9638, accuracy: 0.9639, batch_loss: 0.0358, loss: 0.1072 ||:  20%|##        | 1447/7188 [02:22<21:19,  4.49it/s]
2022-03-21 11:56:22,116 - INFO - tqdm - f1: 0.9642, accuracy: 0.9643, batch_loss: 0.0514, loss: 0.1058 ||:  22%|##1       | 1555/7188 [02:32<21:25,  4.38it/s]
2022-03-21 11:56:32,475 - INFO - tqdm - f1: 0.9641, accuracy: 0.9643, batch_loss: 0.0463, loss: 0.1054 ||:  23%|##3       | 1665/7188 [02:43<19:54,  4.62it/s]
2022-03-21 11:56:42,586 - INFO - tqdm - f1: 0.9641, accuracy: 0.9642, batch_loss: 0.0507, loss: 0.1054 ||:  25%|##4       | 1773/7188 [02:53<15:18,  5.90it/s]
2022-03-21 11:56:52,680 - INFO - tqdm - f1: 0.9639, accuracy: 0.9640, batch_loss: 0.0163, loss: 0.1059 ||:  26%|##6       | 1879/7188 [03:03<15:21,  5.76it/s]
2022-03-21 11:57:02,796 - INFO - tqdm - f1: 0.9639, accuracy: 0.9640, batch_loss: 0.0209, loss: 0.1058 ||:  28%|##7       | 1985/7188 [03:13<15:23,  5.63it/s]
2022-03-21 11:57:12,865 - INFO - tqdm - f1: 0.9639, accuracy: 0.9639, batch_loss: 0.0300, loss: 0.1060 ||:  29%|##9       | 2093/7188 [03:23<11:55,  7.12it/s]
2022-03-21 11:57:23,395 - INFO - tqdm - f1: 0.9640, accuracy: 0.9639, batch_loss: 0.0227, loss: 0.1057 ||:  31%|###       | 2201/7188 [03:34<18:24,  4.52it/s]
2022-03-21 11:57:33,438 - INFO - tqdm - f1: 0.9636, accuracy: 0.9636, batch_loss: 0.0580, loss: 0.1063 ||:  32%|###2      | 2303/7188 [03:44<18:54,  4.31it/s]
2022-03-21 11:57:44,174 - INFO - tqdm - f1: 0.9637, accuracy: 0.9636, batch_loss: 0.2831, loss: 0.1068 ||:  34%|###3      | 2417/7188 [03:54<16:14,  4.89it/s]
2022-03-21 11:57:54,310 - INFO - tqdm - f1: 0.9638, accuracy: 0.9637, batch_loss: 0.0412, loss: 0.1066 ||:  35%|###5      | 2519/7188 [04:05<18:02,  4.31it/s]
2022-03-21 11:58:04,939 - INFO - tqdm - f1: 0.9639, accuracy: 0.9638, batch_loss: 0.0197, loss: 0.1063 ||:  37%|###6      | 2629/7188 [04:15<17:10,  4.42it/s]
2022-03-21 11:58:15,595 - INFO - tqdm - f1: 0.9640, accuracy: 0.9639, batch_loss: 0.0858, loss: 0.1060 ||:  38%|###8      | 2741/7188 [04:26<16:38,  4.45it/s]
2022-03-21 11:58:26,091 - INFO - tqdm - f1: 0.9638, accuracy: 0.9637, batch_loss: 0.1173, loss: 0.1063 ||:  40%|###9      | 2853/7188 [04:36<16:08,  4.48it/s]
2022-03-21 11:58:36,372 - INFO - tqdm - f1: 0.9638, accuracy: 0.9637, batch_loss: 0.1588, loss: 0.1063 ||:  41%|####1     | 2963/7188 [04:47<15:40,  4.49it/s]
2022-03-21 11:58:46,749 - INFO - tqdm - f1: 0.9634, accuracy: 0.9633, batch_loss: 0.0114, loss: 0.1076 ||:  43%|####2     | 3073/7188 [04:57<15:05,  4.54it/s]
2022-03-21 11:58:56,768 - INFO - tqdm - f1: 0.9635, accuracy: 0.9634, batch_loss: 0.0303, loss: 0.1075 ||:  44%|####4     | 3177/7188 [05:07<11:46,  5.68it/s]
2022-03-21 11:59:06,847 - INFO - tqdm - f1: 0.9635, accuracy: 0.9634, batch_loss: 0.0215, loss: 0.1073 ||:  46%|####5     | 3281/7188 [05:17<11:21,  5.73it/s]
2022-03-21 11:59:16,863 - INFO - tqdm - f1: 0.9636, accuracy: 0.9635, batch_loss: 0.0222, loss: 0.1070 ||:  47%|####7     | 3385/7188 [05:27<09:24,  6.74it/s]
2022-03-21 11:59:26,881 - INFO - tqdm - f1: 0.9636, accuracy: 0.9635, batch_loss: 0.0101, loss: 0.1068 ||:  49%|####8     | 3491/7188 [05:37<08:32,  7.21it/s]
2022-03-21 11:59:36,905 - INFO - tqdm - f1: 0.9637, accuracy: 0.9636, batch_loss: 0.0975, loss: 0.1063 ||:  50%|#####     | 3595/7188 [05:47<10:29,  5.71it/s]
2022-03-21 11:59:46,915 - INFO - tqdm - f1: 0.9638, accuracy: 0.9637, batch_loss: 0.1059, loss: 0.1061 ||:  51%|#####1    | 3697/7188 [05:57<08:12,  7.09it/s]
2022-03-21 11:59:56,964 - INFO - tqdm - f1: 0.9636, accuracy: 0.9635, batch_loss: 0.2400, loss: 0.1066 ||:  53%|#####2    | 3801/7188 [06:07<09:45,  5.78it/s]
2022-03-21 12:00:07,079 - INFO - tqdm - f1: 0.9636, accuracy: 0.9635, batch_loss: 0.0533, loss: 0.1066 ||:  54%|#####4    | 3907/7188 [06:17<07:48,  7.00it/s]
2022-03-21 12:00:17,089 - INFO - tqdm - f1: 0.9636, accuracy: 0.9635, batch_loss: 0.0053, loss: 0.1070 ||:  56%|#####5    | 4009/7188 [06:27<07:39,  6.92it/s]
2022-03-21 12:00:27,099 - INFO - tqdm - f1: 0.9634, accuracy: 0.9633, batch_loss: 0.0920, loss: 0.1076 ||:  57%|#####7    | 4113/7188 [06:37<07:22,  6.95it/s]
2022-03-21 12:00:37,210 - INFO - tqdm - f1: 0.9634, accuracy: 0.9633, batch_loss: 0.1764, loss: 0.1075 ||:  59%|#####8    | 4217/7188 [06:47<07:09,  6.91it/s]
2022-03-21 12:00:47,348 - INFO - tqdm - f1: 0.9634, accuracy: 0.9633, batch_loss: 0.0493, loss: 0.1078 ||:  60%|######    | 4321/7188 [06:58<08:30,  5.62it/s]
2022-03-21 12:00:57,457 - INFO - tqdm - f1: 0.9634, accuracy: 0.9633, batch_loss: 0.0203, loss: 0.1081 ||:  62%|######1   | 4423/7188 [07:08<06:40,  6.91it/s]
2022-03-21 12:01:07,496 - INFO - tqdm - f1: 0.9633, accuracy: 0.9632, batch_loss: 0.0806, loss: 0.1085 ||:  63%|######2   | 4527/7188 [07:18<06:16,  7.08it/s]
2022-03-21 12:01:17,499 - INFO - tqdm - f1: 0.9632, accuracy: 0.9631, batch_loss: 0.0084, loss: 0.1086 ||:  64%|######4   | 4631/7188 [07:28<07:20,  5.80it/s]
2022-03-21 12:01:27,516 - INFO - tqdm - f1: 0.9630, accuracy: 0.9629, batch_loss: 0.0207, loss: 0.1092 ||:  66%|######5   | 4735/7188 [07:38<07:03,  5.80it/s]
2022-03-21 12:01:37,543 - INFO - tqdm - f1: 0.9630, accuracy: 0.9629, batch_loss: 0.1329, loss: 0.1095 ||:  67%|######7   | 4841/7188 [07:48<06:43,  5.81it/s]
2022-03-21 12:01:47,668 - INFO - tqdm - f1: 0.9629, accuracy: 0.9628, batch_loss: 0.0190, loss: 0.1095 ||:  69%|######8   | 4947/7188 [07:58<08:19,  4.49it/s]
2022-03-21 12:01:58,049 - INFO - tqdm - f1: 0.9630, accuracy: 0.9628, batch_loss: 0.0595, loss: 0.1096 ||:  70%|#######   | 5055/7188 [08:08<07:45,  4.58it/s]
2022-03-21 12:02:08,309 - INFO - tqdm - f1: 0.9629, accuracy: 0.9628, batch_loss: 0.0104, loss: 0.1097 ||:  72%|#######1  | 5163/7188 [08:19<07:24,  4.55it/s]
2022-03-21 12:02:18,613 - INFO - tqdm - f1: 0.9627, accuracy: 0.9626, batch_loss: 0.0578, loss: 0.1101 ||:  73%|#######3  | 5271/7188 [08:29<06:55,  4.62it/s]
2022-03-21 12:02:28,668 - INFO - tqdm - f1: 0.9625, accuracy: 0.9624, batch_loss: 0.0379, loss: 0.1105 ||:  75%|#######4  | 5373/7188 [08:39<04:24,  6.85it/s]
2022-03-21 12:02:38,778 - INFO - tqdm - f1: 0.9624, accuracy: 0.9623, batch_loss: 0.0236, loss: 0.1108 ||:  76%|#######6  | 5479/7188 [08:49<05:02,  5.64it/s]
2022-03-21 12:02:49,042 - INFO - tqdm - f1: 0.9624, accuracy: 0.9623, batch_loss: 0.0245, loss: 0.1108 ||:  78%|#######7  | 5591/7188 [08:59<05:47,  4.60it/s]
2022-03-21 12:02:59,169 - INFO - tqdm - f1: 0.9624, accuracy: 0.9623, batch_loss: 0.0727, loss: 0.1107 ||:  79%|#######9  | 5699/7188 [09:09<05:32,  4.48it/s]
2022-03-21 12:03:09,305 - INFO - tqdm - f1: 0.9624, accuracy: 0.9623, batch_loss: 0.0133, loss: 0.1107 ||:  81%|########  | 5803/7188 [09:20<04:06,  5.63it/s]
2022-03-21 12:03:19,435 - INFO - tqdm - f1: 0.9623, accuracy: 0.9622, batch_loss: 0.0069, loss: 0.1110 ||:  82%|########2 | 5911/7188 [09:30<04:39,  4.56it/s]
2022-03-21 12:03:29,794 - INFO - tqdm - f1: 0.9623, accuracy: 0.9623, batch_loss: 0.0306, loss: 0.1109 ||:  84%|########3 | 6023/7188 [09:40<04:10,  4.64it/s]
2022-03-21 12:03:39,881 - INFO - tqdm - f1: 0.9623, accuracy: 0.9623, batch_loss: 0.0101, loss: 0.1109 ||:  85%|########5 | 6129/7188 [09:50<03:51,  4.58it/s]
2022-03-21 12:03:49,989 - INFO - tqdm - f1: 0.9623, accuracy: 0.9623, batch_loss: 0.1742, loss: 0.1108 ||:  87%|########6 | 6233/7188 [10:00<02:46,  5.74it/s]
2022-03-21 12:04:00,128 - INFO - tqdm - f1: 0.9624, accuracy: 0.9624, batch_loss: 0.0439, loss: 0.1108 ||:  88%|########8 | 6339/7188 [10:10<03:08,  4.52it/s]
2022-03-21 12:04:10,194 - INFO - tqdm - f1: 0.9623, accuracy: 0.9623, batch_loss: 0.0944, loss: 0.1110 ||:  90%|########9 | 6445/7188 [10:20<02:45,  4.49it/s]
2022-03-21 12:04:20,277 - INFO - tqdm - f1: 0.9622, accuracy: 0.9622, batch_loss: 0.3954, loss: 0.1112 ||:  91%|#########1| 6551/7188 [10:31<02:24,  4.42it/s]
2022-03-21 12:04:31,036 - INFO - tqdm - f1: 0.9622, accuracy: 0.9622, batch_loss: 0.0814, loss: 0.1112 ||:  93%|#########2| 6663/7188 [10:41<01:58,  4.43it/s]
2022-03-21 12:04:41,539 - INFO - tqdm - f1: 0.9622, accuracy: 0.9622, batch_loss: 0.0803, loss: 0.1114 ||:  94%|#########4| 6779/7188 [10:52<01:28,  4.63it/s]
2022-03-21 12:04:52,601 - INFO - tqdm - f1: 0.9622, accuracy: 0.9621, batch_loss: 0.0973, loss: 0.1113 ||:  96%|#########5| 6899/7188 [11:03<01:06,  4.37it/s]
2022-03-21 12:05:02,781 - INFO - tqdm - f1: 0.9622, accuracy: 0.9622, batch_loss: 0.0277, loss: 0.1113 ||:  97%|#########7| 7005/7188 [11:13<00:40,  4.57it/s]
2022-03-21 12:05:12,880 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0045, loss: 0.1114 ||:  99%|#########8| 7109/7188 [11:23<00:17,  4.54it/s]
2022-03-21 12:05:16,848 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.1662, loss: 0.1112 ||: 100%|#########9| 7153/7188 [11:27<00:03,  9.79it/s]
2022-03-21 12:05:16,973 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.2075, loss: 0.1112 ||: 100%|#########9| 7155/7188 [11:27<00:02, 11.08it/s]
2022-03-21 12:05:17,099 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0045, loss: 0.1112 ||: 100%|#########9| 7157/7188 [11:27<00:02, 12.19it/s]
2022-03-21 12:05:17,248 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.1875, loss: 0.1113 ||: 100%|#########9| 7159/7188 [11:28<00:02, 12.53it/s]
2022-03-21 12:05:17,388 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0292, loss: 0.1113 ||: 100%|#########9| 7161/7188 [11:28<00:02, 13.01it/s]
2022-03-21 12:05:17,526 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0077, loss: 0.1113 ||: 100%|#########9| 7163/7188 [11:28<00:01, 13.40it/s]
2022-03-21 12:05:17,662 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0033, loss: 0.1112 ||: 100%|#########9| 7165/7188 [11:28<00:01, 13.79it/s]
2022-03-21 12:05:17,798 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0676, loss: 0.1112 ||: 100%|#########9| 7167/7188 [11:28<00:01, 14.04it/s]
2022-03-21 12:05:17,957 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0061, loss: 0.1112 ||: 100%|#########9| 7169/7188 [11:28<00:01, 13.57it/s]
2022-03-21 12:05:18,110 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0264, loss: 0.1112 ||: 100%|#########9| 7171/7188 [11:28<00:01, 13.42it/s]
2022-03-21 12:05:18,253 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0361, loss: 0.1112 ||: 100%|#########9| 7173/7188 [11:29<00:01, 13.58it/s]
2022-03-21 12:05:18,380 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0456, loss: 0.1112 ||: 100%|#########9| 7175/7188 [11:29<00:00, 14.17it/s]
2022-03-21 12:05:18,514 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0951, loss: 0.1112 ||: 100%|#########9| 7177/7188 [11:29<00:00, 14.39it/s]
2022-03-21 12:05:19,703 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.1975, loss: 0.1112 ||: 100%|#########9| 7179/7188 [11:30<00:02,  4.40it/s]
2022-03-21 12:05:19,847 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.1208, loss: 0.1112 ||: 100%|#########9| 7181/7188 [11:30<00:01,  5.54it/s]
2022-03-21 12:05:20,007 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0690, loss: 0.1112 ||: 100%|#########9| 7183/7188 [11:30<00:00,  6.65it/s]
2022-03-21 12:05:20,156 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0455, loss: 0.1112 ||: 100%|#########9| 7185/7188 [11:30<00:00,  7.84it/s]
2022-03-21 12:05:20,305 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0570, loss: 0.1112 ||: 100%|#########9| 7187/7188 [11:31<00:00,  8.96it/s]
2022-03-21 12:05:20,431 - INFO - tqdm - f1: 0.9621, accuracy: 0.9621, batch_loss: 0.0397, loss: 0.1112 ||: 100%|##########| 7188/7188 [11:31<00:00, 10.40it/s]
2022-03-21 12:05:20,449 - INFO - allennlp.training.trainer - Validating
2022-03-21 12:05:20,456 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 12:05:30,463 - INFO - tqdm - f1: 0.9393, accuracy: 0.9393, batch_loss: 0.1212, loss: 0.2181 ||:  97%|#########6| 303/313 [00:09<00:00, 20.26it/s]
2022-03-21 12:05:30,676 - INFO - tqdm - f1: 0.9396, accuracy: 0.9396, batch_loss: 0.1548, loss: 0.2170 ||: 100%|##########| 313/313 [00:10<00:00, 29.20it/s]
2022-03-21 12:05:30,691 - INFO - tqdm - f1: 0.9396, accuracy: 0.9396, batch_loss: 0.1548, loss: 0.2170 ||: 100%|##########| 313/313 [00:10<00:00, 30.62it/s]
2022-03-21 12:05:30,734 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 12:05:30,745 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.962  |     0.940
2022-03-21 12:05:30,759 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.962  |     0.940
2022-03-21 12:05:30,773 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 12:05:30,787 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.111  |     0.217
2022-03-21 12:05:30,801 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7592.430  |       N/A
2022-03-21 12:05:30,815 - INFO - allennlp.training.trainer - Epoch duration: 0:11:41.646285
2022-03-21 12:05:30,828 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:10:27
2022-03-21 12:05:30,842 - INFO - allennlp.training.trainer - Epoch 4/9
2022-03-21 12:05:30,855 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-21 12:05:30,871 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 12:05:30,885 - INFO - allennlp.training.trainer - Training
2022-03-21 12:05:30,900 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 12:05:40,983 - INFO - tqdm - f1: 0.9742, accuracy: 0.9743, batch_loss: 0.1482, loss: 0.0806 ||:   1%|1         | 95/7188 [00:10<14:34,  8.11it/s]
2022-03-21 12:05:51,038 - INFO - tqdm - f1: 0.9730, accuracy: 0.9732, batch_loss: 0.3277, loss: 0.0798 ||:   3%|2         | 203/7188 [00:20<16:36,  7.01it/s]
2022-03-21 12:06:01,107 - INFO - tqdm - f1: 0.9723, accuracy: 0.9723, batch_loss: 0.0108, loss: 0.0776 ||:   4%|4         | 307/7188 [00:30<20:31,  5.59it/s]
2022-03-21 12:06:11,216 - INFO - tqdm - f1: 0.9728, accuracy: 0.9729, batch_loss: 0.0017, loss: 0.0762 ||:   6%|5         | 415/7188 [00:40<19:49,  5.69it/s]
2022-03-21 12:06:21,305 - INFO - tqdm - f1: 0.9719, accuracy: 0.9719, batch_loss: 0.0077, loss: 0.0793 ||:   7%|7         | 521/7188 [00:50<19:24,  5.73it/s]
2022-03-21 12:06:31,537 - INFO - tqdm - f1: 0.9711, accuracy: 0.9711, batch_loss: 0.1886, loss: 0.0808 ||:   9%|8         | 627/7188 [01:00<24:04,  4.54it/s]
2022-03-21 12:06:41,668 - INFO - tqdm - f1: 0.9713, accuracy: 0.9714, batch_loss: 0.0070, loss: 0.0823 ||:  10%|#         | 733/7188 [01:10<23:58,  4.49it/s]
2022-03-21 12:06:51,765 - INFO - tqdm - f1: 0.9707, accuracy: 0.9707, batch_loss: 0.1322, loss: 0.0844 ||:  12%|#1        | 839/7188 [01:20<22:56,  4.61it/s]
2022-03-21 12:07:01,814 - INFO - tqdm - f1: 0.9711, accuracy: 0.9710, batch_loss: 0.5409, loss: 0.0846 ||:  13%|#3        | 949/7188 [01:30<22:31,  4.62it/s]
2022-03-21 12:07:11,940 - INFO - tqdm - f1: 0.9710, accuracy: 0.9709, batch_loss: 0.1351, loss: 0.0851 ||:  15%|#4        | 1055/7188 [01:41<17:58,  5.69it/s]
2022-03-21 12:07:21,989 - INFO - tqdm - f1: 0.9711, accuracy: 0.9710, batch_loss: 0.0029, loss: 0.0851 ||:  16%|#6        | 1163/7188 [01:51<14:04,  7.14it/s]
2022-03-21 12:07:32,000 - INFO - tqdm - f1: 0.9706, accuracy: 0.9706, batch_loss: 0.1536, loss: 0.0864 ||:  18%|#7        | 1267/7188 [02:01<17:04,  5.78it/s]
2022-03-21 12:07:42,137 - INFO - tqdm - f1: 0.9702, accuracy: 0.9701, batch_loss: 0.1105, loss: 0.0869 ||:  19%|#9        | 1373/7188 [02:11<21:20,  4.54it/s]
2022-03-21 12:07:52,237 - INFO - tqdm - f1: 0.9702, accuracy: 0.9700, batch_loss: 0.0063, loss: 0.0876 ||:  21%|##        | 1477/7188 [02:21<21:11,  4.49it/s]
2022-03-21 12:08:02,424 - INFO - tqdm - f1: 0.9703, accuracy: 0.9702, batch_loss: 0.4240, loss: 0.0872 ||:  22%|##2       | 1583/7188 [02:31<20:33,  4.54it/s]
2022-03-21 12:08:12,457 - INFO - tqdm - f1: 0.9707, accuracy: 0.9707, batch_loss: 0.0792, loss: 0.0864 ||:  23%|##3       | 1687/7188 [02:41<20:01,  4.58it/s]
2022-03-21 12:08:22,527 - INFO - tqdm - f1: 0.9705, accuracy: 0.9704, batch_loss: 0.0165, loss: 0.0872 ||:  25%|##4       | 1793/7188 [02:51<19:28,  4.62it/s]
2022-03-21 12:08:32,531 - INFO - tqdm - f1: 0.9705, accuracy: 0.9705, batch_loss: 0.2195, loss: 0.0871 ||:  26%|##6       | 1897/7188 [03:01<19:22,  4.55it/s]
2022-03-21 12:08:42,583 - INFO - tqdm - f1: 0.9708, accuracy: 0.9707, batch_loss: 0.1303, loss: 0.0860 ||:  28%|##7       | 2003/7188 [03:11<19:33,  4.42it/s]
2022-03-21 12:08:52,715 - INFO - tqdm - f1: 0.9710, accuracy: 0.9710, batch_loss: 0.2580, loss: 0.0859 ||:  29%|##9       | 2109/7188 [03:21<14:37,  5.79it/s]
2022-03-21 12:09:02,783 - INFO - tqdm - f1: 0.9706, accuracy: 0.9705, batch_loss: 0.0311, loss: 0.0869 ||:  31%|###       | 2211/7188 [03:31<12:05,  6.86it/s]
2022-03-21 12:09:12,785 - INFO - tqdm - f1: 0.9703, accuracy: 0.9702, batch_loss: 0.0109, loss: 0.0871 ||:  32%|###2      | 2315/7188 [03:41<11:41,  6.94it/s]
2022-03-21 12:09:22,841 - INFO - tqdm - f1: 0.9705, accuracy: 0.9704, batch_loss: 0.1317, loss: 0.0865 ||:  34%|###3      | 2421/7188 [03:51<09:25,  8.43it/s]
2022-03-21 12:09:32,941 - INFO - tqdm - f1: 0.9708, accuracy: 0.9707, batch_loss: 0.0119, loss: 0.0862 ||:  35%|###5      | 2527/7188 [04:02<09:35,  8.09it/s]
2022-03-21 12:09:43,071 - INFO - tqdm - f1: 0.9708, accuracy: 0.9707, batch_loss: 0.0591, loss: 0.0863 ||:  37%|###6      | 2633/7188 [04:12<07:34, 10.01it/s]
2022-03-21 12:09:53,186 - INFO - tqdm - f1: 0.9707, accuracy: 0.9706, batch_loss: 0.1154, loss: 0.0865 ||:  38%|###8      | 2737/7188 [04:22<06:54, 10.73it/s]
2022-03-21 12:10:03,251 - INFO - tqdm - f1: 0.9709, accuracy: 0.9708, batch_loss: 0.0334, loss: 0.0863 ||:  40%|###9      | 2843/7188 [04:32<06:01, 12.04it/s]
2022-03-21 12:10:13,257 - INFO - tqdm - f1: 0.9708, accuracy: 0.9707, batch_loss: 0.0076, loss: 0.0868 ||:  41%|####      | 2947/7188 [04:42<05:35, 12.63it/s]
2022-03-21 12:10:23,350 - INFO - tqdm - f1: 0.9709, accuracy: 0.9708, batch_loss: 0.3236, loss: 0.0869 ||:  42%|####2     | 3053/7188 [04:52<05:18, 12.97it/s]
2022-03-21 12:10:33,396 - INFO - tqdm - f1: 0.9710, accuracy: 0.9710, batch_loss: 0.0121, loss: 0.0868 ||:  44%|####3     | 3161/7188 [05:02<04:51, 13.83it/s]
2022-03-21 12:10:43,430 - INFO - tqdm - f1: 0.9710, accuracy: 0.9709, batch_loss: 0.0029, loss: 0.0868 ||:  45%|####5     | 3267/7188 [05:12<04:50, 13.52it/s]
2022-03-21 12:10:53,462 - INFO - tqdm - f1: 0.9709, accuracy: 0.9708, batch_loss: 0.1932, loss: 0.0874 ||:  47%|####6     | 3371/7188 [05:22<05:19, 11.96it/s]
2022-03-21 12:11:03,498 - INFO - tqdm - f1: 0.9707, accuracy: 0.9706, batch_loss: 0.0684, loss: 0.0879 ||:  48%|####8     | 3477/7188 [05:32<04:26, 13.94it/s]
2022-03-21 12:11:13,556 - INFO - tqdm - f1: 0.9709, accuracy: 0.9708, batch_loss: 0.2136, loss: 0.0875 ||:  50%|####9     | 3581/7188 [05:42<04:28, 13.42it/s]
2022-03-21 12:11:23,643 - INFO - tqdm - f1: 0.9710, accuracy: 0.9709, batch_loss: 0.0023, loss: 0.0871 ||:  51%|#####1    | 3689/7188 [05:52<03:51, 15.09it/s]
2022-03-21 12:11:33,686 - INFO - tqdm - f1: 0.9709, accuracy: 0.9709, batch_loss: 0.0261, loss: 0.0869 ||:  53%|#####2    | 3797/7188 [06:02<03:55, 14.38it/s]
2022-03-21 12:11:43,820 - INFO - tqdm - f1: 0.9708, accuracy: 0.9708, batch_loss: 0.2500, loss: 0.0876 ||:  54%|#####4    | 3905/7188 [06:12<03:42, 14.74it/s]
2022-03-21 12:11:53,957 - INFO - tqdm - f1: 0.9709, accuracy: 0.9708, batch_loss: 0.2751, loss: 0.0876 ||:  56%|#####5    | 4011/7188 [06:23<03:42, 14.26it/s]
2022-03-21 12:12:04,098 - INFO - tqdm - f1: 0.9711, accuracy: 0.9711, batch_loss: 0.0020, loss: 0.0870 ||:  57%|#####7    | 4115/7188 [06:33<03:41, 13.85it/s]
2022-03-21 12:12:14,220 - INFO - tqdm - f1: 0.9712, accuracy: 0.9711, batch_loss: 0.0128, loss: 0.0867 ||:  59%|#####8    | 4219/7188 [06:43<03:33, 13.89it/s]
2022-03-21 12:12:24,297 - INFO - tqdm - f1: 0.9711, accuracy: 0.9710, batch_loss: 0.1804, loss: 0.0873 ||:  60%|######    | 4325/7188 [06:53<03:09, 15.11it/s]
2022-03-21 12:12:34,368 - INFO - tqdm - f1: 0.9711, accuracy: 0.9711, batch_loss: 0.1552, loss: 0.0876 ||:  62%|######1   | 4433/7188 [07:03<03:08, 14.58it/s]
2022-03-21 12:12:44,447 - INFO - tqdm - f1: 0.9710, accuracy: 0.9709, batch_loss: 0.0563, loss: 0.0879 ||:  63%|######3   | 4539/7188 [07:13<03:06, 14.20it/s]
2022-03-21 12:12:54,500 - INFO - tqdm - f1: 0.9710, accuracy: 0.9710, batch_loss: 0.0198, loss: 0.0879 ||:  65%|######4   | 4645/7188 [07:23<02:59, 14.16it/s]
2022-03-21 12:13:04,564 - INFO - tqdm - f1: 0.9710, accuracy: 0.9710, batch_loss: 0.0533, loss: 0.0877 ||:  66%|######6   | 4749/7188 [07:33<02:52, 14.17it/s]
2022-03-21 12:13:14,601 - INFO - tqdm - f1: 0.9711, accuracy: 0.9710, batch_loss: 0.1251, loss: 0.0880 ||:  67%|######7   | 4851/7188 [07:43<02:49, 13.80it/s]
2022-03-21 12:13:24,616 - INFO - tqdm - f1: 0.9711, accuracy: 0.9710, batch_loss: 0.0072, loss: 0.0879 ||:  69%|######8   | 4953/7188 [07:53<02:44, 13.62it/s]
2022-03-21 12:13:34,722 - INFO - tqdm - f1: 0.9710, accuracy: 0.9710, batch_loss: 0.2160, loss: 0.0880 ||:  70%|#######   | 5059/7188 [08:03<02:27, 14.39it/s]
2022-03-21 12:13:44,780 - INFO - tqdm - f1: 0.9711, accuracy: 0.9711, batch_loss: 0.0122, loss: 0.0877 ||:  72%|#######1  | 5161/7188 [08:13<02:14, 15.10it/s]
2022-03-21 12:13:54,845 - INFO - tqdm - f1: 0.9711, accuracy: 0.9711, batch_loss: 0.0537, loss: 0.0878 ||:  73%|#######3  | 5267/7188 [08:23<02:17, 14.02it/s]
2022-03-21 12:14:04,950 - INFO - tqdm - f1: 0.9710, accuracy: 0.9710, batch_loss: 0.0386, loss: 0.0882 ||:  75%|#######4  | 5375/7188 [08:34<01:59, 15.13it/s]
2022-03-21 12:14:15,000 - INFO - tqdm - f1: 0.9709, accuracy: 0.9708, batch_loss: 0.0798, loss: 0.0888 ||:  76%|#######6  | 5481/7188 [08:44<01:45, 16.20it/s]
2022-03-21 12:14:25,054 - INFO - tqdm - f1: 0.9709, accuracy: 0.9708, batch_loss: 0.0334, loss: 0.0888 ||:  78%|#######7  | 5585/7188 [08:54<01:44, 15.31it/s]
2022-03-21 12:14:36,204 - INFO - tqdm - f1: 0.9710, accuracy: 0.9710, batch_loss: 0.0655, loss: 0.0888 ||:  79%|#######9  | 5691/7188 [09:05<05:38,  4.43it/s]
2022-03-21 12:14:46,272 - INFO - tqdm - f1: 0.9709, accuracy: 0.9709, batch_loss: 0.1553, loss: 0.0887 ||:  81%|########  | 5799/7188 [09:15<03:55,  5.91it/s]
2022-03-21 12:14:56,295 - INFO - tqdm - f1: 0.9708, accuracy: 0.9707, batch_loss: 0.0449, loss: 0.0894 ||:  82%|########2 | 5905/7188 [09:25<03:44,  5.71it/s]
2022-03-21 12:15:06,348 - INFO - tqdm - f1: 0.9707, accuracy: 0.9707, batch_loss: 0.0211, loss: 0.0894 ||:  84%|########3 | 6013/7188 [09:35<02:45,  7.11it/s]
2022-03-21 12:15:16,354 - INFO - tqdm - f1: 0.9707, accuracy: 0.9707, batch_loss: 0.1654, loss: 0.0892 ||:  85%|########5 | 6121/7188 [09:45<03:02,  5.84it/s]
2022-03-21 12:15:26,369 - INFO - tqdm - f1: 0.9707, accuracy: 0.9707, batch_loss: 0.0388, loss: 0.0892 ||:  87%|########6 | 6227/7188 [09:55<02:11,  7.30it/s]
2022-03-21 12:15:36,416 - INFO - tqdm - f1: 0.9707, accuracy: 0.9707, batch_loss: 0.2805, loss: 0.0893 ||:  88%|########8 | 6335/7188 [10:05<02:01,  7.03it/s]
2022-03-21 12:15:46,495 - INFO - tqdm - f1: 0.9706, accuracy: 0.9706, batch_loss: 0.2249, loss: 0.0894 ||:  90%|########9 | 6443/7188 [10:15<02:09,  5.75it/s]
2022-03-21 12:15:56,544 - INFO - tqdm - f1: 0.9705, accuracy: 0.9705, batch_loss: 0.2365, loss: 0.0895 ||:  91%|#########1| 6551/7188 [10:25<01:50,  5.78it/s]
2022-03-21 12:16:06,547 - INFO - tqdm - f1: 0.9704, accuracy: 0.9704, batch_loss: 0.0148, loss: 0.0897 ||:  93%|#########2| 6657/7188 [10:35<01:54,  4.63it/s]
2022-03-21 12:16:16,596 - INFO - tqdm - f1: 0.9704, accuracy: 0.9703, batch_loss: 0.0271, loss: 0.0899 ||:  94%|#########4| 6763/7188 [10:45<01:15,  5.60it/s]
2022-03-21 12:16:26,636 - INFO - tqdm - f1: 0.9704, accuracy: 0.9704, batch_loss: 0.0203, loss: 0.0897 ||:  96%|#########5| 6865/7188 [10:55<00:57,  5.63it/s]
2022-03-21 12:16:36,734 - INFO - tqdm - f1: 0.9704, accuracy: 0.9703, batch_loss: 0.0148, loss: 0.0899 ||:  97%|#########6| 6969/7188 [11:05<00:26,  8.32it/s]
2022-03-21 12:16:46,848 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0075, loss: 0.0902 ||:  98%|#########8| 7073/7188 [11:15<00:13,  8.22it/s]
2022-03-21 12:16:54,336 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0079, loss: 0.0902 ||: 100%|#########9| 7153/7188 [11:23<00:02, 13.97it/s]
2022-03-21 12:16:54,494 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0039, loss: 0.0901 ||: 100%|#########9| 7155/7188 [11:23<00:02, 13.53it/s]
2022-03-21 12:16:54,645 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0717, loss: 0.0901 ||: 100%|#########9| 7157/7188 [11:23<00:02, 13.46it/s]
2022-03-21 12:16:54,820 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.1096, loss: 0.0902 ||: 100%|#########9| 7159/7188 [11:23<00:02, 12.76it/s]
2022-03-21 12:16:54,972 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.5170, loss: 0.0902 ||: 100%|#########9| 7161/7188 [11:24<00:02, 12.88it/s]
2022-03-21 12:16:55,119 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0875, loss: 0.0902 ||: 100%|#########9| 7163/7188 [11:24<00:01, 13.10it/s]
2022-03-21 12:16:55,276 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0496, loss: 0.0902 ||: 100%|#########9| 7165/7188 [11:24<00:01, 12.99it/s]
2022-03-21 12:16:55,402 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0905, loss: 0.0902 ||: 100%|#########9| 7167/7188 [11:24<00:01, 13.74it/s]
2022-03-21 12:16:56,614 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0108, loss: 0.0902 ||: 100%|#########9| 7169/7188 [11:25<00:04,  4.30it/s]
2022-03-21 12:16:56,752 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0229, loss: 0.0902 ||: 100%|#########9| 7171/7188 [11:25<00:03,  5.44it/s]
2022-03-21 12:16:56,908 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0575, loss: 0.0902 ||: 100%|#########9| 7173/7188 [11:25<00:02,  6.58it/s]
2022-03-21 12:16:57,079 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0064, loss: 0.0902 ||: 100%|#########9| 7175/7188 [11:26<00:01,  7.58it/s]
2022-03-21 12:16:57,219 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0482, loss: 0.0902 ||: 100%|#########9| 7177/7188 [11:26<00:01,  8.82it/s]
2022-03-21 12:16:57,363 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0400, loss: 0.0902 ||: 100%|#########9| 7179/7188 [11:26<00:00,  9.91it/s]
2022-03-21 12:16:57,497 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0043, loss: 0.0902 ||: 100%|#########9| 7181/7188 [11:26<00:00, 11.01it/s]
2022-03-21 12:16:57,642 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0763, loss: 0.0902 ||: 100%|#########9| 7183/7188 [11:26<00:00, 11.72it/s]
2022-03-21 12:16:57,799 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0983, loss: 0.0902 ||: 100%|#########9| 7185/7188 [11:26<00:00, 12.00it/s]
2022-03-21 12:16:57,964 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0473, loss: 0.0902 ||: 100%|#########9| 7187/7188 [11:27<00:00, 12.03it/s]
2022-03-21 12:16:58,096 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0744, loss: 0.0902 ||: 100%|##########| 7188/7188 [11:27<00:00, 10.46it/s]
2022-03-21 12:16:58,112 - INFO - allennlp.training.trainer - Validating
2022-03-21 12:16:58,122 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 12:17:07,993 - INFO - tqdm - f1: 0.9426, accuracy: 0.9426, batch_loss: 0.2047, loss: 0.2264 ||: 100%|##########| 313/313 [00:09<00:00, 31.76it/s]
2022-03-21 12:17:08,042 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/ag_base_hyper_small_seed_97/best.th'.
2022-03-21 12:17:12,452 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 12:17:12,468 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.970  |     0.943
2022-03-21 12:17:12,483 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.970  |     0.943
2022-03-21 12:17:12,499 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 12:17:12,515 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.090  |     0.226
2022-03-21 12:17:12,532 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7592.430  |       N/A
2022-03-21 12:17:12,548 - INFO - allennlp.training.trainer - Epoch duration: 0:11:41.706105
2022-03-21 12:17:12,568 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:58:39
2022-03-21 12:17:12,584 - INFO - allennlp.training.trainer - Epoch 5/9
2022-03-21 12:17:12,606 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-21 12:17:12,617 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 12:17:12,637 - INFO - allennlp.training.trainer - Training
2022-03-21 12:17:12,649 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 12:17:23,296 - INFO - tqdm - f1: 0.9771, accuracy: 0.9774, batch_loss: 0.3327, loss: 0.0594 ||:   1%|1         | 105/7188 [00:10<25:40,  4.60it/s]
2022-03-21 12:17:33,326 - INFO - tqdm - f1: 0.9776, accuracy: 0.9775, batch_loss: 0.3189, loss: 0.0755 ||:   3%|2         | 211/7188 [00:20<20:21,  5.71it/s]
2022-03-21 12:17:43,453 - INFO - tqdm - f1: 0.9769, accuracy: 0.9769, batch_loss: 0.0034, loss: 0.0756 ||:   4%|4         | 319/7188 [00:30<25:37,  4.47it/s]
2022-03-21 12:17:53,530 - INFO - tqdm - f1: 0.9788, accuracy: 0.9788, batch_loss: 0.0665, loss: 0.0682 ||:   6%|5         | 427/7188 [00:40<19:23,  5.81it/s]
2022-03-21 12:18:03,563 - INFO - tqdm - f1: 0.9790, accuracy: 0.9790, batch_loss: 0.0119, loss: 0.0647 ||:   7%|7         | 533/7188 [00:50<19:19,  5.74it/s]
2022-03-21 12:18:13,589 - INFO - tqdm - f1: 0.9783, accuracy: 0.9782, batch_loss: 0.2336, loss: 0.0668 ||:   9%|8         | 639/7188 [01:00<23:32,  4.64it/s]
2022-03-21 12:18:23,756 - INFO - tqdm - f1: 0.9776, accuracy: 0.9775, batch_loss: 0.0077, loss: 0.0693 ||:  10%|#         | 747/7188 [01:11<23:28,  4.57it/s]
2022-03-21 12:18:34,078 - INFO - tqdm - f1: 0.9765, accuracy: 0.9764, batch_loss: 0.0989, loss: 0.0721 ||:  12%|#1        | 857/7188 [01:21<21:08,  4.99it/s]
2022-03-21 12:18:44,128 - INFO - tqdm - f1: 0.9764, accuracy: 0.9764, batch_loss: 0.0032, loss: 0.0716 ||:  13%|#3        | 963/7188 [01:31<22:50,  4.54it/s]
2022-03-21 12:18:54,129 - INFO - tqdm - f1: 0.9762, accuracy: 0.9762, batch_loss: 0.0500, loss: 0.0722 ||:  15%|#4        | 1067/7188 [01:41<22:37,  4.51it/s]
2022-03-21 12:19:04,158 - INFO - tqdm - f1: 0.9762, accuracy: 0.9762, batch_loss: 0.0029, loss: 0.0714 ||:  16%|#6        | 1169/7188 [01:51<22:00,  4.56it/s]
2022-03-21 12:19:14,310 - INFO - tqdm - f1: 0.9763, accuracy: 0.9763, batch_loss: 0.0661, loss: 0.0709 ||:  18%|#7        | 1277/7188 [02:01<21:39,  4.55it/s]
2022-03-21 12:19:24,328 - INFO - tqdm - f1: 0.9769, accuracy: 0.9769, batch_loss: 0.0197, loss: 0.0694 ||:  19%|#9        | 1383/7188 [02:11<21:42,  4.46it/s]
2022-03-21 12:19:34,377 - INFO - tqdm - f1: 0.9766, accuracy: 0.9766, batch_loss: 0.0404, loss: 0.0698 ||:  21%|##        | 1491/7188 [02:21<13:10,  7.21it/s]
2022-03-21 12:19:44,475 - INFO - tqdm - f1: 0.9763, accuracy: 0.9763, batch_loss: 0.0544, loss: 0.0704 ||:  22%|##2       | 1599/7188 [02:31<09:55,  9.39it/s]
2022-03-21 12:19:54,517 - INFO - tqdm - f1: 0.9763, accuracy: 0.9763, batch_loss: 0.6901, loss: 0.0705 ||:  24%|##3       | 1705/7188 [02:41<11:13,  8.14it/s]
2022-03-21 12:20:04,601 - INFO - tqdm - f1: 0.9762, accuracy: 0.9762, batch_loss: 0.0126, loss: 0.0706 ||:  25%|##5       | 1809/7188 [02:51<08:20, 10.75it/s]
2022-03-21 12:20:14,694 - INFO - tqdm - f1: 0.9758, accuracy: 0.9758, batch_loss: 0.0039, loss: 0.0712 ||:  27%|##6       | 1913/7188 [03:02<10:41,  8.23it/s]
2022-03-21 12:20:24,706 - INFO - tqdm - f1: 0.9760, accuracy: 0.9760, batch_loss: 0.0057, loss: 0.0713 ||:  28%|##8       | 2017/7188 [03:12<12:36,  6.84it/s]
2022-03-21 12:20:34,785 - INFO - tqdm - f1: 0.9760, accuracy: 0.9761, batch_loss: 0.0237, loss: 0.0716 ||:  30%|##9       | 2125/7188 [03:22<11:56,  7.07it/s]
2022-03-21 12:20:44,833 - INFO - tqdm - f1: 0.9760, accuracy: 0.9760, batch_loss: 0.0677, loss: 0.0720 ||:  31%|###1      | 2231/7188 [03:32<11:52,  6.95it/s]
2022-03-21 12:20:54,918 - INFO - tqdm - f1: 0.9762, accuracy: 0.9761, batch_loss: 0.0152, loss: 0.0721 ||:  33%|###2      | 2337/7188 [03:42<09:53,  8.17it/s]
2022-03-21 12:21:04,961 - INFO - tqdm - f1: 0.9762, accuracy: 0.9762, batch_loss: 0.1242, loss: 0.0718 ||:  34%|###3      | 2443/7188 [03:52<11:18,  7.00it/s]
2022-03-21 12:21:14,976 - INFO - tqdm - f1: 0.9761, accuracy: 0.9761, batch_loss: 0.0705, loss: 0.0721 ||:  35%|###5      | 2549/7188 [04:02<13:35,  5.69it/s]
2022-03-21 12:21:25,088 - INFO - tqdm - f1: 0.9759, accuracy: 0.9759, batch_loss: 0.0036, loss: 0.0725 ||:  37%|###6      | 2655/7188 [04:12<13:03,  5.79it/s]
2022-03-21 12:21:35,148 - INFO - tqdm - f1: 0.9762, accuracy: 0.9761, batch_loss: 0.0022, loss: 0.0718 ||:  38%|###8      | 2759/7188 [04:22<12:50,  5.74it/s]
2022-03-21 12:21:45,155 - INFO - tqdm - f1: 0.9761, accuracy: 0.9761, batch_loss: 0.0087, loss: 0.0722 ||:  40%|###9      | 2867/7188 [04:32<12:27,  5.78it/s]
2022-03-21 12:21:55,292 - INFO - tqdm - f1: 0.9761, accuracy: 0.9761, batch_loss: 0.0025, loss: 0.0718 ||:  41%|####1     | 2977/7188 [04:42<15:33,  4.51it/s]
2022-03-21 12:22:05,566 - INFO - tqdm - f1: 0.9758, accuracy: 0.9758, batch_loss: 0.0728, loss: 0.0728 ||:  43%|####2     | 3085/7188 [04:52<15:10,  4.51it/s]
2022-03-21 12:22:15,983 - INFO - tqdm - f1: 0.9756, accuracy: 0.9756, batch_loss: 0.2147, loss: 0.0731 ||:  44%|####4     | 3197/7188 [05:03<14:34,  4.56it/s]
2022-03-21 12:22:26,308 - INFO - tqdm - f1: 0.9758, accuracy: 0.9757, batch_loss: 0.3928, loss: 0.0726 ||:  46%|####6     | 3307/7188 [05:13<14:30,  4.46it/s]
2022-03-21 12:22:36,474 - INFO - tqdm - f1: 0.9756, accuracy: 0.9756, batch_loss: 0.0057, loss: 0.0731 ||:  47%|####7     | 3413/7188 [05:23<14:02,  4.48it/s]
2022-03-21 12:22:46,558 - INFO - tqdm - f1: 0.9757, accuracy: 0.9756, batch_loss: 0.0164, loss: 0.0730 ||:  49%|####8     | 3519/7188 [05:33<10:32,  5.80it/s]
2022-03-21 12:22:57,166 - INFO - tqdm - f1: 0.9756, accuracy: 0.9756, batch_loss: 0.0138, loss: 0.0733 ||:  50%|#####     | 3629/7188 [05:44<13:13,  4.48it/s]
2022-03-21 12:23:08,065 - INFO - tqdm - f1: 0.9756, accuracy: 0.9756, batch_loss: 0.4881, loss: 0.0732 ||:  52%|#####2    | 3743/7188 [05:55<13:04,  4.39it/s]
2022-03-21 12:23:19,181 - INFO - tqdm - f1: 0.9756, accuracy: 0.9756, batch_loss: 0.0095, loss: 0.0731 ||:  54%|#####3    | 3859/7188 [06:06<12:42,  4.37it/s]
2022-03-21 12:23:30,107 - INFO - tqdm - f1: 0.9756, accuracy: 0.9756, batch_loss: 0.0269, loss: 0.0731 ||:  55%|#####5    | 3975/7188 [06:17<12:06,  4.42it/s]
2022-03-21 12:23:40,949 - INFO - tqdm - f1: 0.9756, accuracy: 0.9756, batch_loss: 0.0034, loss: 0.0733 ||:  57%|#####6    | 4093/7188 [06:28<09:36,  5.37it/s]
2022-03-21 12:23:52,116 - INFO - tqdm - f1: 0.9754, accuracy: 0.9754, batch_loss: 0.2244, loss: 0.0738 ||:  59%|#####8    | 4211/7188 [06:39<11:21,  4.37it/s]
2022-03-21 12:24:02,218 - INFO - tqdm - f1: 0.9753, accuracy: 0.9753, batch_loss: 0.0053, loss: 0.0742 ||:  60%|######    | 4317/7188 [06:49<10:37,  4.50it/s]
2022-03-21 12:24:12,271 - INFO - tqdm - f1: 0.9753, accuracy: 0.9753, batch_loss: 0.0056, loss: 0.0737 ||:  62%|######1   | 4421/7188 [06:59<08:06,  5.69it/s]
2022-03-21 12:24:22,365 - INFO - tqdm - f1: 0.9753, accuracy: 0.9753, batch_loss: 0.0646, loss: 0.0738 ||:  63%|######3   | 4531/7188 [07:09<07:47,  5.68it/s]
2022-03-21 12:24:32,523 - INFO - tqdm - f1: 0.9753, accuracy: 0.9753, batch_loss: 0.0127, loss: 0.0741 ||:  65%|######4   | 4639/7188 [07:19<09:19,  4.56it/s]
2022-03-21 12:24:42,538 - INFO - tqdm - f1: 0.9753, accuracy: 0.9753, batch_loss: 0.0566, loss: 0.0744 ||:  66%|######6   | 4745/7188 [07:29<09:02,  4.50it/s]
2022-03-21 12:24:52,597 - INFO - tqdm - f1: 0.9753, accuracy: 0.9753, batch_loss: 0.0030, loss: 0.0745 ||:  68%|######7   | 4853/7188 [07:39<08:28,  4.59it/s]
2022-03-21 12:25:02,815 - INFO - tqdm - f1: 0.9753, accuracy: 0.9753, batch_loss: 0.2312, loss: 0.0745 ||:  69%|######9   | 4961/7188 [07:50<07:59,  4.64it/s]
2022-03-21 12:25:12,962 - INFO - tqdm - f1: 0.9751, accuracy: 0.9752, batch_loss: 0.0048, loss: 0.0748 ||:  70%|#######   | 5065/7188 [08:00<07:42,  4.59it/s]
2022-03-21 12:25:22,995 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.0466, loss: 0.0750 ||:  72%|#######1  | 5169/7188 [08:10<07:28,  4.50it/s]
2022-03-21 12:25:33,317 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.0078, loss: 0.0751 ||:  73%|#######3  | 5281/7188 [08:20<05:47,  5.49it/s]
2022-03-21 12:25:43,340 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.1056, loss: 0.0750 ||:  75%|#######4  | 5385/7188 [08:30<02:16, 13.17it/s]
2022-03-21 12:25:53,433 - INFO - tqdm - f1: 0.9751, accuracy: 0.9751, batch_loss: 0.0933, loss: 0.0749 ||:  76%|#######6  | 5489/7188 [08:40<02:04, 13.67it/s]
2022-03-21 12:26:03,561 - INFO - tqdm - f1: 0.9751, accuracy: 0.9751, batch_loss: 0.2836, loss: 0.0748 ||:  78%|#######7  | 5595/7188 [08:50<01:51, 14.30it/s]
2022-03-21 12:26:13,655 - INFO - tqdm - f1: 0.9751, accuracy: 0.9751, batch_loss: 0.0209, loss: 0.0750 ||:  79%|#######9  | 5701/7188 [09:00<01:42, 14.53it/s]
2022-03-21 12:26:23,731 - INFO - tqdm - f1: 0.9752, accuracy: 0.9752, batch_loss: 0.0675, loss: 0.0748 ||:  81%|########  | 5807/7188 [09:11<01:38, 14.08it/s]
2022-03-21 12:26:33,746 - INFO - tqdm - f1: 0.9752, accuracy: 0.9752, batch_loss: 0.1400, loss: 0.0749 ||:  82%|########2 | 5913/7188 [09:21<01:33, 13.63it/s]
2022-03-21 12:26:43,764 - INFO - tqdm - f1: 0.9752, accuracy: 0.9752, batch_loss: 0.2767, loss: 0.0750 ||:  84%|########3 | 6017/7188 [09:31<01:19, 14.69it/s]
2022-03-21 12:26:53,810 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.0075, loss: 0.0753 ||:  85%|########5 | 6123/7188 [09:41<01:15, 14.08it/s]
2022-03-21 12:27:03,925 - INFO - tqdm - f1: 0.9749, accuracy: 0.9749, batch_loss: 0.1660, loss: 0.0755 ||:  87%|########6 | 6233/7188 [09:51<01:13, 12.92it/s]
2022-03-21 12:27:14,033 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.3075, loss: 0.0755 ||:  88%|########8 | 6339/7188 [10:01<01:05, 13.00it/s]
2022-03-21 12:27:24,144 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.0212, loss: 0.0755 ||:  90%|########9 | 6445/7188 [10:11<00:57, 12.88it/s]
2022-03-21 12:27:34,184 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.0147, loss: 0.0755 ||:  91%|#########1| 6547/7188 [10:21<00:45, 14.14it/s]
2022-03-21 12:27:44,247 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.0372, loss: 0.0755 ||:  93%|#########2| 6653/7188 [10:31<00:33, 15.83it/s]
2022-03-21 12:27:54,309 - INFO - tqdm - f1: 0.9749, accuracy: 0.9749, batch_loss: 0.0177, loss: 0.0756 ||:  94%|#########4| 6761/7188 [10:41<00:28, 14.86it/s]
2022-03-21 12:28:04,378 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.1947, loss: 0.0757 ||:  96%|#########5| 6867/7188 [10:51<00:21, 14.86it/s]
2022-03-21 12:28:14,495 - INFO - tqdm - f1: 0.9749, accuracy: 0.9749, batch_loss: 0.0575, loss: 0.0760 ||:  97%|#########7| 6973/7188 [11:01<00:14, 14.65it/s]
2022-03-21 12:28:24,571 - INFO - tqdm - f1: 0.9749, accuracy: 0.9749, batch_loss: 0.1729, loss: 0.0759 ||:  98%|#########8| 7077/7188 [11:11<00:08, 13.63it/s]
2022-03-21 12:28:31,678 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0370, loss: 0.0761 ||: 100%|#########9| 7153/7188 [11:19<00:02, 15.09it/s]
2022-03-21 12:28:31,818 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0245, loss: 0.0761 ||: 100%|#########9| 7155/7188 [11:19<00:02, 14.86it/s]
2022-03-21 12:28:31,942 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.1133, loss: 0.0761 ||: 100%|#########9| 7157/7188 [11:19<00:02, 15.21it/s]
2022-03-21 12:28:32,063 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0468, loss: 0.0761 ||: 100%|#########9| 7159/7188 [11:19<00:01, 15.59it/s]
2022-03-21 12:28:33,224 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.2044, loss: 0.0761 ||: 100%|#########9| 7161/7188 [11:20<00:05,  4.56it/s]
2022-03-21 12:28:33,377 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0061, loss: 0.0761 ||: 100%|#########9| 7163/7188 [11:20<00:04,  5.67it/s]
2022-03-21 12:28:33,532 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0044, loss: 0.0761 ||: 100%|#########9| 7165/7188 [11:20<00:03,  6.82it/s]
2022-03-21 12:28:33,691 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.1383, loss: 0.0761 ||: 100%|#########9| 7167/7188 [11:21<00:02,  7.91it/s]
2022-03-21 12:28:33,844 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0087, loss: 0.0761 ||: 100%|#########9| 7169/7188 [11:21<00:02,  8.97it/s]
2022-03-21 12:28:33,984 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0045, loss: 0.0761 ||: 100%|#########9| 7171/7188 [11:21<00:01, 10.10it/s]
2022-03-21 12:28:34,135 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0048, loss: 0.0761 ||: 100%|#########9| 7173/7188 [11:21<00:01, 10.87it/s]
2022-03-21 12:28:34,288 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.1434, loss: 0.0761 ||: 100%|#########9| 7175/7188 [11:21<00:01, 11.44it/s]
2022-03-21 12:28:34,445 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0028, loss: 0.0761 ||: 100%|#########9| 7177/7188 [11:21<00:00, 11.80it/s]
2022-03-21 12:28:34,601 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0059, loss: 0.0761 ||: 100%|#########9| 7179/7188 [11:21<00:00, 12.09it/s]
2022-03-21 12:28:34,746 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0053, loss: 0.0761 ||: 100%|#########9| 7181/7188 [11:22<00:00, 12.56it/s]
2022-03-21 12:28:34,886 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0031, loss: 0.0761 ||: 100%|#########9| 7183/7188 [11:22<00:00, 13.03it/s]
2022-03-21 12:28:35,039 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0302, loss: 0.0761 ||: 100%|#########9| 7185/7188 [11:22<00:00, 13.05it/s]
2022-03-21 12:28:35,159 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0530, loss: 0.0761 ||: 100%|#########9| 7187/7188 [11:22<00:00, 13.95it/s]
2022-03-21 12:28:35,280 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0027, loss: 0.0761 ||: 100%|##########| 7188/7188 [11:22<00:00, 10.53it/s]
2022-03-21 12:28:35,344 - INFO - allennlp.training.trainer - Validating
2022-03-21 12:28:35,363 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 12:28:45,409 - INFO - tqdm - f1: 0.9357, accuracy: 0.9358, batch_loss: 0.0019, loss: 0.2603 ||:  98%|#########8| 307/313 [00:10<00:00, 42.97it/s]
2022-03-21 12:28:45,525 - INFO - tqdm - f1: 0.9357, accuracy: 0.9358, batch_loss: 0.1732, loss: 0.2589 ||: 100%|#########9| 312/313 [00:10<00:00, 43.04it/s]
2022-03-21 12:28:45,562 - INFO - tqdm - f1: 0.9357, accuracy: 0.9358, batch_loss: 0.0962, loss: 0.2584 ||: 100%|##########| 313/313 [00:10<00:00, 30.74it/s]
2022-03-21 12:28:45,600 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 12:28:45,612 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.975  |     0.936
2022-03-21 12:28:45,630 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.975  |     0.936
2022-03-21 12:28:45,650 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 12:28:45,670 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.076  |     0.258
2022-03-21 12:28:45,693 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7592.430  |       N/A
2022-03-21 12:28:45,716 - INFO - allennlp.training.trainer - Epoch duration: 0:11:33.132299
2022-03-21 12:28:45,735 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:46:48
2022-03-21 12:28:45,755 - INFO - allennlp.training.trainer - Epoch 6/9
2022-03-21 12:28:45,776 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-21 12:28:45,798 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 12:28:45,820 - INFO - allennlp.training.trainer - Training
2022-03-21 12:28:45,839 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 12:28:55,867 - INFO - tqdm - f1: 0.9794, accuracy: 0.9796, batch_loss: 0.0152, loss: 0.0558 ||:   1%|1         | 107/7188 [00:10<07:34, 15.57it/s]
2022-03-21 12:29:05,888 - INFO - tqdm - f1: 0.9798, accuracy: 0.9799, batch_loss: 0.0211, loss: 0.0558 ||:   3%|2         | 211/7188 [00:20<07:04, 16.44it/s]
2022-03-21 12:29:15,993 - INFO - tqdm - f1: 0.9800, accuracy: 0.9800, batch_loss: 0.0198, loss: 0.0538 ||:   4%|4         | 315/7188 [00:30<07:44, 14.79it/s]
2022-03-21 12:29:26,011 - INFO - tqdm - f1: 0.9805, accuracy: 0.9805, batch_loss: 0.0175, loss: 0.0545 ||:   6%|5         | 411/7188 [00:40<09:20, 12.09it/s]
2022-03-21 12:29:36,094 - INFO - tqdm - f1: 0.9798, accuracy: 0.9798, batch_loss: 0.0045, loss: 0.0573 ||:   8%|7         | 555/7188 [00:50<07:39, 14.43it/s]
2022-03-21 12:29:46,107 - INFO - tqdm - f1: 0.9804, accuracy: 0.9806, batch_loss: 0.0065, loss: 0.0564 ||:  10%|9         | 707/7188 [01:00<06:52, 15.71it/s]
2022-03-21 12:29:56,135 - INFO - tqdm - f1: 0.9809, accuracy: 0.9810, batch_loss: 0.0019, loss: 0.0566 ||:  12%|#1        | 857/7188 [01:10<07:16, 14.50it/s]
2022-03-21 12:30:06,237 - INFO - tqdm - f1: 0.9813, accuracy: 0.9814, batch_loss: 0.1840, loss: 0.0574 ||:  14%|#4        | 1013/7188 [01:20<06:38, 15.50it/s]
2022-03-21 12:30:16,309 - INFO - tqdm - f1: 0.9814, accuracy: 0.9815, batch_loss: 0.0030, loss: 0.0576 ||:  16%|#6        | 1167/7188 [01:30<06:18, 15.89it/s]
2022-03-21 12:30:26,403 - INFO - tqdm - f1: 0.9814, accuracy: 0.9815, batch_loss: 0.0185, loss: 0.0574 ||:  18%|#8        | 1323/7188 [01:40<06:19, 15.44it/s]
2022-03-21 12:30:36,463 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0293, loss: 0.0567 ||:  21%|##        | 1481/7188 [01:50<05:56, 16.00it/s]
2022-03-21 12:30:46,525 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0327, loss: 0.0567 ||:  23%|##2       | 1635/7188 [02:00<05:49, 15.87it/s]
2022-03-21 12:30:56,608 - INFO - tqdm - f1: 0.9814, accuracy: 0.9815, batch_loss: 0.0532, loss: 0.0573 ||:  25%|##4       | 1789/7188 [02:10<05:55, 15.17it/s]
2022-03-21 12:31:06,677 - INFO - tqdm - f1: 0.9811, accuracy: 0.9811, batch_loss: 0.0068, loss: 0.0578 ||:  27%|##7       | 1943/7188 [02:20<05:39, 15.44it/s]
2022-03-21 12:31:16,709 - INFO - tqdm - f1: 0.9807, accuracy: 0.9807, batch_loss: 0.0040, loss: 0.0580 ||:  29%|##9       | 2095/7188 [02:30<05:31, 15.36it/s]
2022-03-21 12:31:26,797 - INFO - tqdm - f1: 0.9805, accuracy: 0.9805, batch_loss: 0.0684, loss: 0.0592 ||:  31%|###1      | 2247/7188 [02:40<05:25, 15.17it/s]
2022-03-21 12:31:36,911 - INFO - tqdm - f1: 0.9800, accuracy: 0.9801, batch_loss: 0.0135, loss: 0.0600 ||:  33%|###3      | 2383/7188 [02:51<05:06, 15.70it/s]
2022-03-21 12:31:46,994 - INFO - tqdm - f1: 0.9798, accuracy: 0.9798, batch_loss: 0.0164, loss: 0.0605 ||:  35%|###5      | 2537/7188 [03:01<05:30, 14.05it/s]
2022-03-21 12:31:57,041 - INFO - tqdm - f1: 0.9798, accuracy: 0.9798, batch_loss: 0.0021, loss: 0.0606 ||:  37%|###7      | 2691/7188 [03:11<04:53, 15.30it/s]
2022-03-21 12:32:07,152 - INFO - tqdm - f1: 0.9794, accuracy: 0.9794, batch_loss: 0.0128, loss: 0.0619 ||:  40%|###9      | 2845/7188 [03:21<04:44, 15.29it/s]
2022-03-21 12:32:17,167 - INFO - tqdm - f1: 0.9795, accuracy: 0.9795, batch_loss: 0.0181, loss: 0.0619 ||:  42%|####1     | 3001/7188 [03:31<04:11, 16.63it/s]
2022-03-21 12:32:27,233 - INFO - tqdm - f1: 0.9795, accuracy: 0.9795, batch_loss: 0.0269, loss: 0.0620 ||:  44%|####3     | 3155/7188 [03:41<04:40, 14.39it/s]
2022-03-21 12:32:37,251 - INFO - tqdm - f1: 0.9793, accuracy: 0.9793, batch_loss: 0.0074, loss: 0.0622 ||:  46%|####6     | 3307/7188 [03:51<04:13, 15.33it/s]
2022-03-21 12:32:47,278 - INFO - tqdm - f1: 0.9792, accuracy: 0.9792, batch_loss: 0.0104, loss: 0.0629 ||:  48%|####8     | 3457/7188 [04:01<04:03, 15.33it/s]
2022-03-21 12:32:57,348 - INFO - tqdm - f1: 0.9793, accuracy: 0.9793, batch_loss: 0.3587, loss: 0.0629 ||:  50%|#####     | 3611/7188 [04:11<03:52, 15.36it/s]
2022-03-21 12:33:07,492 - INFO - tqdm - f1: 0.9793, accuracy: 0.9793, batch_loss: 0.0833, loss: 0.0630 ||:  52%|#####2    | 3763/7188 [04:21<04:01, 14.18it/s]
2022-03-21 12:33:17,537 - INFO - tqdm - f1: 0.9793, accuracy: 0.9793, batch_loss: 0.0176, loss: 0.0631 ||:  55%|#####4    | 3919/7188 [04:31<03:48, 14.31it/s]
2022-03-21 12:33:27,587 - INFO - tqdm - f1: 0.9792, accuracy: 0.9792, batch_loss: 0.0205, loss: 0.0635 ||:  57%|#####6    | 4073/7188 [04:41<03:13, 16.09it/s]
2022-03-21 12:33:37,680 - INFO - tqdm - f1: 0.9792, accuracy: 0.9792, batch_loss: 0.1249, loss: 0.0640 ||:  59%|#####8    | 4225/7188 [04:51<03:10, 15.54it/s]
2022-03-21 12:33:47,776 - INFO - tqdm - f1: 0.9792, accuracy: 0.9792, batch_loss: 0.0475, loss: 0.0642 ||:  61%|######    | 4379/7188 [05:01<03:11, 14.64it/s]
2022-03-21 12:33:57,833 - INFO - tqdm - f1: 0.9793, accuracy: 0.9793, batch_loss: 0.0026, loss: 0.0639 ||:  63%|######3   | 4531/7188 [05:11<02:56, 15.09it/s]
2022-03-21 12:34:07,874 - INFO - tqdm - f1: 0.9793, accuracy: 0.9793, batch_loss: 0.0023, loss: 0.0640 ||:  65%|######5   | 4685/7188 [05:22<02:37, 15.85it/s]
2022-03-21 12:34:17,915 - INFO - tqdm - f1: 0.9793, accuracy: 0.9793, batch_loss: 0.0066, loss: 0.0639 ||:  67%|######7   | 4839/7188 [05:32<02:32, 15.36it/s]
2022-03-21 12:34:27,971 - INFO - tqdm - f1: 0.9792, accuracy: 0.9792, batch_loss: 0.0161, loss: 0.0638 ||:  69%|######9   | 4991/7188 [05:42<02:23, 15.26it/s]
2022-03-21 12:34:38,025 - INFO - tqdm - f1: 0.9791, accuracy: 0.9791, batch_loss: 0.0020, loss: 0.0640 ||:  72%|#######1  | 5147/7188 [05:52<02:06, 16.18it/s]
2022-03-21 12:34:48,095 - INFO - tqdm - f1: 0.9792, accuracy: 0.9792, batch_loss: 0.1608, loss: 0.0638 ||:  74%|#######3  | 5301/7188 [06:02<02:02, 15.40it/s]
2022-03-21 12:34:58,218 - INFO - tqdm - f1: 0.9792, accuracy: 0.9792, batch_loss: 0.0016, loss: 0.0638 ||:  76%|#######5  | 5455/7188 [06:12<01:52, 15.34it/s]
2022-03-21 12:35:08,307 - INFO - tqdm - f1: 0.9791, accuracy: 0.9791, batch_loss: 0.0283, loss: 0.0643 ||:  78%|#######8  | 5613/7188 [06:22<01:43, 15.20it/s]
2022-03-21 12:35:18,332 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0025, loss: 0.0645 ||:  80%|########  | 5767/7188 [06:32<01:32, 15.34it/s]
2022-03-21 12:35:28,434 - INFO - tqdm - f1: 0.9790, accuracy: 0.9790, batch_loss: 0.3231, loss: 0.0644 ||:  82%|########2 | 5923/7188 [06:42<01:21, 15.48it/s]
2022-03-21 12:35:38,570 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0715, loss: 0.0645 ||:  84%|########4 | 6067/7188 [06:52<01:25, 13.13it/s]
2022-03-21 12:35:48,600 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.1845, loss: 0.0646 ||:  86%|########5 | 6171/7188 [07:02<01:09, 14.62it/s]
2022-03-21 12:35:58,697 - INFO - tqdm - f1: 0.9790, accuracy: 0.9790, batch_loss: 0.0138, loss: 0.0645 ||:  87%|########7 | 6275/7188 [07:12<01:02, 14.57it/s]
2022-03-21 12:36:08,736 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.2786, loss: 0.0647 ||:  89%|########8 | 6379/7188 [07:22<00:54, 14.81it/s]
2022-03-21 12:36:18,825 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0212, loss: 0.0650 ||:  90%|######### | 6483/7188 [07:32<00:48, 14.61it/s]
2022-03-21 12:36:28,916 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0081, loss: 0.0650 ||:  92%|#########1| 6589/7188 [07:43<00:42, 13.95it/s]
2022-03-21 12:36:39,054 - INFO - tqdm - f1: 0.9788, accuracy: 0.9788, batch_loss: 0.0090, loss: 0.0653 ||:  93%|#########3| 6695/7188 [07:53<00:36, 13.45it/s]
2022-03-21 12:36:49,178 - INFO - tqdm - f1: 0.9788, accuracy: 0.9788, batch_loss: 0.0352, loss: 0.0652 ||:  94%|#########4| 6783/7188 [08:03<00:34, 11.78it/s]
2022-03-21 12:36:59,282 - INFO - tqdm - f1: 0.9788, accuracy: 0.9788, batch_loss: 0.0059, loss: 0.0652 ||:  96%|#########5| 6889/7188 [08:13<00:24, 12.43it/s]
2022-03-21 12:37:09,584 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0138, loss: 0.0650 ||:  97%|#########7| 6991/7188 [08:23<00:43,  4.52it/s]
2022-03-21 12:37:19,658 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0354, loss: 0.0649 ||:  99%|#########9| 7137/7188 [08:33<00:03, 14.76it/s]
2022-03-21 12:37:20,672 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.2540, loss: 0.0650 ||: 100%|#########9| 7153/7188 [08:34<00:02, 16.56it/s]
2022-03-21 12:37:20,805 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0597, loss: 0.0650 ||: 100%|#########9| 7155/7188 [08:34<00:02, 16.10it/s]
2022-03-21 12:37:20,947 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0167, loss: 0.0650 ||: 100%|#########9| 7157/7188 [08:35<00:02, 15.41it/s]
2022-03-21 12:37:21,102 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0199, loss: 0.0650 ||: 100%|#########9| 7159/7188 [08:35<00:01, 14.59it/s]
2022-03-21 12:37:21,248 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.1230, loss: 0.0650 ||: 100%|#########9| 7161/7188 [08:35<00:01, 14.31it/s]
2022-03-21 12:37:21,387 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.1591, loss: 0.0650 ||: 100%|#########9| 7163/7188 [08:35<00:01, 14.33it/s]
2022-03-21 12:37:21,522 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0382, loss: 0.0650 ||: 100%|#########9| 7165/7188 [08:35<00:01, 14.45it/s]
2022-03-21 12:37:21,659 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0172, loss: 0.0650 ||: 100%|#########9| 7167/7188 [08:35<00:01, 14.50it/s]
2022-03-21 12:37:21,800 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.1916, loss: 0.0650 ||: 100%|#########9| 7169/7188 [08:35<00:01, 14.42it/s]
2022-03-21 12:37:21,944 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0460, loss: 0.0650 ||: 100%|#########9| 7171/7188 [08:36<00:01, 14.24it/s]
2022-03-21 12:37:22,100 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0123, loss: 0.0650 ||: 100%|#########9| 7173/7188 [08:36<00:01, 13.77it/s]
2022-03-21 12:37:22,250 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0041, loss: 0.0650 ||: 100%|#########9| 7175/7188 [08:36<00:00, 13.64it/s]
2022-03-21 12:37:22,394 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.2634, loss: 0.0650 ||: 100%|#########9| 7177/7188 [08:36<00:00, 13.72it/s]
2022-03-21 12:37:22,536 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0112, loss: 0.0650 ||: 100%|#########9| 7179/7188 [08:36<00:00, 13.82it/s]
2022-03-21 12:37:22,679 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0064, loss: 0.0650 ||: 100%|#########9| 7181/7188 [08:36<00:00, 13.89it/s]
2022-03-21 12:37:22,811 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0076, loss: 0.0650 ||: 100%|#########9| 7183/7188 [08:36<00:00, 14.24it/s]
2022-03-21 12:37:22,969 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.1636, loss: 0.0650 ||: 100%|#########9| 7185/7188 [08:37<00:00, 13.74it/s]
2022-03-21 12:37:23,116 - INFO - tqdm - f1: 0.9789, accuracy: 0.9788, batch_loss: 0.0226, loss: 0.0651 ||: 100%|#########9| 7187/7188 [08:37<00:00, 13.69it/s]
2022-03-21 12:37:23,239 - INFO - tqdm - f1: 0.9789, accuracy: 0.9789, batch_loss: 0.0046, loss: 0.0650 ||: 100%|##########| 7188/7188 [08:37<00:00, 13.89it/s]
2022-03-21 12:37:23,255 - INFO - allennlp.training.trainer - Validating
2022-03-21 12:37:23,260 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 12:37:29,624 - INFO - tqdm - f1: 0.9343, accuracy: 0.9342, batch_loss: 0.5202, loss: 0.2639 ||: 100%|##########| 313/313 [00:06<00:00, 49.21it/s]
2022-03-21 12:37:29,635 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 12:37:29,637 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.979  |     0.934
2022-03-21 12:37:29,639 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.979  |     0.934
2022-03-21 12:37:29,641 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 12:37:29,644 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.065  |     0.264
2022-03-21 12:37:29,645 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7592.430  |       N/A
2022-03-21 12:37:29,648 - INFO - allennlp.training.trainer - Epoch duration: 0:08:43.893000
2022-03-21 12:37:29,650 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:33:50
2022-03-21 12:37:29,655 - INFO - allennlp.training.trainer - Epoch 7/9
2022-03-21 12:37:29,657 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-21 12:37:29,662 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 12:37:29,666 - INFO - allennlp.training.trainer - Training
2022-03-21 12:37:29,674 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 12:37:39,701 - INFO - tqdm - f1: 0.9874, accuracy: 0.9875, batch_loss: 0.0310, loss: 0.0441 ||:   1%|1         | 105/7188 [00:10<07:43, 15.27it/s]
2022-03-21 12:37:49,745 - INFO - tqdm - f1: 0.9848, accuracy: 0.9849, batch_loss: 0.0023, loss: 0.0488 ||:   4%|3         | 261/7188 [00:20<07:26, 15.52it/s]
2022-03-21 12:37:59,821 - INFO - tqdm - f1: 0.9850, accuracy: 0.9851, batch_loss: 0.0035, loss: 0.0466 ||:   6%|5         | 419/7188 [00:30<07:08, 15.79it/s]
2022-03-21 12:38:09,869 - INFO - tqdm - f1: 0.9839, accuracy: 0.9840, batch_loss: 0.0132, loss: 0.0521 ||:   8%|8         | 577/7188 [00:40<06:26, 17.12it/s]
2022-03-21 12:38:19,971 - INFO - tqdm - f1: 0.9835, accuracy: 0.9835, batch_loss: 0.0258, loss: 0.0516 ||:  10%|#         | 733/7188 [00:50<06:54, 15.59it/s]
2022-03-21 12:38:30,085 - INFO - tqdm - f1: 0.9837, accuracy: 0.9837, batch_loss: 0.0015, loss: 0.0503 ||:  12%|#2        | 891/7188 [01:00<06:58, 15.04it/s]
2022-03-21 12:38:40,110 - INFO - tqdm - f1: 0.9836, accuracy: 0.9837, batch_loss: 0.0382, loss: 0.0496 ||:  15%|#4        | 1045/7188 [01:10<06:31, 15.68it/s]
2022-03-21 12:38:50,204 - INFO - tqdm - f1: 0.9832, accuracy: 0.9833, batch_loss: 0.0076, loss: 0.0519 ||:  17%|#6        | 1203/7188 [01:20<06:31, 15.29it/s]
2022-03-21 12:39:00,277 - INFO - tqdm - f1: 0.9828, accuracy: 0.9829, batch_loss: 0.0666, loss: 0.0520 ||:  19%|#8        | 1359/7188 [01:30<06:11, 15.70it/s]
2022-03-21 12:39:10,347 - INFO - tqdm - f1: 0.9832, accuracy: 0.9833, batch_loss: 0.0419, loss: 0.0513 ||:  21%|##1       | 1515/7188 [01:40<06:13, 15.18it/s]
2022-03-21 12:39:20,360 - INFO - tqdm - f1: 0.9830, accuracy: 0.9831, batch_loss: 0.0019, loss: 0.0514 ||:  23%|##3       | 1671/7188 [01:50<06:02, 15.23it/s]
2022-03-21 12:39:30,411 - INFO - tqdm - f1: 0.9828, accuracy: 0.9829, batch_loss: 0.7202, loss: 0.0525 ||:  25%|##5       | 1827/7188 [02:00<05:44, 15.54it/s]
2022-03-21 12:39:40,490 - INFO - tqdm - f1: 0.9830, accuracy: 0.9830, batch_loss: 0.0028, loss: 0.0524 ||:  28%|##7       | 1979/7188 [02:10<05:53, 14.75it/s]
2022-03-21 12:39:50,534 - INFO - tqdm - f1: 0.9829, accuracy: 0.9829, batch_loss: 0.1608, loss: 0.0522 ||:  30%|##9       | 2133/7188 [02:20<05:29, 15.35it/s]
2022-03-21 12:40:00,545 - INFO - tqdm - f1: 0.9828, accuracy: 0.9829, batch_loss: 0.0113, loss: 0.0526 ||:  32%|###1      | 2287/7188 [02:30<05:21, 15.24it/s]
2022-03-21 12:40:10,642 - INFO - tqdm - f1: 0.9827, accuracy: 0.9827, batch_loss: 0.0075, loss: 0.0532 ||:  34%|###3      | 2441/7188 [02:40<05:09, 15.33it/s]
2022-03-21 12:40:20,644 - INFO - tqdm - f1: 0.9826, accuracy: 0.9826, batch_loss: 0.0197, loss: 0.0534 ||:  36%|###6      | 2595/7188 [02:50<04:42, 16.24it/s]
2022-03-21 12:40:30,760 - INFO - tqdm - f1: 0.9829, accuracy: 0.9830, batch_loss: 0.3511, loss: 0.0525 ||:  38%|###8      | 2751/7188 [03:01<04:54, 15.09it/s]
2022-03-21 12:40:40,850 - INFO - tqdm - f1: 0.9828, accuracy: 0.9829, batch_loss: 0.0533, loss: 0.0527 ||:  40%|####      | 2907/7188 [03:11<04:32, 15.71it/s]
2022-03-21 12:40:50,912 - INFO - tqdm - f1: 0.9832, accuracy: 0.9832, batch_loss: 0.2761, loss: 0.0517 ||:  43%|####2     | 3063/7188 [03:21<04:04, 16.90it/s]
2022-03-21 12:41:00,954 - INFO - tqdm - f1: 0.9831, accuracy: 0.9831, batch_loss: 0.0056, loss: 0.0521 ||:  45%|####4     | 3219/7188 [03:31<04:04, 16.21it/s]
2022-03-21 12:41:10,977 - INFO - tqdm - f1: 0.9830, accuracy: 0.9830, batch_loss: 0.0286, loss: 0.0525 ||:  47%|####6     | 3373/7188 [03:41<04:03, 15.66it/s]
2022-03-21 12:41:20,980 - INFO - tqdm - f1: 0.9828, accuracy: 0.9828, batch_loss: 0.0024, loss: 0.0529 ||:  49%|####9     | 3531/7188 [03:51<03:46, 16.17it/s]
2022-03-21 12:41:31,028 - INFO - tqdm - f1: 0.9827, accuracy: 0.9827, batch_loss: 0.0038, loss: 0.0530 ||:  51%|#####1    | 3687/7188 [04:01<03:44, 15.63it/s]
2022-03-21 12:41:41,045 - INFO - tqdm - f1: 0.9827, accuracy: 0.9827, batch_loss: 0.0028, loss: 0.0529 ||:  53%|#####3    | 3841/7188 [04:11<03:29, 15.99it/s]
2022-03-21 12:41:51,176 - INFO - tqdm - f1: 0.9828, accuracy: 0.9828, batch_loss: 0.0363, loss: 0.0525 ||:  56%|#####5    | 3997/7188 [04:21<03:31, 15.06it/s]
2022-03-21 12:42:01,291 - INFO - tqdm - f1: 0.9827, accuracy: 0.9827, batch_loss: 0.0212, loss: 0.0530 ||:  58%|#####7    | 4153/7188 [04:31<03:12, 15.76it/s]
2022-03-21 12:42:11,356 - INFO - tqdm - f1: 0.9827, accuracy: 0.9827, batch_loss: 0.0300, loss: 0.0534 ||:  60%|#####9    | 4309/7188 [04:41<03:02, 15.81it/s]
2022-03-21 12:42:21,411 - INFO - tqdm - f1: 0.9825, accuracy: 0.9825, batch_loss: 0.0048, loss: 0.0537 ||:  62%|######2   | 4465/7188 [04:51<02:56, 15.47it/s]
2022-03-21 12:42:31,503 - INFO - tqdm - f1: 0.9825, accuracy: 0.9826, batch_loss: 0.0112, loss: 0.0537 ||:  64%|######4   | 4621/7188 [05:01<02:49, 15.12it/s]
2022-03-21 12:42:41,505 - INFO - tqdm - f1: 0.9825, accuracy: 0.9825, batch_loss: 0.1527, loss: 0.0539 ||:  66%|######6   | 4777/7188 [05:11<02:41, 14.94it/s]
2022-03-21 12:42:51,608 - INFO - tqdm - f1: 0.9824, accuracy: 0.9825, batch_loss: 0.0572, loss: 0.0540 ||:  69%|######8   | 4933/7188 [05:21<02:23, 15.72it/s]
2022-03-21 12:43:01,679 - INFO - tqdm - f1: 0.9823, accuracy: 0.9823, batch_loss: 0.0096, loss: 0.0543 ||:  71%|#######   | 5085/7188 [05:32<02:36, 13.43it/s]
2022-03-21 12:43:11,731 - INFO - tqdm - f1: 0.9824, accuracy: 0.9824, batch_loss: 0.0064, loss: 0.0542 ||:  73%|#######2  | 5239/7188 [05:42<02:08, 15.15it/s]
2022-03-21 12:43:22,727 - INFO - tqdm - f1: 0.9824, accuracy: 0.9824, batch_loss: 0.0021, loss: 0.0543 ||:  75%|#######4  | 5365/7188 [05:53<06:42,  4.53it/s]
2022-03-21 12:43:33,336 - INFO - tqdm - f1: 0.9823, accuracy: 0.9823, batch_loss: 0.0091, loss: 0.0547 ||:  76%|#######6  | 5475/7188 [06:03<06:19,  4.51it/s]
2022-03-21 12:43:43,452 - INFO - tqdm - f1: 0.9823, accuracy: 0.9823, batch_loss: 0.0377, loss: 0.0550 ||:  78%|#######7  | 5579/7188 [06:13<06:05,  4.40it/s]
2022-03-21 12:43:53,602 - INFO - tqdm - f1: 0.9823, accuracy: 0.9822, batch_loss: 0.1184, loss: 0.0549 ||:  79%|#######9  | 5683/7188 [06:23<05:37,  4.46it/s]
2022-03-21 12:44:03,620 - INFO - tqdm - f1: 0.9823, accuracy: 0.9823, batch_loss: 0.0012, loss: 0.0549 ||:  80%|########  | 5785/7188 [06:33<04:06,  5.69it/s]
2022-03-21 12:44:13,858 - INFO - tqdm - f1: 0.9823, accuracy: 0.9823, batch_loss: 0.0473, loss: 0.0550 ||:  82%|########1 | 5893/7188 [06:44<04:46,  4.51it/s]
2022-03-21 12:44:24,368 - INFO - tqdm - f1: 0.9822, accuracy: 0.9821, batch_loss: 0.1186, loss: 0.0554 ||:  83%|########3 | 6001/7188 [06:54<04:25,  4.47it/s]
2022-03-21 12:44:35,439 - INFO - tqdm - f1: 0.9821, accuracy: 0.9821, batch_loss: 0.0307, loss: 0.0557 ||:  85%|########5 | 6117/7188 [07:05<04:03,  4.39it/s]
2022-03-21 12:44:45,522 - INFO - tqdm - f1: 0.9820, accuracy: 0.9820, batch_loss: 0.0025, loss: 0.0558 ||:  87%|########6 | 6231/7188 [07:15<01:05, 14.63it/s]
2022-03-21 12:44:55,656 - INFO - tqdm - f1: 0.9821, accuracy: 0.9820, batch_loss: 0.0029, loss: 0.0557 ||:  89%|########8 | 6383/7188 [07:25<00:55, 14.60it/s]
2022-03-21 12:45:05,682 - INFO - tqdm - f1: 0.9821, accuracy: 0.9821, batch_loss: 0.0060, loss: 0.0556 ||:  91%|######### | 6537/7188 [07:36<00:43, 15.06it/s]
2022-03-21 12:45:15,727 - INFO - tqdm - f1: 0.9819, accuracy: 0.9819, batch_loss: 0.0046, loss: 0.0557 ||:  93%|#########3| 6691/7188 [07:46<00:34, 14.21it/s]
2022-03-21 12:45:25,824 - INFO - tqdm - f1: 0.9818, accuracy: 0.9818, batch_loss: 0.0149, loss: 0.0562 ||:  95%|#########5| 6843/7188 [07:56<00:23, 14.95it/s]
2022-03-21 12:45:35,942 - INFO - tqdm - f1: 0.9817, accuracy: 0.9817, batch_loss: 0.4896, loss: 0.0567 ||:  97%|#########7| 6997/7188 [08:06<00:12, 15.36it/s]
2022-03-21 12:45:45,946 - INFO - tqdm - f1: 0.9817, accuracy: 0.9817, batch_loss: 0.4579, loss: 0.0568 ||:  99%|#########8| 7113/7188 [08:16<00:13,  5.76it/s]
2022-03-21 12:45:49,671 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0335, loss: 0.0569 ||: 100%|#########9| 7153/7188 [08:19<00:03,  9.39it/s]
2022-03-21 12:45:49,799 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0180, loss: 0.0569 ||: 100%|#########9| 7155/7188 [08:20<00:03, 10.68it/s]
2022-03-21 12:45:49,923 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0203, loss: 0.0569 ||: 100%|#########9| 7157/7188 [08:20<00:02, 11.88it/s]
2022-03-21 12:45:50,065 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0021, loss: 0.0569 ||: 100%|#########9| 7159/7188 [08:20<00:02, 12.46it/s]
2022-03-21 12:45:50,214 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0045, loss: 0.0569 ||: 100%|#########9| 7161/7188 [08:20<00:02, 12.74it/s]
2022-03-21 12:45:50,358 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0056, loss: 0.0570 ||: 100%|#########9| 7163/7188 [08:20<00:01, 13.06it/s]
2022-03-21 12:45:50,513 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0059, loss: 0.0569 ||: 100%|#########9| 7165/7188 [08:20<00:01, 13.03it/s]
2022-03-21 12:45:50,662 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.1847, loss: 0.0570 ||: 100%|#########9| 7167/7188 [08:20<00:01, 13.12it/s]
2022-03-21 12:45:50,816 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0403, loss: 0.0570 ||: 100%|#########9| 7169/7188 [08:21<00:01, 13.10it/s]
2022-03-21 12:45:50,957 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.2092, loss: 0.0570 ||: 100%|#########9| 7171/7188 [08:21<00:01, 13.39it/s]
2022-03-21 12:45:51,100 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0027, loss: 0.0570 ||: 100%|#########9| 7173/7188 [08:21<00:01, 13.56it/s]
2022-03-21 12:45:51,245 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0145, loss: 0.0570 ||: 100%|#########9| 7175/7188 [08:21<00:00, 13.64it/s]
2022-03-21 12:45:51,376 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0027, loss: 0.0570 ||: 100%|#########9| 7177/7188 [08:21<00:00, 14.08it/s]
2022-03-21 12:45:52,545 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0125, loss: 0.0569 ||: 100%|#########9| 7179/7188 [08:22<00:02,  4.44it/s]
2022-03-21 12:45:52,695 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0531, loss: 0.0569 ||: 100%|#########9| 7181/7188 [08:23<00:01,  5.56it/s]
2022-03-21 12:45:52,854 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0643, loss: 0.0570 ||: 100%|#########9| 7183/7188 [08:23<00:00,  6.67it/s]
2022-03-21 12:45:52,998 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0528, loss: 0.0570 ||: 100%|#########9| 7185/7188 [08:23<00:00,  7.90it/s]
2022-03-21 12:45:53,136 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.0030, loss: 0.0570 ||: 100%|#########9| 7187/7188 [08:23<00:00,  9.16it/s]
2022-03-21 12:45:53,249 - INFO - tqdm - f1: 0.9816, accuracy: 0.9816, batch_loss: 0.2037, loss: 0.0570 ||: 100%|##########| 7188/7188 [08:23<00:00, 14.27it/s]
2022-03-21 12:45:53,288 - INFO - allennlp.training.trainer - Validating
2022-03-21 12:45:53,306 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 12:46:03,399 - INFO - tqdm - f1: 0.9285, accuracy: 0.9288, batch_loss: 0.0127, loss: 0.2619 ||:  96%|#########5| 299/313 [00:10<00:01, 12.20it/s]
2022-03-21 12:46:03,715 - INFO - tqdm - f1: 0.9285, accuracy: 0.9288, batch_loss: 0.8332, loss: 0.2660 ||: 100%|#########9| 312/313 [00:10<00:00, 21.80it/s]
2022-03-21 12:46:03,745 - INFO - tqdm - f1: 0.9285, accuracy: 0.9288, batch_loss: 0.0969, loss: 0.2654 ||: 100%|##########| 313/313 [00:10<00:00, 30.02it/s]
2022-03-21 12:46:03,752 - INFO - allennlp.training.trainer - Ran out of patience.  Stopping training.
2022-03-21 12:46:03,754 - INFO - dont_stop_pretraining.training.roberta_checkpointer - loading best weights
2022-03-21 12:46:05,220 - INFO - allennlp.commands.train - The model will be evaluated using the best epoch weights.
2022-03-21 12:46:05,226 - INFO - allennlp.training.util - Iterating over dataset
2022-03-21 12:46:05,233 - INFO - tqdm - 0it [00:00, ?it/s]
2022-03-21 12:46:05,254 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-21 12:46:05,263 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-21 12:46:15,273 - INFO - tqdm - f1: 0.94, accuracy: 0.94, loss: 0.23 ||: : 307it [00:10, 43.29it/s]
2022-03-21 12:46:21,291 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 4,
  "peak_worker_0_memory_MB": 7592.4296875,
  "peak_gpu_0_memory_MB": 0,
  "training_duration": "1:18:56.902130",
  "training_start_epoch": 0,
  "training_epochs": 6,
  "epoch": 6,
  "training_f1": 0.9788604974746704,
  "training_accuracy": 0.9788521739130435,
  "training_loss": 0.06504243425990093,
  "training_worker_0_memory_MB": 7592.4296875,
  "training_gpu_0_memory_MB": 0.0,
  "validation_f1": 0.9342628717422485,
  "validation_accuracy": 0.9342,
  "validation_loss": 0.26388549441865944,
  "best_validation_f1": 0.9426444321870804,
  "best_validation_accuracy": 0.9426,
  "best_validation_loss": 0.22635939423911297,
  "test_f1": 0.9411174654960632,
  "test_accuracy": 0.9411842105263157,
  "test_loss": 0.2234804891777764
}
2022-03-21 12:46:21,369 - INFO - allennlp.models.archival - archiving weights and vocabulary to model_logs/ag_base_hyper_small_seed_97/model.tar.gz
