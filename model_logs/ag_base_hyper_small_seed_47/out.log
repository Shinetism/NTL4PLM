2022-03-21 10:06:32,455 - INFO - allennlp.common.params - random_seed = 47
2022-03-21 10:06:32,478 - INFO - allennlp.common.params - numpy_seed = 47
2022-03-21 10:06:32,498 - INFO - allennlp.common.params - pytorch_seed = 47
2022-03-21 10:06:32,526 - INFO - allennlp.common.checks - Pytorch version: 1.8.0+cu111
2022-03-21 10:06:32,540 - INFO - allennlp.common.params - type = default
2022-03-21 10:06:32,562 - INFO - allennlp.common.params - dataset_reader.type = text_classification_json_with_sampling
2022-03-21 10:06:32,582 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer
2022-03-21 10:06:32,603 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2022-03-21 10:06:32,623 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = roberta-base
2022-03-21 10:06:32,643 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags
2022-03-21 10:06:32,663 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512
2022-03-21 10:06:32,681 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None
2022-03-21 10:06:46,603 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer
2022-03-21 10:06:46,618 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = roberta-base
2022-03-21 10:06:46,628 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True
2022-03-21 10:06:46,643 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = 512
2022-03-21 10:06:46,658 - INFO - allennlp.common.params - dataset_reader.tokenizer.stride = 0
2022-03-21 10:06:46,673 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None
2022-03-21 10:06:46,688 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = 512
2022-03-21 10:06:46,702 - INFO - allennlp.common.params - dataset_reader.sample = None
2022-03-21 10:06:46,717 - INFO - allennlp.common.params - dataset_reader.skip_label_indexing = False
2022-03-21 10:06:46,732 - INFO - allennlp.common.params - train_data_path = datasets/ag/train.jsonl
2022-03-21 10:06:46,747 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f47abbae150>
2022-03-21 10:06:46,761 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-03-21 10:06:46,776 - INFO - allennlp.common.params - validation_dataset_reader.type = text_classification_json_with_sampling
2022-03-21 10:06:46,791 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer
2022-03-21 10:06:46,806 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2022-03-21 10:06:46,820 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = roberta-base
2022-03-21 10:06:46,835 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags
2022-03-21 10:06:46,851 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512
2022-03-21 10:06:46,865 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None
2022-03-21 10:06:46,880 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.type = pretrained_transformer
2022-03-21 10:06:46,895 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.model_name = roberta-base
2022-03-21 10:06:46,912 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.add_special_tokens = True
2022-03-21 10:06:46,927 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.max_length = 512
2022-03-21 10:06:46,942 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.stride = 0
2022-03-21 10:06:46,957 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.tokenizer_kwargs = None
2022-03-21 10:06:46,972 - INFO - allennlp.common.params - validation_dataset_reader.max_sequence_length = 512
2022-03-21 10:06:46,987 - INFO - allennlp.common.params - validation_dataset_reader.sample = None
2022-03-21 10:06:47,001 - INFO - allennlp.common.params - validation_dataset_reader.skip_label_indexing = False
2022-03-21 10:06:47,016 - INFO - allennlp.common.params - validation_data_path = datasets/ag/dev.jsonl
2022-03-21 10:06:47,031 - INFO - allennlp.common.params - validation_data_loader = None
2022-03-21 10:06:47,045 - INFO - allennlp.common.params - test_data_path = datasets/ag/test.jsonl
2022-03-21 10:06:47,060 - INFO - allennlp.common.params - evaluate_on_test = True
2022-03-21 10:06:47,075 - INFO - allennlp.common.params - batch_weight_key = 
2022-03-21 10:06:47,090 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-21 10:06:47,105 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-21 10:06:47,123 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-21 10:06:47,134 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-21 10:06:47,149 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-21 10:06:47,164 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-21 10:06:47,178 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-21 10:06:47,193 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-21 10:06:47,207 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-21 10:06:47,222 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-21 10:06:47,237 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-21 10:06:47,251 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-21 10:06:47,267 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-21 10:06:47,282 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-21 10:06:47,304 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-21 10:06:57,372 - INFO - tqdm - loading instances: 31412it [00:10, 3683.42it/s]
2022-03-21 10:07:07,418 - INFO - tqdm - loading instances: 62976it [00:20, 3770.24it/s]
2022-03-21 10:07:17,427 - INFO - tqdm - loading instances: 93642it [00:30, 3504.16it/s]
2022-03-21 10:07:24,432 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-21 10:07:24,439 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-21 10:07:24,450 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-21 10:07:24,452 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-21 10:07:24,453 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-21 10:07:24,455 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-21 10:07:24,457 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-21 10:07:24,458 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-21 10:07:24,459 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-21 10:07:24,461 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-21 10:07:24,462 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-21 10:07:24,463 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-21 10:07:24,464 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-21 10:07:24,483 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-21 10:07:24,498 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-21 10:07:25,848 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-21 10:07:25,861 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-21 10:07:25,871 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-21 10:07:25,886 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-21 10:07:25,902 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-21 10:07:25,904 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-21 10:07:25,921 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-21 10:07:25,937 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-21 10:07:25,952 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-21 10:07:25,953 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-21 10:07:25,955 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-21 10:07:25,956 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-21 10:07:25,957 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-21 10:07:25,959 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-21 10:07:25,960 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-21 10:07:27,995 - INFO - allennlp.common.params - type = from_instances
2022-03-21 10:07:28,013 - INFO - allennlp.common.params - min_count = None
2022-03-21 10:07:28,024 - INFO - allennlp.common.params - max_vocab_size = None
2022-03-21 10:07:28,039 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-03-21 10:07:28,054 - INFO - allennlp.common.params - pretrained_files = None
2022-03-21 10:07:28,068 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-03-21 10:07:28,083 - INFO - allennlp.common.params - tokens_to_add = None
2022-03-21 10:07:28,098 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-03-21 10:07:28,113 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-03-21 10:07:28,128 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-03-21 10:07:28,143 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-03-21 10:07:28,159 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-03-21 10:07:28,826 - INFO - allennlp.common.params - model.type = basic_classifier_with_f1
2022-03-21 10:07:28,837 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2022-03-21 10:07:28,848 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer
2022-03-21 10:07:28,863 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = roberta-base
2022-03-21 10:07:28,878 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512
2022-03-21 10:07:28,893 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_module = None
2022-03-21 10:07:28,908 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True
2022-03-21 10:07:28,923 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True
2022-03-21 10:07:28,938 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None
2022-03-21 10:07:28,953 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None
2022-03-21 10:07:28,968 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None
2022-03-21 10:07:28,983 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None
2022-03-21 10:07:28,998 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None
2022-03-21 10:07:35,389 - INFO - allennlp.common.params - model.seq2vec_encoder.type = cls_pooler
2022-03-21 10:07:35,407 - INFO - allennlp.common.params - model.seq2vec_encoder.embedding_dim = 768
2022-03-21 10:07:35,417 - INFO - allennlp.common.params - model.seq2vec_encoder.cls_is_last_token = False
2022-03-21 10:07:35,432 - INFO - allennlp.common.params - model.feedforward_layer.input_dim = 768
2022-03-21 10:07:35,447 - INFO - allennlp.common.params - model.feedforward_layer.num_layers = 1
2022-03-21 10:07:35,461 - INFO - allennlp.common.params - model.feedforward_layer.hidden_dims = 768
2022-03-21 10:07:35,476 - INFO - allennlp.common.params - model.feedforward_layer.activations = tanh
2022-03-21 10:07:35,491 - INFO - allennlp.common.params - type = tanh
2022-03-21 10:07:35,506 - INFO - allennlp.common.params - model.feedforward_layer.dropout = 0.0
2022-03-21 10:07:35,524 - INFO - allennlp.common.params - model.seq2seq_encoder = None
2022-03-21 10:07:35,535 - INFO - allennlp.common.params - model.dropout = 0.1
2022-03-21 10:07:35,550 - INFO - allennlp.common.params - model.num_labels = None
2022-03-21 10:07:35,564 - INFO - allennlp.common.params - model.label_namespace = labels
2022-03-21 10:07:35,579 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f491dc03190>
2022-03-21 10:07:35,594 - INFO - allennlp.common.params - model.regularizer = None
2022-03-21 10:07:35,595 - INFO - allennlp.common.params - model.track_weights = False
2022-03-21 10:07:35,596 - INFO - allennlp.common.params - model.disable_layers = []
2022-03-21 10:07:35,598 - INFO - allennlp.nn.initializers - Initializing parameters
2022-03-21 10:07:35,601 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-03-21 10:07:35,602 - INFO - allennlp.nn.initializers -    _classification_layer.bias
2022-03-21 10:07:35,619 - INFO - allennlp.nn.initializers -    _classification_layer.weight
2022-03-21 10:07:35,634 - INFO - allennlp.nn.initializers -    _feedforward_layer._linear_layers.0.bias
2022-03-21 10:07:35,649 - INFO - allennlp.nn.initializers -    _feedforward_layer._linear_layers.0.weight
2022-03-21 10:07:35,664 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias
2022-03-21 10:07:35,678 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight
2022-03-21 10:07:35,693 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight
2022-03-21 10:07:35,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight
2022-03-21 10:07:35,723 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight
2022-03-21 10:07:35,739 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2022-03-21 10:07:35,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2022-03-21 10:07:35,767 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias
2022-03-21 10:07:35,783 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight
2022-03-21 10:07:35,785 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias
2022-03-21 10:07:35,786 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight
2022-03-21 10:07:35,788 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias
2022-03-21 10:07:35,789 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight
2022-03-21 10:07:35,808 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias
2022-03-21 10:07:35,823 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight
2022-03-21 10:07:35,838 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias
2022-03-21 10:07:35,852 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight
2022-03-21 10:07:35,867 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias
2022-03-21 10:07:35,882 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight
2022-03-21 10:07:35,897 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias
2022-03-21 10:07:35,911 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight
2022-03-21 10:07:35,926 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2022-03-21 10:07:35,928 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2022-03-21 10:07:35,930 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias
2022-03-21 10:07:35,931 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight
2022-03-21 10:07:35,933 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias
2022-03-21 10:07:35,934 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight
2022-03-21 10:07:35,952 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias
2022-03-21 10:07:35,966 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight
2022-03-21 10:07:35,981 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias
2022-03-21 10:07:35,996 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight
2022-03-21 10:07:36,011 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias
2022-03-21 10:07:36,026 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight
2022-03-21 10:07:36,041 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias
2022-03-21 10:07:36,056 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight
2022-03-21 10:07:36,071 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias
2022-03-21 10:07:36,085 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight
2022-03-21 10:07:36,100 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2022-03-21 10:07:36,116 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2022-03-21 10:07:36,130 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias
2022-03-21 10:07:36,145 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight
2022-03-21 10:07:36,159 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias
2022-03-21 10:07:36,174 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight
2022-03-21 10:07:36,176 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias
2022-03-21 10:07:36,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight
2022-03-21 10:07:36,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias
2022-03-21 10:07:36,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight
2022-03-21 10:07:36,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias
2022-03-21 10:07:36,200 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight
2022-03-21 10:07:36,215 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias
2022-03-21 10:07:36,230 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight
2022-03-21 10:07:36,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias
2022-03-21 10:07:36,259 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight
2022-03-21 10:07:36,275 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2022-03-21 10:07:36,289 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2022-03-21 10:07:36,304 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias
2022-03-21 10:07:36,319 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight
2022-03-21 10:07:36,335 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias
2022-03-21 10:07:36,353 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight
2022-03-21 10:07:36,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias
2022-03-21 10:07:36,379 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight
2022-03-21 10:07:36,394 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias
2022-03-21 10:07:36,409 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight
2022-03-21 10:07:36,425 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias
2022-03-21 10:07:36,426 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight
2022-03-21 10:07:36,428 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias
2022-03-21 10:07:36,429 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight
2022-03-21 10:07:36,431 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias
2022-03-21 10:07:36,432 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight
2022-03-21 10:07:36,451 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2022-03-21 10:07:36,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2022-03-21 10:07:36,481 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias
2022-03-21 10:07:36,496 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight
2022-03-21 10:07:36,511 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias
2022-03-21 10:07:36,512 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight
2022-03-21 10:07:36,528 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias
2022-03-21 10:07:36,530 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight
2022-03-21 10:07:36,531 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias
2022-03-21 10:07:36,548 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight
2022-03-21 10:07:36,562 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias
2022-03-21 10:07:36,577 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight
2022-03-21 10:07:36,592 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias
2022-03-21 10:07:36,607 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight
2022-03-21 10:07:36,621 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias
2022-03-21 10:07:36,637 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight
2022-03-21 10:07:36,652 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2022-03-21 10:07:36,672 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2022-03-21 10:07:36,674 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias
2022-03-21 10:07:36,676 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight
2022-03-21 10:07:36,693 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias
2022-03-21 10:07:36,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight
2022-03-21 10:07:36,723 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias
2022-03-21 10:07:36,738 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight
2022-03-21 10:07:36,739 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias
2022-03-21 10:07:36,741 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight
2022-03-21 10:07:36,742 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias
2022-03-21 10:07:36,744 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight
2022-03-21 10:07:36,745 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias
2022-03-21 10:07:36,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight
2022-03-21 10:07:36,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias
2022-03-21 10:07:36,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight
2022-03-21 10:07:36,766 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2022-03-21 10:07:36,781 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2022-03-21 10:07:36,795 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias
2022-03-21 10:07:36,810 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight
2022-03-21 10:07:36,826 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias
2022-03-21 10:07:36,839 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight
2022-03-21 10:07:36,854 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias
2022-03-21 10:07:36,869 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight
2022-03-21 10:07:36,885 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias
2022-03-21 10:07:36,899 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight
2022-03-21 10:07:36,914 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias
2022-03-21 10:07:36,930 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight
2022-03-21 10:07:36,944 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias
2022-03-21 10:07:36,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight
2022-03-21 10:07:36,974 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias
2022-03-21 10:07:36,989 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight
2022-03-21 10:07:37,004 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2022-03-21 10:07:37,018 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2022-03-21 10:07:37,033 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias
2022-03-21 10:07:37,035 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight
2022-03-21 10:07:37,036 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias
2022-03-21 10:07:37,038 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight
2022-03-21 10:07:37,053 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias
2022-03-21 10:07:37,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight
2022-03-21 10:07:37,082 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias
2022-03-21 10:07:37,097 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight
2022-03-21 10:07:37,117 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias
2022-03-21 10:07:37,127 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight
2022-03-21 10:07:37,142 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias
2022-03-21 10:07:37,156 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight
2022-03-21 10:07:37,172 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias
2022-03-21 10:07:37,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight
2022-03-21 10:07:37,201 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2022-03-21 10:07:37,216 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2022-03-21 10:07:37,231 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias
2022-03-21 10:07:37,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight
2022-03-21 10:07:37,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias
2022-03-21 10:07:37,276 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight
2022-03-21 10:07:37,291 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias
2022-03-21 10:07:37,306 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight
2022-03-21 10:07:37,321 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias
2022-03-21 10:07:37,322 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight
2022-03-21 10:07:37,323 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias
2022-03-21 10:07:37,324 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight
2022-03-21 10:07:37,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias
2022-03-21 10:07:37,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight
2022-03-21 10:07:37,345 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias
2022-03-21 10:07:37,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight
2022-03-21 10:07:37,375 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2022-03-21 10:07:37,390 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2022-03-21 10:07:37,405 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias
2022-03-21 10:07:37,406 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight
2022-03-21 10:07:37,425 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias
2022-03-21 10:07:37,440 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight
2022-03-21 10:07:37,442 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias
2022-03-21 10:07:37,443 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight
2022-03-21 10:07:37,444 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias
2022-03-21 10:07:37,460 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight
2022-03-21 10:07:37,475 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias
2022-03-21 10:07:37,490 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight
2022-03-21 10:07:37,505 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias
2022-03-21 10:07:37,521 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight
2022-03-21 10:07:37,537 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias
2022-03-21 10:07:37,550 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight
2022-03-21 10:07:37,565 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2022-03-21 10:07:37,580 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2022-03-21 10:07:37,581 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias
2022-03-21 10:07:37,583 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight
2022-03-21 10:07:37,584 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias
2022-03-21 10:07:37,585 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight
2022-03-21 10:07:37,587 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias
2022-03-21 10:07:37,605 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight
2022-03-21 10:07:37,620 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias
2022-03-21 10:07:37,635 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight
2022-03-21 10:07:37,636 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias
2022-03-21 10:07:37,653 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight
2022-03-21 10:07:37,668 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias
2022-03-21 10:07:37,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight
2022-03-21 10:07:37,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias
2022-03-21 10:07:37,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight
2022-03-21 10:07:37,687 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2022-03-21 10:07:37,688 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2022-03-21 10:07:37,689 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias
2022-03-21 10:07:37,691 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight
2022-03-21 10:07:37,692 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias
2022-03-21 10:07:37,694 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight
2022-03-21 10:07:37,696 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias
2022-03-21 10:07:37,697 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight
2022-03-21 10:07:37,699 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias
2022-03-21 10:07:37,700 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight
2022-03-21 10:07:37,718 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias
2022-03-21 10:07:37,733 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight
2022-03-21 10:07:37,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias
2022-03-21 10:07:37,763 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight
2022-03-21 10:07:37,778 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias
2022-03-21 10:07:37,793 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight
2022-03-21 10:07:37,807 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias
2022-03-21 10:07:37,809 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight
2022-03-21 10:07:43,763 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-03-21 10:07:43,770 - INFO - allennlp.common.params - trainer.patience = 3
2022-03-21 10:07:43,771 - INFO - allennlp.common.params - trainer.validation_metric = +f1
2022-03-21 10:07:43,797 - INFO - allennlp.common.params - trainer.num_epochs = 10
2022-03-21 10:07:43,810 - INFO - allennlp.common.params - trainer.cuda_device = 2
2022-03-21 10:07:43,828 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-03-21 10:07:43,847 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-03-21 10:07:43,862 - INFO - allennlp.common.params - trainer.distributed = False
2022-03-21 10:07:43,879 - INFO - allennlp.common.params - trainer.world_size = 1
2022-03-21 10:07:43,896 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1
2022-03-21 10:07:43,913 - INFO - allennlp.common.params - trainer.use_amp = False
2022-03-21 10:07:43,931 - INFO - allennlp.common.params - trainer.no_grad = None
2022-03-21 10:07:43,932 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2022-03-21 10:07:43,934 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-03-21 10:07:43,935 - INFO - allennlp.common.params - trainer.moving_average = None
2022-03-21 10:07:43,956 - INFO - allennlp.common.params - trainer.callbacks = None
2022-03-21 10:07:43,973 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True
2022-03-21 10:07:56,010 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw_str_lr
2022-03-21 10:07:56,031 - INFO - allennlp.common.params - trainer.optimizer.lr = 2e-05
2022-03-21 10:07:56,041 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-03-21 10:07:56,056 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-03-21 10:07:56,071 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-03-21 10:07:56,087 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = False
2022-03-21 10:07:56,104 - INFO - allennlp.training.optimizers - Done constructing parameter groups.
2022-03-21 10:07:56,117 - INFO - allennlp.training.optimizers - Group 0: ['_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias', '_feedforward_layer._linear_layers.0.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias', '_classification_layer.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias'], {'weight_decay': 0}
2022-03-21 10:07:56,133 - INFO - allennlp.training.optimizers - Group 1: ['_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight', '_feedforward_layer._linear_layers.0.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight', '_classification_layer.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight'], {}
2022-03-21 10:07:56,148 - WARNING - allennlp.training.optimizers - When constructing parameter groups, layer_norm.weight does not match any parameter name
2022-03-21 10:07:56,162 - INFO - allennlp.training.optimizers - Number of trainable parameters: 125239300
2022-03-21 10:07:56,177 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-03-21 10:07:56,193 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-03-21 10:07:56,207 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight
2022-03-21 10:07:56,222 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight
2022-03-21 10:07:56,237 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight
2022-03-21 10:07:56,252 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight
2022-03-21 10:07:56,267 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias
2022-03-21 10:07:56,283 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight
2022-03-21 10:07:56,298 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias
2022-03-21 10:07:56,314 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight
2022-03-21 10:07:56,329 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias
2022-03-21 10:07:56,344 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight
2022-03-21 10:07:56,359 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias
2022-03-21 10:07:56,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight
2022-03-21 10:07:56,389 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias
2022-03-21 10:07:56,404 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2022-03-21 10:07:56,419 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2022-03-21 10:07:56,434 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight
2022-03-21 10:07:56,450 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias
2022-03-21 10:07:56,469 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight
2022-03-21 10:07:56,484 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias
2022-03-21 10:07:56,499 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight
2022-03-21 10:07:56,513 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias
2022-03-21 10:07:56,528 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight
2022-03-21 10:07:56,543 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias
2022-03-21 10:07:56,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight
2022-03-21 10:07:56,574 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias
2022-03-21 10:07:56,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight
2022-03-21 10:07:56,604 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias
2022-03-21 10:07:56,619 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight
2022-03-21 10:07:56,635 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias
2022-03-21 10:07:56,650 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2022-03-21 10:07:56,665 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2022-03-21 10:07:56,680 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight
2022-03-21 10:07:56,695 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias
2022-03-21 10:07:56,710 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight
2022-03-21 10:07:56,725 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias
2022-03-21 10:07:56,740 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight
2022-03-21 10:07:56,755 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias
2022-03-21 10:07:56,770 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight
2022-03-21 10:07:56,785 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias
2022-03-21 10:07:56,800 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight
2022-03-21 10:07:56,816 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias
2022-03-21 10:07:56,831 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight
2022-03-21 10:07:56,847 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias
2022-03-21 10:07:56,849 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight
2022-03-21 10:07:56,850 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias
2022-03-21 10:07:56,852 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2022-03-21 10:07:56,853 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2022-03-21 10:07:56,854 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight
2022-03-21 10:07:56,856 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias
2022-03-21 10:07:56,857 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight
2022-03-21 10:07:56,859 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias
2022-03-21 10:07:56,860 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight
2022-03-21 10:07:56,861 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias
2022-03-21 10:07:56,880 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight
2022-03-21 10:07:56,895 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias
2022-03-21 10:07:56,910 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight
2022-03-21 10:07:56,925 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias
2022-03-21 10:07:56,940 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight
2022-03-21 10:07:56,959 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias
2022-03-21 10:07:56,971 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight
2022-03-21 10:07:56,986 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias
2022-03-21 10:07:57,001 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2022-03-21 10:07:57,016 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2022-03-21 10:07:57,031 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight
2022-03-21 10:07:57,046 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias
2022-03-21 10:07:57,061 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight
2022-03-21 10:07:57,076 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias
2022-03-21 10:07:57,091 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight
2022-03-21 10:07:57,107 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias
2022-03-21 10:07:57,121 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight
2022-03-21 10:07:57,137 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias
2022-03-21 10:07:57,151 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight
2022-03-21 10:07:57,166 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias
2022-03-21 10:07:57,181 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight
2022-03-21 10:07:57,196 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias
2022-03-21 10:07:57,211 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight
2022-03-21 10:07:57,229 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias
2022-03-21 10:07:57,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2022-03-21 10:07:57,257 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2022-03-21 10:07:57,272 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight
2022-03-21 10:07:57,287 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias
2022-03-21 10:07:57,302 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight
2022-03-21 10:07:57,317 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias
2022-03-21 10:07:57,332 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight
2022-03-21 10:07:57,347 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias
2022-03-21 10:07:57,363 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight
2022-03-21 10:07:57,377 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias
2022-03-21 10:07:57,392 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight
2022-03-21 10:07:57,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias
2022-03-21 10:07:57,423 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight
2022-03-21 10:07:57,438 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias
2022-03-21 10:07:57,453 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight
2022-03-21 10:07:57,468 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias
2022-03-21 10:07:57,483 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2022-03-21 10:07:57,485 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2022-03-21 10:07:57,486 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight
2022-03-21 10:07:57,487 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias
2022-03-21 10:07:57,489 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight
2022-03-21 10:07:57,490 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias
2022-03-21 10:07:57,492 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight
2022-03-21 10:07:57,510 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias
2022-03-21 10:07:57,525 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight
2022-03-21 10:07:57,540 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias
2022-03-21 10:07:57,555 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight
2022-03-21 10:07:57,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias
2022-03-21 10:07:57,585 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight
2022-03-21 10:07:57,600 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias
2022-03-21 10:07:57,615 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight
2022-03-21 10:07:57,630 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias
2022-03-21 10:07:57,645 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2022-03-21 10:07:57,660 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2022-03-21 10:07:57,675 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight
2022-03-21 10:07:57,690 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias
2022-03-21 10:07:57,705 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight
2022-03-21 10:07:57,720 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias
2022-03-21 10:07:57,736 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight
2022-03-21 10:07:57,751 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias
2022-03-21 10:07:57,765 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight
2022-03-21 10:07:57,781 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias
2022-03-21 10:07:57,796 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight
2022-03-21 10:07:57,812 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias
2022-03-21 10:07:57,826 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight
2022-03-21 10:07:57,841 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias
2022-03-21 10:07:57,856 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight
2022-03-21 10:07:57,871 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias
2022-03-21 10:07:57,887 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2022-03-21 10:07:57,903 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2022-03-21 10:07:57,918 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight
2022-03-21 10:07:57,933 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias
2022-03-21 10:07:57,948 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight
2022-03-21 10:07:57,949 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias
2022-03-21 10:07:57,951 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight
2022-03-21 10:07:57,952 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias
2022-03-21 10:07:57,954 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight
2022-03-21 10:07:57,969 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias
2022-03-21 10:07:57,970 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight
2022-03-21 10:07:57,986 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias
2022-03-21 10:07:57,987 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight
2022-03-21 10:07:57,988 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias
2022-03-21 10:07:57,990 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight
2022-03-21 10:07:58,005 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias
2022-03-21 10:07:58,020 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2022-03-21 10:07:58,035 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2022-03-21 10:07:58,051 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight
2022-03-21 10:07:58,066 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias
2022-03-21 10:07:58,082 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight
2022-03-21 10:07:58,096 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias
2022-03-21 10:07:58,111 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight
2022-03-21 10:07:58,126 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias
2022-03-21 10:07:58,141 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight
2022-03-21 10:07:58,157 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias
2022-03-21 10:07:58,172 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight
2022-03-21 10:07:58,187 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias
2022-03-21 10:07:58,202 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight
2022-03-21 10:07:58,217 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias
2022-03-21 10:07:58,232 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight
2022-03-21 10:07:58,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias
2022-03-21 10:07:58,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2022-03-21 10:07:58,277 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2022-03-21 10:07:58,293 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight
2022-03-21 10:07:58,308 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias
2022-03-21 10:07:58,323 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight
2022-03-21 10:07:58,325 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias
2022-03-21 10:07:58,326 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight
2022-03-21 10:07:58,327 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias
2022-03-21 10:07:58,329 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight
2022-03-21 10:07:58,331 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias
2022-03-21 10:07:58,332 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight
2022-03-21 10:07:58,334 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias
2022-03-21 10:07:58,350 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight
2022-03-21 10:07:58,365 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias
2022-03-21 10:07:58,380 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight
2022-03-21 10:07:58,395 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias
2022-03-21 10:07:58,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2022-03-21 10:07:58,425 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2022-03-21 10:07:58,440 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight
2022-03-21 10:07:58,457 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias
2022-03-21 10:07:58,470 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight
2022-03-21 10:07:58,485 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias
2022-03-21 10:07:58,500 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight
2022-03-21 10:07:58,515 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias
2022-03-21 10:07:58,530 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight
2022-03-21 10:07:58,545 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias
2022-03-21 10:07:58,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight
2022-03-21 10:07:58,575 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias
2022-03-21 10:07:58,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight
2022-03-21 10:07:58,605 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias
2022-03-21 10:07:58,621 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight
2022-03-21 10:07:58,636 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias
2022-03-21 10:07:58,651 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2022-03-21 10:07:58,666 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2022-03-21 10:07:58,668 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight
2022-03-21 10:07:58,670 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias
2022-03-21 10:07:58,671 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight
2022-03-21 10:07:58,672 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias
2022-03-21 10:07:58,674 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight
2022-03-21 10:07:58,675 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias
2022-03-21 10:07:58,677 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight
2022-03-21 10:07:58,696 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias
2022-03-21 10:07:58,711 - INFO - allennlp.common.util - _feedforward_layer._linear_layers.0.weight
2022-03-21 10:07:58,726 - INFO - allennlp.common.util - _feedforward_layer._linear_layers.0.bias
2022-03-21 10:07:58,741 - INFO - allennlp.common.util - _classification_layer.weight
2022-03-21 10:07:58,756 - INFO - allennlp.common.util - _classification_layer.bias
2022-03-21 10:07:58,772 - INFO - allennlp.common.params - trainer.checkpointer.type = roberta_default
2022-03-21 10:07:58,774 - INFO - allennlp.common.params - trainer.checkpointer.keep_serialized_model_every_num_seconds = None
2022-03-21 10:07:58,775 - INFO - allennlp.common.params - trainer.checkpointer.num_serialized_models_to_keep = 0
2022-03-21 10:07:58,793 - INFO - allennlp.common.params - trainer.checkpointer.model_save_interval = None
2022-03-21 10:07:58,808 - INFO - allennlp.common.params - trainer.checkpointer.num_epochs = 10
2022-03-21 10:07:58,824 - INFO - allennlp.common.params - trainer.checkpointer.skip_early_stopping = False
2022-03-21 10:07:58,853 - INFO - allennlp.training.trainer - Beginning training.
2022-03-21 10:07:58,869 - INFO - allennlp.training.trainer - Epoch 0/9
2022-03-21 10:07:58,884 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.2G
2022-03-21 10:07:58,886 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 10:07:58,888 - INFO - allennlp.training.trainer - Training
2022-03-21 10:07:58,890 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 10:07:58,991 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-21 10:07:59,008 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-21 10:08:08,896 - INFO - tqdm - f1: 0.7771, accuracy: 0.7717, batch_loss: 0.0612, loss: 0.6015 ||:   1%|1         | 98/7188 [00:10<08:59, 13.15it/s]
2022-03-21 10:08:19,003 - INFO - tqdm - f1: 0.8449, accuracy: 0.8443, batch_loss: 0.2427, loss: 0.4314 ||:   3%|3         | 238/7188 [00:20<08:32, 13.55it/s]
2022-03-21 10:08:29,082 - INFO - tqdm - f1: 0.8609, accuracy: 0.8613, batch_loss: 0.2515, loss: 0.3903 ||:   5%|5         | 382/7188 [00:30<08:06, 13.99it/s]
2022-03-21 10:08:39,200 - INFO - tqdm - f1: 0.8675, accuracy: 0.8681, batch_loss: 0.3489, loss: 0.3727 ||:   7%|7         | 526/7188 [00:40<07:12, 15.41it/s]
2022-03-21 10:08:49,234 - INFO - tqdm - f1: 0.8749, accuracy: 0.8754, batch_loss: 0.9258, loss: 0.3559 ||:   9%|9         | 670/7188 [00:50<06:52, 15.82it/s]
2022-03-21 10:08:59,372 - INFO - tqdm - f1: 0.8810, accuracy: 0.8811, batch_loss: 0.5049, loss: 0.3407 ||:  11%|#1        | 814/7188 [01:00<07:35, 13.98it/s]
2022-03-21 10:09:09,391 - INFO - tqdm - f1: 0.8833, accuracy: 0.8834, batch_loss: 0.1718, loss: 0.3347 ||:  13%|#3        | 954/7188 [01:10<07:19, 14.17it/s]
2022-03-21 10:09:19,459 - INFO - tqdm - f1: 0.8870, accuracy: 0.8871, batch_loss: 0.2197, loss: 0.3236 ||:  15%|#5        | 1098/7188 [01:20<07:23, 13.72it/s]
2022-03-21 10:09:29,466 - INFO - tqdm - f1: 0.8902, accuracy: 0.8904, batch_loss: 0.4145, loss: 0.3166 ||:  17%|#7        | 1242/7188 [01:30<07:00, 14.15it/s]
2022-03-21 10:09:39,576 - INFO - tqdm - f1: 0.8926, accuracy: 0.8926, batch_loss: 0.0676, loss: 0.3094 ||:  19%|#9        | 1386/7188 [01:40<07:00, 13.81it/s]
2022-03-21 10:09:49,713 - INFO - tqdm - f1: 0.8935, accuracy: 0.8936, batch_loss: 0.4371, loss: 0.3066 ||:  21%|##1       | 1530/7188 [01:50<06:57, 13.56it/s]
2022-03-21 10:09:59,740 - INFO - tqdm - f1: 0.8951, accuracy: 0.8953, batch_loss: 0.6178, loss: 0.3022 ||:  23%|##3       | 1674/7188 [02:00<07:00, 13.12it/s]
2022-03-21 10:10:09,842 - INFO - tqdm - f1: 0.8970, accuracy: 0.8971, batch_loss: 0.0164, loss: 0.2957 ||:  25%|##5       | 1818/7188 [02:10<06:16, 14.26it/s]
2022-03-21 10:10:19,859 - INFO - tqdm - f1: 0.8982, accuracy: 0.8982, batch_loss: 0.0416, loss: 0.2919 ||:  27%|##7       | 1964/7188 [02:20<06:00, 14.50it/s]
2022-03-21 10:10:29,976 - INFO - tqdm - f1: 0.8993, accuracy: 0.8993, batch_loss: 0.1223, loss: 0.2882 ||:  29%|##9       | 2110/7188 [02:31<05:53, 14.36it/s]
2022-03-21 10:10:39,995 - INFO - tqdm - f1: 0.9008, accuracy: 0.9008, batch_loss: 0.0510, loss: 0.2849 ||:  31%|###1      | 2256/7188 [02:41<05:30, 14.92it/s]
2022-03-21 10:10:50,042 - INFO - tqdm - f1: 0.9020, accuracy: 0.9020, batch_loss: 0.3231, loss: 0.2816 ||:  33%|###3      | 2402/7188 [02:51<05:15, 15.19it/s]
2022-03-21 10:11:00,064 - INFO - tqdm - f1: 0.9030, accuracy: 0.9030, batch_loss: 0.0889, loss: 0.2785 ||:  35%|###5      | 2550/7188 [03:01<05:03, 15.27it/s]
2022-03-21 10:11:10,073 - INFO - tqdm - f1: 0.9033, accuracy: 0.9033, batch_loss: 0.4565, loss: 0.2770 ||:  38%|###7      | 2698/7188 [03:11<04:39, 16.09it/s]
2022-03-21 10:11:20,166 - INFO - tqdm - f1: 0.9041, accuracy: 0.9040, batch_loss: 0.1116, loss: 0.2752 ||:  40%|###9      | 2848/7188 [03:21<04:54, 14.73it/s]
2022-03-21 10:11:30,271 - INFO - tqdm - f1: 0.9048, accuracy: 0.9047, batch_loss: 0.1327, loss: 0.2727 ||:  42%|####1     | 3000/7188 [03:31<04:14, 16.48it/s]
2022-03-21 10:11:40,324 - INFO - tqdm - f1: 0.9057, accuracy: 0.9056, batch_loss: 0.2584, loss: 0.2705 ||:  44%|####3     | 3150/7188 [03:41<04:16, 15.75it/s]
2022-03-21 10:11:50,474 - INFO - tqdm - f1: 0.9061, accuracy: 0.9060, batch_loss: 0.3888, loss: 0.2686 ||:  46%|####5     | 3300/7188 [03:51<04:22, 14.80it/s]
2022-03-21 10:12:00,541 - INFO - tqdm - f1: 0.9069, accuracy: 0.9068, batch_loss: 0.0839, loss: 0.2665 ||:  48%|####7     | 3450/7188 [04:01<04:31, 13.75it/s]
2022-03-21 10:12:10,589 - INFO - tqdm - f1: 0.9075, accuracy: 0.9074, batch_loss: 0.0513, loss: 0.2647 ||:  50%|#####     | 3598/7188 [04:11<04:06, 14.58it/s]
2022-03-21 10:12:20,682 - INFO - tqdm - f1: 0.9082, accuracy: 0.9081, batch_loss: 0.2759, loss: 0.2625 ||:  52%|#####2    | 3748/7188 [04:21<03:58, 14.44it/s]
2022-03-21 10:12:30,713 - INFO - tqdm - f1: 0.9084, accuracy: 0.9083, batch_loss: 0.1198, loss: 0.2620 ||:  54%|#####4    | 3896/7188 [04:31<03:44, 14.69it/s]
2022-03-21 10:12:40,805 - INFO - tqdm - f1: 0.9091, accuracy: 0.9089, batch_loss: 0.2263, loss: 0.2608 ||:  56%|#####6    | 4044/7188 [04:41<03:45, 13.96it/s]
2022-03-21 10:12:50,919 - INFO - tqdm - f1: 0.9094, accuracy: 0.9091, batch_loss: 0.5110, loss: 0.2597 ||:  58%|#####8    | 4190/7188 [04:52<03:42, 13.48it/s]
2022-03-21 10:13:00,945 - INFO - tqdm - f1: 0.9097, accuracy: 0.9094, batch_loss: 0.0511, loss: 0.2589 ||:  60%|######    | 4336/7188 [05:02<03:04, 15.42it/s]
2022-03-21 10:13:10,955 - INFO - tqdm - f1: 0.9102, accuracy: 0.9099, batch_loss: 0.1352, loss: 0.2574 ||:  62%|######2   | 4480/7188 [05:12<03:06, 14.52it/s]
2022-03-21 10:13:20,990 - INFO - tqdm - f1: 0.9104, accuracy: 0.9102, batch_loss: 0.2038, loss: 0.2563 ||:  64%|######4   | 4626/7188 [05:22<02:48, 15.21it/s]
2022-03-21 10:13:31,024 - INFO - tqdm - f1: 0.9107, accuracy: 0.9105, batch_loss: 0.4020, loss: 0.2558 ||:  66%|######6   | 4772/7188 [05:32<02:40, 15.07it/s]
2022-03-21 10:13:41,025 - INFO - tqdm - f1: 0.9109, accuracy: 0.9107, batch_loss: 0.1329, loss: 0.2549 ||:  68%|######8   | 4914/7188 [05:42<02:43, 13.94it/s]
2022-03-21 10:13:51,117 - INFO - tqdm - f1: 0.9111, accuracy: 0.9108, batch_loss: 0.1917, loss: 0.2546 ||:  70%|#######   | 5058/7188 [05:52<02:22, 14.95it/s]
2022-03-21 10:14:01,217 - INFO - tqdm - f1: 0.9117, accuracy: 0.9114, batch_loss: 0.1893, loss: 0.2529 ||:  72%|#######2  | 5204/7188 [06:02<02:15, 14.60it/s]
2022-03-21 10:14:11,263 - INFO - tqdm - f1: 0.9122, accuracy: 0.9119, batch_loss: 0.0513, loss: 0.2518 ||:  74%|#######4  | 5348/7188 [06:12<01:58, 15.48it/s]
2022-03-21 10:14:21,386 - INFO - tqdm - f1: 0.9124, accuracy: 0.9122, batch_loss: 0.3782, loss: 0.2511 ||:  76%|#######6  | 5492/7188 [06:22<01:56, 14.59it/s]
2022-03-21 10:14:31,524 - INFO - tqdm - f1: 0.9129, accuracy: 0.9127, batch_loss: 0.0255, loss: 0.2495 ||:  78%|#######8  | 5638/7188 [06:32<01:45, 14.68it/s]
2022-03-21 10:14:41,620 - INFO - tqdm - f1: 0.9133, accuracy: 0.9131, batch_loss: 0.0984, loss: 0.2487 ||:  80%|########  | 5782/7188 [06:42<01:38, 14.29it/s]
2022-03-21 10:14:51,674 - INFO - tqdm - f1: 0.9135, accuracy: 0.9134, batch_loss: 0.1026, loss: 0.2477 ||:  82%|########2 | 5928/7188 [06:52<01:21, 15.45it/s]
2022-03-21 10:15:01,720 - INFO - tqdm - f1: 0.9138, accuracy: 0.9137, batch_loss: 0.0500, loss: 0.2470 ||:  84%|########4 | 6072/7188 [07:02<01:18, 14.13it/s]
2022-03-21 10:15:11,813 - INFO - tqdm - f1: 0.9142, accuracy: 0.9141, batch_loss: 0.2799, loss: 0.2461 ||:  87%|########6 | 6218/7188 [07:12<01:06, 14.66it/s]
2022-03-21 10:15:21,872 - INFO - tqdm - f1: 0.9143, accuracy: 0.9143, batch_loss: 0.5015, loss: 0.2457 ||:  89%|########8 | 6366/7188 [07:22<00:58, 13.99it/s]
2022-03-21 10:15:31,883 - INFO - tqdm - f1: 0.9146, accuracy: 0.9145, batch_loss: 0.0372, loss: 0.2448 ||:  91%|######### | 6514/7188 [07:32<00:42, 15.70it/s]
2022-03-21 10:15:41,885 - INFO - tqdm - f1: 0.9149, accuracy: 0.9148, batch_loss: 0.2593, loss: 0.2440 ||:  93%|#########2| 6662/7188 [07:42<00:35, 14.76it/s]
2022-03-21 10:15:51,945 - INFO - tqdm - f1: 0.9155, accuracy: 0.9155, batch_loss: 0.0632, loss: 0.2426 ||:  95%|#########4| 6812/7188 [07:53<00:26, 14.43it/s]
2022-03-21 10:16:01,981 - INFO - tqdm - f1: 0.9159, accuracy: 0.9159, batch_loss: 0.0456, loss: 0.2416 ||:  97%|#########6| 6964/7188 [08:03<00:15, 14.90it/s]
2022-03-21 10:16:12,090 - INFO - tqdm - f1: 0.9163, accuracy: 0.9162, batch_loss: 0.1859, loss: 0.2407 ||:  99%|#########8| 7116/7188 [08:13<00:05, 14.18it/s]
2022-03-21 10:16:14,602 - INFO - tqdm - f1: 0.9163, accuracy: 0.9163, batch_loss: 0.4906, loss: 0.2407 ||: 100%|#########9| 7154/7188 [08:15<00:02, 14.09it/s]
2022-03-21 10:16:14,759 - INFO - tqdm - f1: 0.9163, accuracy: 0.9163, batch_loss: 0.4190, loss: 0.2407 ||: 100%|#########9| 7156/7188 [08:15<00:02, 13.67it/s]
2022-03-21 10:16:14,894 - INFO - tqdm - f1: 0.9163, accuracy: 0.9163, batch_loss: 0.0711, loss: 0.2407 ||: 100%|#########9| 7158/7188 [08:16<00:02, 13.98it/s]
2022-03-21 10:16:15,022 - INFO - tqdm - f1: 0.9163, accuracy: 0.9163, batch_loss: 0.2777, loss: 0.2407 ||: 100%|#########9| 7160/7188 [08:16<00:01, 14.43it/s]
2022-03-21 10:16:15,148 - INFO - tqdm - f1: 0.9163, accuracy: 0.9163, batch_loss: 0.0579, loss: 0.2406 ||: 100%|#########9| 7162/7188 [08:16<00:01, 14.86it/s]
2022-03-21 10:16:15,276 - INFO - tqdm - f1: 0.9164, accuracy: 0.9163, batch_loss: 0.0489, loss: 0.2406 ||: 100%|#########9| 7164/7188 [08:16<00:01, 15.08it/s]
2022-03-21 10:16:15,403 - INFO - tqdm - f1: 0.9164, accuracy: 0.9163, batch_loss: 0.1052, loss: 0.2406 ||: 100%|#########9| 7166/7188 [08:16<00:01, 15.27it/s]
2022-03-21 10:16:15,547 - INFO - tqdm - f1: 0.9164, accuracy: 0.9163, batch_loss: 0.1437, loss: 0.2406 ||: 100%|#########9| 7168/7188 [08:16<00:01, 14.81it/s]
2022-03-21 10:16:15,693 - INFO - tqdm - f1: 0.9164, accuracy: 0.9164, batch_loss: 0.1026, loss: 0.2406 ||: 100%|#########9| 7170/7188 [08:16<00:01, 14.48it/s]
2022-03-21 10:16:15,854 - INFO - tqdm - f1: 0.9164, accuracy: 0.9163, batch_loss: 0.3604, loss: 0.2406 ||: 100%|#########9| 7172/7188 [08:16<00:01, 13.77it/s]
2022-03-21 10:16:16,008 - INFO - tqdm - f1: 0.9164, accuracy: 0.9163, batch_loss: 0.0181, loss: 0.2405 ||: 100%|#########9| 7174/7188 [08:17<00:01, 13.54it/s]
2022-03-21 10:16:16,140 - INFO - tqdm - f1: 0.9164, accuracy: 0.9163, batch_loss: 0.5836, loss: 0.2406 ||: 100%|#########9| 7176/7188 [08:17<00:00, 13.99it/s]
2022-03-21 10:16:16,266 - INFO - tqdm - f1: 0.9164, accuracy: 0.9164, batch_loss: 0.0324, loss: 0.2405 ||: 100%|#########9| 7178/7188 [08:17<00:00, 14.52it/s]
2022-03-21 10:16:16,390 - INFO - tqdm - f1: 0.9164, accuracy: 0.9164, batch_loss: 0.0671, loss: 0.2405 ||: 100%|#########9| 7180/7188 [08:17<00:00, 14.94it/s]
2022-03-21 10:16:16,522 - INFO - tqdm - f1: 0.9164, accuracy: 0.9164, batch_loss: 0.0137, loss: 0.2405 ||: 100%|#########9| 7182/7188 [08:17<00:00, 15.01it/s]
2022-03-21 10:16:16,655 - INFO - tqdm - f1: 0.9164, accuracy: 0.9164, batch_loss: 0.0464, loss: 0.2404 ||: 100%|#########9| 7184/7188 [08:17<00:00, 15.01it/s]
2022-03-21 10:16:16,836 - INFO - tqdm - f1: 0.9164, accuracy: 0.9164, batch_loss: 0.0228, loss: 0.2404 ||: 100%|#########9| 7186/7188 [08:17<00:00, 13.56it/s]
2022-03-21 10:16:16,971 - INFO - tqdm - f1: 0.9165, accuracy: 0.9164, batch_loss: 0.2319, loss: 0.2403 ||: 100%|##########| 7188/7188 [08:18<00:00, 13.92it/s]
2022-03-21 10:16:17,006 - INFO - tqdm - f1: 0.9165, accuracy: 0.9164, batch_loss: 0.2319, loss: 0.2403 ||: 100%|##########| 7188/7188 [08:18<00:00, 14.43it/s]
2022-03-21 10:16:17,014 - INFO - allennlp.training.trainer - Validating
2022-03-21 10:16:17,017 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 10:16:17,023 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-21 10:16:17,030 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-21 10:16:25,298 - INFO - tqdm - f1: 0.9345, accuracy: 0.9344, batch_loss: 0.0972, loss: 0.1983 ||: 100%|##########| 313/313 [00:08<00:00, 37.81it/s]
2022-03-21 10:16:25,342 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/ag_base_hyper_small_seed_47/best.th'.
2022-03-21 10:16:28,698 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 10:16:28,704 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.916  |     0.934
2022-03-21 10:16:28,711 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.916  |     0.935
2022-03-21 10:16:28,712 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 10:16:28,719 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.240  |     0.198
2022-03-21 10:16:28,720 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7389.879  |       N/A
2022-03-21 10:16:28,728 - INFO - allennlp.training.trainer - Epoch duration: 0:08:29.859418
2022-03-21 10:16:28,736 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:16:28
2022-03-21 10:16:28,743 - INFO - allennlp.training.trainer - Epoch 1/9
2022-03-21 10:16:28,750 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-21 10:16:28,757 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 10:16:28,765 - INFO - allennlp.training.trainer - Training
2022-03-21 10:16:28,772 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 10:16:38,848 - INFO - tqdm - f1: 0.9386, accuracy: 0.9384, batch_loss: 0.0534, loss: 0.1963 ||:   2%|1         | 135/7188 [00:10<08:32, 13.76it/s]
2022-03-21 10:16:48,984 - INFO - tqdm - f1: 0.9395, accuracy: 0.9395, batch_loss: 0.0432, loss: 0.1807 ||:   4%|3         | 281/7188 [00:20<08:21, 13.76it/s]
2022-03-21 10:16:59,102 - INFO - tqdm - f1: 0.9397, accuracy: 0.9398, batch_loss: 0.1775, loss: 0.1772 ||:   6%|5         | 427/7188 [00:30<08:16, 13.63it/s]
2022-03-21 10:17:09,169 - INFO - tqdm - f1: 0.9406, accuracy: 0.9408, batch_loss: 0.2309, loss: 0.1749 ||:   8%|7         | 575/7188 [00:40<07:17, 15.12it/s]
2022-03-21 10:17:19,246 - INFO - tqdm - f1: 0.9396, accuracy: 0.9398, batch_loss: 0.2352, loss: 0.1769 ||:  10%|#         | 725/7188 [00:50<07:25, 14.51it/s]
2022-03-21 10:17:29,250 - INFO - tqdm - f1: 0.9387, accuracy: 0.9388, batch_loss: 0.0655, loss: 0.1776 ||:  12%|#2        | 871/7188 [01:00<06:37, 15.91it/s]
2022-03-21 10:17:39,366 - INFO - tqdm - f1: 0.9384, accuracy: 0.9384, batch_loss: 0.0989, loss: 0.1782 ||:  14%|#4        | 1021/7188 [01:10<06:35, 15.60it/s]
2022-03-21 10:17:49,427 - INFO - tqdm - f1: 0.9392, accuracy: 0.9392, batch_loss: 0.1110, loss: 0.1762 ||:  16%|#6        | 1165/7188 [01:20<06:29, 15.46it/s]
2022-03-21 10:17:59,475 - INFO - tqdm - f1: 0.9390, accuracy: 0.9388, batch_loss: 0.1586, loss: 0.1768 ||:  18%|#8        | 1309/7188 [01:30<06:25, 15.25it/s]
2022-03-21 10:18:09,577 - INFO - tqdm - f1: 0.9393, accuracy: 0.9393, batch_loss: 0.0189, loss: 0.1763 ||:  20%|##        | 1459/7188 [01:40<06:14, 15.30it/s]
2022-03-21 10:18:19,638 - INFO - tqdm - f1: 0.9400, accuracy: 0.9399, batch_loss: 0.0535, loss: 0.1747 ||:  22%|##2       | 1607/7188 [01:50<06:14, 14.90it/s]
2022-03-21 10:18:29,641 - INFO - tqdm - f1: 0.9398, accuracy: 0.9397, batch_loss: 0.1345, loss: 0.1743 ||:  24%|##4       | 1757/7188 [02:00<05:42, 15.87it/s]
2022-03-21 10:18:39,646 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.1330, loss: 0.1723 ||:  26%|##6       | 1903/7188 [02:10<06:07, 14.39it/s]
2022-03-21 10:18:49,777 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.2346, loss: 0.1713 ||:  29%|##8       | 2051/7188 [02:20<05:45, 14.88it/s]
2022-03-21 10:18:59,778 - INFO - tqdm - f1: 0.9400, accuracy: 0.9401, batch_loss: 0.1154, loss: 0.1721 ||:  31%|###       | 2197/7188 [02:30<06:06, 13.62it/s]
2022-03-21 10:19:09,885 - INFO - tqdm - f1: 0.9399, accuracy: 0.9399, batch_loss: 0.1790, loss: 0.1736 ||:  33%|###2      | 2347/7188 [02:41<05:21, 15.06it/s]
2022-03-21 10:19:19,999 - INFO - tqdm - f1: 0.9402, accuracy: 0.9402, batch_loss: 0.3238, loss: 0.1723 ||:  35%|###4      | 2493/7188 [02:51<05:17, 14.81it/s]
2022-03-21 10:19:30,056 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0464, loss: 0.1721 ||:  37%|###6      | 2643/7188 [03:01<05:16, 14.34it/s]
2022-03-21 10:19:40,209 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.2703, loss: 0.1729 ||:  39%|###8      | 2793/7188 [03:11<05:17, 13.82it/s]
2022-03-21 10:19:50,224 - INFO - tqdm - f1: 0.9399, accuracy: 0.9399, batch_loss: 0.0566, loss: 0.1728 ||:  41%|####      | 2945/7188 [03:21<04:30, 15.66it/s]
2022-03-21 10:20:00,236 - INFO - tqdm - f1: 0.9396, accuracy: 0.9396, batch_loss: 0.0286, loss: 0.1738 ||:  43%|####3     | 3095/7188 [03:31<04:12, 16.21it/s]
2022-03-21 10:20:10,278 - INFO - tqdm - f1: 0.9400, accuracy: 0.9400, batch_loss: 0.3522, loss: 0.1729 ||:  45%|####5     | 3245/7188 [03:41<04:13, 15.56it/s]
2022-03-21 10:20:20,353 - INFO - tqdm - f1: 0.9399, accuracy: 0.9400, batch_loss: 0.0863, loss: 0.1725 ||:  47%|####7     | 3391/7188 [03:51<04:13, 14.99it/s]
2022-03-21 10:20:30,424 - INFO - tqdm - f1: 0.9398, accuracy: 0.9398, batch_loss: 0.1924, loss: 0.1724 ||:  49%|####9     | 3537/7188 [04:01<04:13, 14.39it/s]
2022-03-21 10:20:40,553 - INFO - tqdm - f1: 0.9400, accuracy: 0.9400, batch_loss: 0.0259, loss: 0.1719 ||:  51%|#####1    | 3679/7188 [04:11<04:02, 14.47it/s]
2022-03-21 10:20:50,577 - INFO - tqdm - f1: 0.9399, accuracy: 0.9399, batch_loss: 0.2724, loss: 0.1721 ||:  53%|#####3    | 3825/7188 [04:21<03:32, 15.84it/s]
2022-03-21 10:21:00,600 - INFO - tqdm - f1: 0.9397, accuracy: 0.9397, batch_loss: 0.1089, loss: 0.1718 ||:  55%|#####5    | 3965/7188 [04:31<03:47, 14.15it/s]
2022-03-21 10:21:10,746 - INFO - tqdm - f1: 0.9397, accuracy: 0.9397, batch_loss: 0.1945, loss: 0.1719 ||:  57%|#####7    | 4107/7188 [04:41<03:41, 13.94it/s]
2022-03-21 10:21:20,854 - INFO - tqdm - f1: 0.9399, accuracy: 0.9398, batch_loss: 0.0361, loss: 0.1714 ||:  59%|#####9    | 4245/7188 [04:52<03:54, 12.54it/s]
2022-03-21 10:21:30,874 - INFO - tqdm - f1: 0.9402, accuracy: 0.9402, batch_loss: 0.0253, loss: 0.1712 ||:  61%|######1   | 4385/7188 [05:02<03:16, 14.24it/s]
2022-03-21 10:21:40,925 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.0765, loss: 0.1715 ||:  63%|######2   | 4527/7188 [05:12<03:23, 13.08it/s]
2022-03-21 10:21:51,007 - INFO - tqdm - f1: 0.9404, accuracy: 0.9403, batch_loss: 0.1110, loss: 0.1716 ||:  65%|######4   | 4665/7188 [05:22<03:07, 13.45it/s]
2022-03-21 10:22:01,138 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0081, loss: 0.1714 ||:  66%|######6   | 4757/7188 [05:32<05:37,  7.20it/s]
2022-03-21 10:22:11,194 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0285, loss: 0.1713 ||:  67%|######7   | 4836/7188 [05:42<05:45,  6.81it/s]
2022-03-21 10:22:21,199 - INFO - tqdm - f1: 0.9402, accuracy: 0.9402, batch_loss: 0.1530, loss: 0.1719 ||:  68%|######8   | 4921/7188 [05:52<05:08,  7.36it/s]
2022-03-21 10:22:31,255 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.1781, loss: 0.1723 ||:  70%|######9   | 5004/7188 [06:02<05:26,  6.68it/s]
2022-03-21 10:22:41,326 - INFO - tqdm - f1: 0.9400, accuracy: 0.9399, batch_loss: 0.1645, loss: 0.1728 ||:  71%|#######   | 5083/7188 [06:12<04:19,  8.11it/s]
2022-03-21 10:22:51,353 - INFO - tqdm - f1: 0.9399, accuracy: 0.9398, batch_loss: 0.3455, loss: 0.1729 ||:  72%|#######1  | 5160/7188 [06:22<04:40,  7.24it/s]
2022-03-21 10:23:01,724 - INFO - tqdm - f1: 0.9399, accuracy: 0.9399, batch_loss: 0.0743, loss: 0.1730 ||:  73%|#######2  | 5215/7188 [06:32<09:16,  3.54it/s]
2022-03-21 10:23:12,273 - INFO - tqdm - f1: 0.9400, accuracy: 0.9399, batch_loss: 0.3803, loss: 0.1730 ||:  74%|#######3  | 5286/7188 [06:43<07:48,  4.06it/s]
2022-03-21 10:23:22,567 - INFO - tqdm - f1: 0.9399, accuracy: 0.9398, batch_loss: 0.2033, loss: 0.1733 ||:  74%|#######4  | 5348/7188 [06:53<10:52,  2.82it/s]
2022-03-21 10:23:33,455 - INFO - tqdm - f1: 0.9400, accuracy: 0.9399, batch_loss: 0.2181, loss: 0.1730 ||:  75%|#######5  | 5412/7188 [07:04<12:13,  2.42it/s]
2022-03-21 10:23:43,722 - INFO - tqdm - f1: 0.9400, accuracy: 0.9400, batch_loss: 0.0121, loss: 0.1730 ||:  76%|#######6  | 5470/7188 [07:14<10:35,  2.70it/s]
2022-03-21 10:23:54,057 - INFO - tqdm - f1: 0.9400, accuracy: 0.9400, batch_loss: 0.0416, loss: 0.1731 ||:  77%|#######6  | 5530/7188 [07:25<09:53,  2.80it/s]
2022-03-21 10:24:04,450 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.2235, loss: 0.1728 ||:  78%|#######8  | 5641/7188 [07:35<05:29,  4.69it/s]
2022-03-21 10:24:14,551 - INFO - tqdm - f1: 0.9399, accuracy: 0.9399, batch_loss: 0.3273, loss: 0.1732 ||:  80%|########  | 5751/7188 [07:45<05:09,  4.64it/s]
2022-03-21 10:24:24,859 - INFO - tqdm - f1: 0.9399, accuracy: 0.9399, batch_loss: 0.0940, loss: 0.1734 ||:  82%|########1 | 5861/7188 [07:56<04:44,  4.67it/s]
2022-03-21 10:24:35,934 - INFO - tqdm - f1: 0.9400, accuracy: 0.9400, batch_loss: 0.0620, loss: 0.1733 ||:  83%|########3 | 5979/7188 [08:07<04:29,  4.48it/s]
2022-03-21 10:24:46,941 - INFO - tqdm - f1: 0.9401, accuracy: 0.9401, batch_loss: 0.2819, loss: 0.1730 ||:  85%|########4 | 6101/7188 [08:18<04:01,  4.50it/s]
2022-03-21 10:24:57,078 - INFO - tqdm - f1: 0.9401, accuracy: 0.9400, batch_loss: 0.5515, loss: 0.1730 ||:  86%|########6 | 6217/7188 [08:28<01:07, 14.42it/s]
2022-03-21 10:25:07,135 - INFO - tqdm - f1: 0.9402, accuracy: 0.9402, batch_loss: 0.0743, loss: 0.1727 ||:  88%|########7 | 6315/7188 [08:38<01:12, 12.08it/s]
2022-03-21 10:25:17,179 - INFO - tqdm - f1: 0.9402, accuracy: 0.9402, batch_loss: 0.2078, loss: 0.1726 ||:  89%|########9 | 6421/7188 [08:48<00:59, 12.88it/s]
2022-03-21 10:25:27,253 - INFO - tqdm - f1: 0.9402, accuracy: 0.9402, batch_loss: 0.0625, loss: 0.1723 ||:  91%|######### | 6532/7188 [08:58<01:07,  9.70it/s]
2022-03-21 10:25:37,360 - INFO - tqdm - f1: 0.9402, accuracy: 0.9402, batch_loss: 0.1453, loss: 0.1724 ||:  92%|#########2| 6633/7188 [09:08<00:51, 10.87it/s]
2022-03-21 10:25:47,491 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.0472, loss: 0.1719 ||:  93%|#########3| 6716/7188 [09:18<01:15,  6.21it/s]
2022-03-21 10:25:57,644 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.0400, loss: 0.1720 ||:  94%|#########4| 6773/7188 [09:28<01:08,  6.04it/s]
2022-03-21 10:26:07,689 - INFO - tqdm - f1: 0.9404, accuracy: 0.9403, batch_loss: 0.0459, loss: 0.1719 ||:  95%|#########5| 6835/7188 [09:38<00:49,  7.10it/s]
2022-03-21 10:26:17,782 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0559, loss: 0.1717 ||:  96%|#########6| 6903/7188 [09:49<00:44,  6.38it/s]
2022-03-21 10:26:27,817 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.4543, loss: 0.1718 ||:  97%|#########7| 6974/7188 [09:59<00:29,  7.36it/s]
2022-03-21 10:26:37,899 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.4108, loss: 0.1718 ||:  98%|#########7| 7033/7188 [10:09<00:27,  5.61it/s]
2022-03-21 10:26:47,928 - INFO - tqdm - f1: 0.9403, accuracy: 0.9403, batch_loss: 0.0112, loss: 0.1716 ||:  98%|#########8| 7079/7188 [10:19<00:26,  4.04it/s]
2022-03-21 10:26:58,055 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0066, loss: 0.1713 ||:  99%|#########9| 7131/7188 [10:29<00:10,  5.51it/s]
2022-03-21 10:27:02,258 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.1429, loss: 0.1712 ||: 100%|#########9| 7153/7188 [10:33<00:06,  5.23it/s]
2022-03-21 10:27:02,441 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.2310, loss: 0.1712 ||: 100%|#########9| 7154/7188 [10:33<00:06,  5.30it/s]
2022-03-21 10:27:02,657 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0156, loss: 0.1712 ||: 100%|#########9| 7155/7188 [10:33<00:06,  5.08it/s]
2022-03-21 10:27:02,930 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0165, loss: 0.1712 ||: 100%|#########9| 7156/7188 [10:34<00:06,  4.57it/s]
2022-03-21 10:27:03,077 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0449, loss: 0.1712 ||: 100%|#########9| 7157/7188 [10:34<00:06,  5.04it/s]
2022-03-21 10:27:03,241 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0155, loss: 0.1712 ||: 100%|#########9| 7158/7188 [10:34<00:05,  5.32it/s]
2022-03-21 10:27:03,399 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.1732, loss: 0.1712 ||: 100%|#########9| 7159/7188 [10:34<00:05,  5.58it/s]
2022-03-21 10:27:03,530 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.2067, loss: 0.1712 ||: 100%|#########9| 7160/7188 [10:34<00:04,  6.07it/s]
2022-03-21 10:27:03,667 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.3154, loss: 0.1712 ||: 100%|#########9| 7161/7188 [10:34<00:04,  6.40it/s]
2022-03-21 10:27:03,847 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.1354, loss: 0.1712 ||: 100%|#########9| 7162/7188 [10:35<00:04,  6.12it/s]
2022-03-21 10:27:04,025 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0077, loss: 0.1712 ||: 100%|#########9| 7163/7188 [10:35<00:04,  6.17it/s]
2022-03-21 10:27:04,187 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.6708, loss: 0.1712 ||: 100%|#########9| 7164/7188 [10:35<00:03,  6.12it/s]
2022-03-21 10:27:04,306 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.2470, loss: 0.1712 ||: 100%|#########9| 7165/7188 [10:35<00:03,  6.48it/s]
2022-03-21 10:27:04,445 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.1080, loss: 0.1712 ||: 100%|#########9| 7166/7188 [10:35<00:03,  6.68it/s]
2022-03-21 10:27:04,597 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.1977, loss: 0.1712 ||: 100%|#########9| 7167/7188 [10:35<00:03,  6.65it/s]
2022-03-21 10:27:04,759 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.1714, loss: 0.1712 ||: 100%|#########9| 7168/7188 [10:35<00:03,  6.50it/s]
2022-03-21 10:27:04,928 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.2798, loss: 0.1712 ||: 100%|#########9| 7169/7188 [10:36<00:03,  6.32it/s]
2022-03-21 10:27:05,071 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0631, loss: 0.1712 ||: 100%|#########9| 7170/7188 [10:36<00:02,  6.51it/s]
2022-03-21 10:27:05,233 - INFO - tqdm - f1: 0.9404, accuracy: 0.9403, batch_loss: 0.1779, loss: 0.1712 ||: 100%|#########9| 7171/7188 [10:36<00:02,  6.39it/s]
2022-03-21 10:27:05,385 - INFO - tqdm - f1: 0.9404, accuracy: 0.9403, batch_loss: 0.4812, loss: 0.1713 ||: 100%|#########9| 7172/7188 [10:36<00:02,  6.45it/s]
2022-03-21 10:27:05,501 - INFO - tqdm - f1: 0.9404, accuracy: 0.9403, batch_loss: 0.0275, loss: 0.1713 ||: 100%|#########9| 7173/7188 [10:36<00:02,  6.98it/s]
2022-03-21 10:27:05,606 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0282, loss: 0.1712 ||: 100%|#########9| 7174/7188 [10:36<00:01,  7.58it/s]
2022-03-21 10:27:05,725 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0708, loss: 0.1712 ||: 100%|#########9| 7175/7188 [10:36<00:01,  7.81it/s]
2022-03-21 10:27:05,896 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0807, loss: 0.1712 ||: 100%|#########9| 7176/7188 [10:37<00:01,  7.10it/s]
2022-03-21 10:27:06,014 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.1640, loss: 0.1712 ||: 100%|#########9| 7177/7188 [10:37<00:01,  7.47it/s]
2022-03-21 10:27:06,132 - INFO - tqdm - f1: 0.9404, accuracy: 0.9403, batch_loss: 0.8315, loss: 0.1713 ||: 100%|#########9| 7178/7188 [10:37<00:01,  7.73it/s]
2022-03-21 10:27:06,363 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0937, loss: 0.1713 ||: 100%|#########9| 7180/7188 [10:37<00:00,  8.14it/s]
2022-03-21 10:27:06,476 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0228, loss: 0.1712 ||: 100%|#########9| 7181/7188 [10:37<00:00,  8.30it/s]
2022-03-21 10:27:06,633 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.3784, loss: 0.1713 ||: 100%|#########9| 7182/7188 [10:37<00:00,  7.76it/s]
2022-03-21 10:27:06,772 - INFO - tqdm - f1: 0.9404, accuracy: 0.9403, batch_loss: 0.1016, loss: 0.1713 ||: 100%|#########9| 7183/7188 [10:37<00:00,  7.53it/s]
2022-03-21 10:27:06,919 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0494, loss: 0.1712 ||: 100%|#########9| 7184/7188 [10:38<00:00,  7.30it/s]
2022-03-21 10:27:07,086 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0196, loss: 0.1712 ||: 100%|#########9| 7185/7188 [10:38<00:00,  6.87it/s]
2022-03-21 10:27:07,191 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.1702, loss: 0.1712 ||: 100%|#########9| 7186/7188 [10:38<00:00,  7.49it/s]
2022-03-21 10:27:07,325 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0917, loss: 0.1712 ||: 100%|#########9| 7187/7188 [10:38<00:00,  7.48it/s]
2022-03-21 10:27:07,507 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0589, loss: 0.1712 ||: 100%|##########| 7188/7188 [10:38<00:00,  6.75it/s]
2022-03-21 10:27:07,610 - INFO - tqdm - f1: 0.9404, accuracy: 0.9404, batch_loss: 0.0589, loss: 0.1712 ||: 100%|##########| 7188/7188 [10:38<00:00, 11.25it/s]
2022-03-21 10:27:07,629 - INFO - allennlp.training.trainer - Validating
2022-03-21 10:27:07,650 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 10:27:17,660 - INFO - tqdm - f1: 0.9379, accuracy: 0.9377, batch_loss: 0.3955, loss: 0.1784 ||:  52%|#####2    | 164/313 [00:10<00:08, 17.03it/s]
2022-03-21 10:27:27,756 - INFO - tqdm - f1: 0.9382, accuracy: 0.9381, batch_loss: 0.0227, loss: 0.1894 ||:  87%|########6 | 271/313 [00:20<00:05,  7.38it/s]
2022-03-21 10:27:33,483 - INFO - tqdm - f1: 0.9372, accuracy: 0.9370, batch_loss: 0.6114, loss: 0.1939 ||: 100%|#########9| 312/313 [00:25<00:00,  8.79it/s]
2022-03-21 10:27:33,586 - INFO - tqdm - f1: 0.9374, accuracy: 0.9372, batch_loss: 0.0093, loss: 0.1933 ||: 100%|##########| 313/313 [00:25<00:00,  8.98it/s]
2022-03-21 10:27:33,604 - INFO - tqdm - f1: 0.9374, accuracy: 0.9372, batch_loss: 0.0093, loss: 0.1933 ||: 100%|##########| 313/313 [00:25<00:00, 12.06it/s]
2022-03-21 10:27:33,670 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/ag_base_hyper_small_seed_47/best.th'.
2022-03-21 10:27:38,542 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 10:27:38,552 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.940  |     0.937
2022-03-21 10:27:38,562 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.940  |     0.937
2022-03-21 10:27:38,573 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 10:27:38,584 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.171  |     0.193
2022-03-21 10:27:38,595 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7574.031  |       N/A
2022-03-21 10:27:38,609 - INFO - allennlp.training.trainer - Epoch duration: 0:11:09.866475
2022-03-21 10:27:38,617 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:18:38
2022-03-21 10:27:38,630 - INFO - allennlp.training.trainer - Epoch 2/9
2022-03-21 10:27:38,641 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-21 10:27:38,651 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 10:27:38,684 - INFO - allennlp.training.trainer - Training
2022-03-21 10:27:38,693 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 10:27:48,824 - INFO - tqdm - f1: 0.9510, accuracy: 0.9507, batch_loss: 0.1976, loss: 0.1430 ||:   1%|          | 57/7188 [00:10<24:09,  4.92it/s]
2022-03-21 10:27:58,840 - INFO - tqdm - f1: 0.9517, accuracy: 0.9512, batch_loss: 0.1216, loss: 0.1378 ||:   2%|1         | 114/7188 [00:20<23:34,  5.00it/s]
2022-03-21 10:28:09,064 - INFO - tqdm - f1: 0.9486, accuracy: 0.9482, batch_loss: 0.1479, loss: 0.1354 ||:   2%|2         | 164/7188 [00:30<26:53,  4.35it/s]
2022-03-21 10:28:19,254 - INFO - tqdm - f1: 0.9519, accuracy: 0.9513, batch_loss: 0.0582, loss: 0.1317 ||:   3%|2         | 213/7188 [00:40<24:11,  4.81it/s]
2022-03-21 10:28:29,378 - INFO - tqdm - f1: 0.9502, accuracy: 0.9498, batch_loss: 0.2437, loss: 0.1344 ||:   4%|3         | 265/7188 [00:50<20:21,  5.67it/s]
2022-03-21 10:28:39,424 - INFO - tqdm - f1: 0.9522, accuracy: 0.9516, batch_loss: 0.0907, loss: 0.1338 ||:   5%|4         | 328/7188 [01:00<16:01,  7.14it/s]
2022-03-21 10:28:49,556 - INFO - tqdm - f1: 0.9534, accuracy: 0.9527, batch_loss: 0.1868, loss: 0.1310 ||:   5%|5         | 394/7188 [01:10<19:25,  5.83it/s]
2022-03-21 10:28:59,677 - INFO - tqdm - f1: 0.9545, accuracy: 0.9540, batch_loss: 0.0617, loss: 0.1281 ||:   7%|6         | 478/7188 [01:20<10:41, 10.46it/s]
2022-03-21 10:29:09,755 - INFO - tqdm - f1: 0.9559, accuracy: 0.9555, batch_loss: 0.1000, loss: 0.1265 ||:   8%|7         | 552/7188 [01:31<18:18,  6.04it/s]
2022-03-21 10:29:19,788 - INFO - tqdm - f1: 0.9560, accuracy: 0.9556, batch_loss: 0.0778, loss: 0.1267 ||:   9%|8         | 618/7188 [01:41<14:35,  7.51it/s]
2022-03-21 10:29:29,876 - INFO - tqdm - f1: 0.9564, accuracy: 0.9559, batch_loss: 0.1282, loss: 0.1254 ||:   9%|9         | 681/7188 [01:51<18:13,  5.95it/s]
2022-03-21 10:29:40,045 - INFO - tqdm - f1: 0.9559, accuracy: 0.9556, batch_loss: 0.0820, loss: 0.1283 ||:  10%|#         | 749/7188 [02:01<17:14,  6.23it/s]
2022-03-21 10:29:50,100 - INFO - tqdm - f1: 0.9551, accuracy: 0.9549, batch_loss: 0.1558, loss: 0.1302 ||:  11%|#1        | 815/7188 [02:11<14:02,  7.56it/s]
2022-03-21 10:30:00,105 - INFO - tqdm - f1: 0.9549, accuracy: 0.9547, batch_loss: 0.1022, loss: 0.1312 ||:  12%|#2        | 885/7188 [02:21<14:29,  7.25it/s]
2022-03-21 10:30:10,119 - INFO - tqdm - f1: 0.9554, accuracy: 0.9555, batch_loss: 0.6153, loss: 0.1303 ||:  13%|#3        | 950/7188 [02:31<16:21,  6.36it/s]
2022-03-21 10:30:20,222 - INFO - tqdm - f1: 0.9556, accuracy: 0.9557, batch_loss: 0.0673, loss: 0.1299 ||:  14%|#4        | 1038/7188 [02:41<10:02, 10.20it/s]
2022-03-21 10:30:30,328 - INFO - tqdm - f1: 0.9554, accuracy: 0.9553, batch_loss: 0.1858, loss: 0.1313 ||:  16%|#5        | 1135/7188 [02:51<10:58,  9.20it/s]
2022-03-21 10:30:40,360 - INFO - tqdm - f1: 0.9547, accuracy: 0.9547, batch_loss: 0.5878, loss: 0.1325 ||:  17%|#6        | 1208/7188 [03:01<08:59, 11.08it/s]
2022-03-21 10:30:50,460 - INFO - tqdm - f1: 0.9547, accuracy: 0.9547, batch_loss: 0.0368, loss: 0.1311 ||:  18%|#8        | 1324/7188 [03:11<08:45, 11.16it/s]
2022-03-21 10:31:00,600 - INFO - tqdm - f1: 0.9544, accuracy: 0.9544, batch_loss: 0.0389, loss: 0.1322 ||:  20%|#9        | 1414/7188 [03:21<09:45,  9.87it/s]
2022-03-21 10:31:10,659 - INFO - tqdm - f1: 0.9540, accuracy: 0.9540, batch_loss: 0.1143, loss: 0.1339 ||:  21%|##        | 1507/7188 [03:31<10:25,  9.08it/s]
2022-03-21 10:31:20,830 - INFO - tqdm - f1: 0.9540, accuracy: 0.9540, batch_loss: 0.0529, loss: 0.1344 ||:  22%|##2       | 1599/7188 [03:42<09:58,  9.33it/s]
2022-03-21 10:31:30,837 - INFO - tqdm - f1: 0.9542, accuracy: 0.9543, batch_loss: 0.0496, loss: 0.1340 ||:  24%|##3       | 1708/7188 [03:52<07:52, 11.60it/s]
2022-03-21 10:31:40,880 - INFO - tqdm - f1: 0.9544, accuracy: 0.9545, batch_loss: 0.0103, loss: 0.1340 ||:  25%|##5       | 1812/7188 [04:02<06:12, 14.43it/s]
2022-03-21 10:31:50,909 - INFO - tqdm - f1: 0.9542, accuracy: 0.9543, batch_loss: 0.2235, loss: 0.1351 ||:  27%|##6       | 1918/7188 [04:12<06:17, 13.95it/s]
2022-03-21 10:32:01,000 - INFO - tqdm - f1: 0.9543, accuracy: 0.9544, batch_loss: 0.3090, loss: 0.1347 ||:  28%|##8       | 2024/7188 [04:22<08:02, 10.70it/s]
2022-03-21 10:32:11,552 - INFO - tqdm - f1: 0.9542, accuracy: 0.9543, batch_loss: 0.0306, loss: 0.1343 ||:  30%|##9       | 2134/7188 [04:32<18:59,  4.43it/s]
2022-03-21 10:32:22,702 - INFO - tqdm - f1: 0.9543, accuracy: 0.9544, batch_loss: 0.4706, loss: 0.1336 ||:  31%|###1      | 2254/7188 [04:43<18:00,  4.57it/s]
2022-03-21 10:32:33,507 - INFO - tqdm - f1: 0.9539, accuracy: 0.9540, batch_loss: 0.1363, loss: 0.1347 ||:  33%|###2      | 2372/7188 [04:54<17:53,  4.49it/s]
2022-03-21 10:32:44,170 - INFO - tqdm - f1: 0.9540, accuracy: 0.9542, batch_loss: 0.0154, loss: 0.1344 ||:  35%|###4      | 2488/7188 [05:05<17:08,  4.57it/s]
2022-03-21 10:32:54,820 - INFO - tqdm - f1: 0.9545, accuracy: 0.9545, batch_loss: 0.0693, loss: 0.1337 ||:  36%|###6      | 2606/7188 [05:16<16:18,  4.68it/s]
2022-03-21 10:33:05,239 - INFO - tqdm - f1: 0.9536, accuracy: 0.9536, batch_loss: 0.1237, loss: 0.1356 ||:  38%|###7      | 2720/7188 [05:26<16:25,  4.54it/s]
2022-03-21 10:33:15,651 - INFO - tqdm - f1: 0.9533, accuracy: 0.9533, batch_loss: 0.0099, loss: 0.1355 ||:  39%|###9      | 2832/7188 [05:36<15:54,  4.56it/s]
2022-03-21 10:33:26,667 - INFO - tqdm - f1: 0.9532, accuracy: 0.9532, batch_loss: 0.1725, loss: 0.1358 ||:  41%|####1     | 2950/7188 [05:47<15:18,  4.62it/s]
2022-03-21 10:33:37,174 - INFO - tqdm - f1: 0.9532, accuracy: 0.9532, batch_loss: 0.1154, loss: 0.1364 ||:  43%|####2     | 3062/7188 [05:58<15:04,  4.56it/s]
2022-03-21 10:33:47,595 - INFO - tqdm - f1: 0.9528, accuracy: 0.9528, batch_loss: 0.1655, loss: 0.1372 ||:  44%|####4     | 3172/7188 [06:08<13:35,  4.93it/s]
2022-03-21 10:33:57,654 - INFO - tqdm - f1: 0.9530, accuracy: 0.9530, batch_loss: 0.0083, loss: 0.1369 ||:  46%|####5     | 3292/7188 [06:18<04:23, 14.77it/s]
2022-03-21 10:34:07,735 - INFO - tqdm - f1: 0.9533, accuracy: 0.9533, batch_loss: 0.1195, loss: 0.1363 ||:  47%|####7     | 3396/7188 [06:29<04:26, 14.21it/s]
2022-03-21 10:34:17,778 - INFO - tqdm - f1: 0.9533, accuracy: 0.9533, batch_loss: 0.0540, loss: 0.1361 ||:  49%|####8     | 3494/7188 [06:39<09:33,  6.44it/s]
2022-03-21 10:34:27,781 - INFO - tqdm - f1: 0.9531, accuracy: 0.9531, batch_loss: 0.1790, loss: 0.1369 ||:  50%|#####     | 3606/7188 [06:49<04:05, 14.60it/s]
2022-03-21 10:34:37,803 - INFO - tqdm - f1: 0.9531, accuracy: 0.9531, batch_loss: 0.1824, loss: 0.1370 ||:  52%|#####1    | 3710/7188 [06:59<03:55, 14.74it/s]
2022-03-21 10:34:47,897 - INFO - tqdm - f1: 0.9526, accuracy: 0.9526, batch_loss: 0.7649, loss: 0.1376 ||:  53%|#####3    | 3816/7188 [07:09<03:49, 14.68it/s]
2022-03-21 10:34:57,981 - INFO - tqdm - f1: 0.9523, accuracy: 0.9522, batch_loss: 0.2093, loss: 0.1386 ||:  55%|#####4    | 3924/7188 [07:19<06:18,  8.63it/s]
2022-03-21 10:35:08,036 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0380, loss: 0.1392 ||:  56%|#####6    | 4028/7188 [07:29<11:52,  4.44it/s]
2022-03-21 10:35:18,482 - INFO - tqdm - f1: 0.9521, accuracy: 0.9520, batch_loss: 0.1111, loss: 0.1393 ||:  58%|#####7    | 4140/7188 [07:39<11:22,  4.47it/s]
2022-03-21 10:35:29,561 - INFO - tqdm - f1: 0.9519, accuracy: 0.9518, batch_loss: 0.1438, loss: 0.1401 ||:  59%|#####9    | 4260/7188 [07:50<10:53,  4.48it/s]
2022-03-21 10:35:40,558 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.0502, loss: 0.1398 ||:  61%|######    | 4380/7188 [08:01<10:25,  4.49it/s]
2022-03-21 10:35:51,508 - INFO - tqdm - f1: 0.9522, accuracy: 0.9522, batch_loss: 0.0169, loss: 0.1396 ||:  63%|######2   | 4500/7188 [08:12<10:01,  4.47it/s]
2022-03-21 10:36:02,441 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0810, loss: 0.1398 ||:  64%|######4   | 4618/7188 [08:23<09:21,  4.58it/s]
2022-03-21 10:36:12,484 - INFO - tqdm - f1: 0.9521, accuracy: 0.9520, batch_loss: 0.0092, loss: 0.1398 ||:  66%|######5   | 4734/7188 [08:33<02:52, 14.23it/s]
2022-03-21 10:36:22,561 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0332, loss: 0.1400 ||:  67%|######7   | 4838/7188 [08:43<02:49, 13.87it/s]
2022-03-21 10:36:32,618 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0522, loss: 0.1400 ||:  69%|######8   | 4942/7188 [08:53<03:19, 11.23it/s]
2022-03-21 10:36:43,519 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.0909, loss: 0.1402 ||:  70%|#######   | 5058/7188 [09:04<07:27,  4.75it/s]
2022-03-21 10:36:54,345 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.1842, loss: 0.1403 ||:  72%|#######1  | 5174/7188 [09:15<07:16,  4.62it/s]
2022-03-21 10:37:05,102 - INFO - tqdm - f1: 0.9521, accuracy: 0.9520, batch_loss: 0.0114, loss: 0.1403 ||:  74%|#######3  | 5292/7188 [09:26<07:07,  4.44it/s]
2022-03-21 10:37:15,223 - INFO - tqdm - f1: 0.9521, accuracy: 0.9520, batch_loss: 0.1880, loss: 0.1400 ||:  75%|#######5  | 5406/7188 [09:36<02:09, 13.80it/s]
2022-03-21 10:37:25,858 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.2164, loss: 0.1403 ||:  77%|#######6  | 5512/7188 [09:47<06:31,  4.28it/s]
2022-03-21 10:37:35,861 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.0065, loss: 0.1404 ||:  78%|#######8  | 5626/7188 [09:57<01:52, 13.91it/s]
2022-03-21 10:37:46,077 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.0848, loss: 0.1404 ||:  80%|#######9  | 5728/7188 [10:07<05:41,  4.27it/s]
2022-03-21 10:37:56,119 - INFO - tqdm - f1: 0.9521, accuracy: 0.9520, batch_loss: 0.2714, loss: 0.1403 ||:  81%|########1 | 5842/7188 [10:17<01:33, 14.40it/s]
2022-03-21 10:38:06,158 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.4368, loss: 0.1404 ||:  83%|########2 | 5942/7188 [10:27<05:01,  4.13it/s]
2022-03-21 10:38:16,252 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0625, loss: 0.1404 ||:  84%|########4 | 6060/7188 [10:37<01:16, 14.70it/s]
2022-03-21 10:38:26,329 - INFO - tqdm - f1: 0.9520, accuracy: 0.9519, batch_loss: 0.0277, loss: 0.1408 ||:  86%|########5 | 6166/7188 [10:47<01:08, 14.85it/s]
2022-03-21 10:38:36,390 - INFO - tqdm - f1: 0.9521, accuracy: 0.9520, batch_loss: 0.0843, loss: 0.1404 ||:  87%|########7 | 6272/7188 [10:57<00:59, 15.34it/s]
2022-03-21 10:38:46,444 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.1222, loss: 0.1405 ||:  89%|########8 | 6380/7188 [11:07<00:45, 17.67it/s]
2022-03-21 10:38:57,591 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.0684, loss: 0.1409 ||:  90%|######### | 6490/7188 [11:18<02:29,  4.68it/s]
2022-03-21 10:39:08,584 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.2967, loss: 0.1408 ||:  92%|#########1| 6608/7188 [11:29<02:09,  4.46it/s]
2022-03-21 10:39:18,678 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.2239, loss: 0.1407 ||:  94%|#########3| 6726/7188 [11:39<00:30, 14.93it/s]
2022-03-21 10:39:28,854 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.1026, loss: 0.1406 ||:  95%|#########5| 6832/7188 [11:50<00:27, 13.13it/s]
2022-03-21 10:39:38,984 - INFO - tqdm - f1: 0.9520, accuracy: 0.9520, batch_loss: 0.2785, loss: 0.1406 ||:  97%|#########6| 6938/7188 [12:00<00:17, 14.42it/s]
2022-03-21 10:39:48,988 - INFO - tqdm - f1: 0.9520, accuracy: 0.9519, batch_loss: 0.0466, loss: 0.1406 ||:  98%|#########7| 7034/7188 [12:10<00:17,  8.86it/s]
2022-03-21 10:39:59,088 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0160, loss: 0.1403 ||:  99%|#########9| 7145/7188 [12:20<00:04,  9.80it/s]
2022-03-21 10:39:59,965 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.2150, loss: 0.1402 ||: 100%|#########9| 7154/7188 [12:21<00:03, 10.84it/s]
2022-03-21 10:40:00,177 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.2235, loss: 0.1402 ||: 100%|#########9| 7156/7188 [12:21<00:03, 10.31it/s]
2022-03-21 10:40:00,418 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0449, loss: 0.1402 ||: 100%|#########9| 7158/7188 [12:21<00:03,  9.55it/s]
2022-03-21 10:40:00,528 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.3524, loss: 0.1402 ||: 100%|#########9| 7159/7188 [12:21<00:03,  9.46it/s]
2022-03-21 10:40:00,641 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.1384, loss: 0.1402 ||: 100%|#########9| 7160/7188 [12:21<00:03,  9.33it/s]
2022-03-21 10:40:00,758 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.3368, loss: 0.1402 ||: 100%|#########9| 7161/7188 [12:22<00:02,  9.13it/s]
2022-03-21 10:40:00,867 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.1075, loss: 0.1402 ||: 100%|#########9| 7162/7188 [12:22<00:02,  9.14it/s]
2022-03-21 10:40:00,990 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.3836, loss: 0.1403 ||: 100%|#########9| 7163/7188 [12:22<00:02,  8.85it/s]
2022-03-21 10:40:01,090 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0072, loss: 0.1402 ||: 100%|#########9| 7164/7188 [12:22<00:02,  9.13it/s]
2022-03-21 10:40:01,208 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0382, loss: 0.1402 ||: 100%|#########9| 7165/7188 [12:22<00:02,  8.95it/s]
2022-03-21 10:40:01,377 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.1129, loss: 0.1402 ||: 100%|#########9| 7167/7188 [12:22<00:02, 10.04it/s]
2022-03-21 10:40:01,483 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.4045, loss: 0.1402 ||: 100%|#########9| 7168/7188 [12:22<00:02,  9.87it/s]
2022-03-21 10:40:01,600 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0043, loss: 0.1402 ||: 100%|#########9| 7169/7188 [12:22<00:01,  9.50it/s]
2022-03-21 10:40:01,703 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.1846, loss: 0.1402 ||: 100%|#########9| 7170/7188 [12:22<00:01,  9.57it/s]
2022-03-21 10:40:01,932 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.1390, loss: 0.1402 ||: 100%|#########9| 7172/7188 [12:23<00:01,  9.17it/s]
2022-03-21 10:40:02,089 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0064, loss: 0.1402 ||: 100%|#########9| 7173/7188 [12:23<00:01,  8.32it/s]
2022-03-21 10:40:02,202 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.1318, loss: 0.1402 ||: 100%|#########9| 7174/7188 [12:23<00:01,  8.44it/s]
2022-03-21 10:40:02,315 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0079, loss: 0.1402 ||: 100%|#########9| 7175/7188 [12:23<00:01,  8.55it/s]
2022-03-21 10:40:02,446 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.1047, loss: 0.1402 ||: 100%|#########9| 7176/7188 [12:23<00:01,  8.28it/s]
2022-03-21 10:40:02,644 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.5057, loss: 0.1402 ||: 100%|#########9| 7178/7188 [12:23<00:01,  8.99it/s]
2022-03-21 10:40:02,858 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.1252, loss: 0.1403 ||: 100%|#########9| 7180/7188 [12:24<00:00,  9.12it/s]
2022-03-21 10:40:02,973 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0222, loss: 0.1402 ||: 100%|#########9| 7181/7188 [12:24<00:00,  9.03it/s]
2022-03-21 10:40:03,186 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0270, loss: 0.1402 ||: 100%|#########9| 7183/7188 [12:24<00:00,  9.16it/s]
2022-03-21 10:40:03,331 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0181, loss: 0.1402 ||: 100%|#########9| 7184/7188 [12:24<00:00,  8.56it/s]
2022-03-21 10:40:03,448 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0969, loss: 0.1402 ||: 100%|#########9| 7185/7188 [12:24<00:00,  8.56it/s]
2022-03-21 10:40:03,571 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.1918, loss: 0.1402 ||: 100%|#########9| 7186/7188 [12:24<00:00,  8.44it/s]
2022-03-21 10:40:03,675 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.2188, loss: 0.1402 ||: 100%|#########9| 7187/7188 [12:24<00:00,  8.72it/s]
2022-03-21 10:40:03,780 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0150, loss: 0.1402 ||: 100%|##########| 7188/7188 [12:25<00:00,  8.93it/s]
2022-03-21 10:40:03,827 - INFO - tqdm - f1: 0.9521, accuracy: 0.9521, batch_loss: 0.0150, loss: 0.1402 ||: 100%|##########| 7188/7188 [12:25<00:00,  9.65it/s]
2022-03-21 10:40:03,835 - INFO - allennlp.training.trainer - Validating
2022-03-21 10:40:03,847 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 10:40:13,866 - INFO - tqdm - f1: 0.9412, accuracy: 0.9408, batch_loss: 0.0129, loss: 0.2039 ||:  51%|#####1    | 160/313 [00:09<00:08, 17.57it/s]
2022-03-21 10:40:23,990 - INFO - tqdm - f1: 0.9418, accuracy: 0.9418, batch_loss: 0.2781, loss: 0.2026 ||:  99%|#########9| 310/313 [00:20<00:00, 10.66it/s]
2022-03-21 10:40:24,213 - INFO - tqdm - f1: 0.9422, accuracy: 0.9422, batch_loss: 0.0186, loss: 0.2015 ||: 100%|#########9| 312/313 [00:20<00:00, 10.09it/s]
2022-03-21 10:40:24,368 - INFO - tqdm - f1: 0.9423, accuracy: 0.9422, batch_loss: 0.2423, loss: 0.2016 ||: 100%|##########| 313/313 [00:20<00:00, 15.29it/s]
2022-03-21 10:40:24,404 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/ag_base_hyper_small_seed_47/best.th'.
2022-03-21 10:40:28,962 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 10:40:28,978 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.952  |     0.942
2022-03-21 10:40:28,992 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.952  |     0.942
2022-03-21 10:40:29,007 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 10:40:29,021 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.140  |     0.202
2022-03-21 10:40:29,037 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7574.758  |       N/A
2022-03-21 10:40:29,054 - INFO - allennlp.training.trainer - Epoch duration: 0:12:50.423743
2022-03-21 10:40:29,068 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:15:50
2022-03-21 10:40:29,090 - INFO - allennlp.training.trainer - Epoch 3/9
2022-03-21 10:40:29,097 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-21 10:40:29,112 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 10:40:29,127 - INFO - allennlp.training.trainer - Training
2022-03-21 10:40:29,142 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 10:40:39,162 - INFO - tqdm - f1: 0.9568, accuracy: 0.9577, batch_loss: 0.0645, loss: 0.1184 ||:   0%|          | 34/7188 [00:10<20:10,  5.91it/s]
2022-03-21 10:40:49,164 - INFO - tqdm - f1: 0.9673, accuracy: 0.9677, batch_loss: 0.1739, loss: 0.0971 ||:   1%|1         | 91/7188 [00:20<21:47,  5.43it/s]
2022-03-21 10:40:59,166 - INFO - tqdm - f1: 0.9641, accuracy: 0.9639, batch_loss: 0.0695, loss: 0.1075 ||:   2%|2         | 154/7188 [00:30<18:42,  6.27it/s]
2022-03-21 10:41:09,274 - INFO - tqdm - f1: 0.9637, accuracy: 0.9638, batch_loss: 0.3511, loss: 0.1058 ||:   3%|3         | 226/7188 [00:40<16:08,  7.19it/s]
2022-03-21 10:41:19,405 - INFO - tqdm - f1: 0.9628, accuracy: 0.9632, batch_loss: 0.0242, loss: 0.1086 ||:   4%|4         | 297/7188 [00:50<17:35,  6.53it/s]
2022-03-21 10:41:29,545 - INFO - tqdm - f1: 0.9625, accuracy: 0.9628, batch_loss: 0.2507, loss: 0.1114 ||:   5%|5         | 373/7188 [01:00<16:13,  7.00it/s]
2022-03-21 10:41:39,646 - INFO - tqdm - f1: 0.9627, accuracy: 0.9628, batch_loss: 0.1468, loss: 0.1113 ||:   6%|6         | 462/7188 [01:10<10:38, 10.54it/s]
2022-03-21 10:41:49,705 - INFO - tqdm - f1: 0.9642, accuracy: 0.9643, batch_loss: 0.0241, loss: 0.1081 ||:   8%|7         | 563/7188 [01:20<11:24,  9.68it/s]
2022-03-21 10:41:59,784 - INFO - tqdm - f1: 0.9643, accuracy: 0.9644, batch_loss: 0.0026, loss: 0.1057 ||:   9%|8         | 639/7188 [01:30<15:54,  6.86it/s]
2022-03-21 10:42:09,890 - INFO - tqdm - f1: 0.9640, accuracy: 0.9641, batch_loss: 0.0214, loss: 0.1063 ||:  10%|9         | 706/7188 [01:40<16:23,  6.59it/s]
2022-03-21 10:42:19,997 - INFO - tqdm - f1: 0.9642, accuracy: 0.9643, batch_loss: 0.1799, loss: 0.1059 ||:  11%|#         | 775/7188 [01:50<13:02,  8.19it/s]
2022-03-21 10:42:30,010 - INFO - tqdm - f1: 0.9637, accuracy: 0.9637, batch_loss: 0.0051, loss: 0.1054 ||:  12%|#1        | 850/7188 [02:00<14:57,  7.06it/s]
2022-03-21 10:42:40,121 - INFO - tqdm - f1: 0.9633, accuracy: 0.9631, batch_loss: 0.3165, loss: 0.1061 ||:  13%|#2        | 919/7188 [02:10<13:48,  7.57it/s]
2022-03-21 10:42:50,236 - INFO - tqdm - f1: 0.9634, accuracy: 0.9633, batch_loss: 0.6901, loss: 0.1056 ||:  14%|#3        | 987/7188 [02:21<14:11,  7.28it/s]
2022-03-21 10:43:00,337 - INFO - tqdm - f1: 0.9633, accuracy: 0.9633, batch_loss: 0.0042, loss: 0.1054 ||:  15%|#4        | 1061/7188 [02:31<09:26, 10.82it/s]
2022-03-21 10:43:10,406 - INFO - tqdm - f1: 0.9630, accuracy: 0.9630, batch_loss: 0.0999, loss: 0.1059 ||:  16%|#6        | 1165/7188 [02:41<08:58, 11.18it/s]
2022-03-21 10:43:20,481 - INFO - tqdm - f1: 0.9635, accuracy: 0.9636, batch_loss: 0.0103, loss: 0.1049 ||:  17%|#7        | 1237/7188 [02:51<15:33,  6.38it/s]
2022-03-21 10:43:30,549 - INFO - tqdm - f1: 0.9634, accuracy: 0.9634, batch_loss: 0.0129, loss: 0.1058 ||:  18%|#8        | 1308/7188 [03:01<11:09,  8.79it/s]
2022-03-21 10:43:40,592 - INFO - tqdm - f1: 0.9634, accuracy: 0.9634, batch_loss: 0.1384, loss: 0.1059 ||:  19%|#9        | 1377/7188 [03:11<14:11,  6.82it/s]
2022-03-21 10:43:50,741 - INFO - tqdm - f1: 0.9633, accuracy: 0.9632, batch_loss: 0.1583, loss: 0.1062 ||:  20%|##        | 1449/7188 [03:21<13:31,  7.07it/s]
2022-03-21 10:44:00,989 - INFO - tqdm - f1: 0.9633, accuracy: 0.9632, batch_loss: 0.1498, loss: 0.1064 ||:  21%|##1       | 1517/7188 [03:31<17:17,  5.47it/s]
2022-03-21 10:44:11,118 - INFO - tqdm - f1: 0.9631, accuracy: 0.9630, batch_loss: 0.3048, loss: 0.1074 ||:  22%|##1       | 1573/7188 [03:41<17:32,  5.33it/s]
2022-03-21 10:44:21,194 - INFO - tqdm - f1: 0.9632, accuracy: 0.9631, batch_loss: 0.1333, loss: 0.1075 ||:  23%|##2       | 1626/7188 [03:52<15:06,  6.14it/s]
2022-03-21 10:44:31,310 - INFO - tqdm - f1: 0.9631, accuracy: 0.9631, batch_loss: 0.0491, loss: 0.1069 ||:  24%|##3       | 1691/7188 [04:02<08:21, 10.95it/s]
2022-03-21 10:44:41,328 - INFO - tqdm - f1: 0.9630, accuracy: 0.9631, batch_loss: 0.1777, loss: 0.1069 ||:  25%|##4       | 1790/7188 [04:12<12:18,  7.31it/s]
2022-03-21 10:44:51,446 - INFO - tqdm - f1: 0.9629, accuracy: 0.9629, batch_loss: 0.1044, loss: 0.1080 ||:  26%|##5       | 1862/7188 [04:22<07:18, 12.15it/s]
2022-03-21 10:45:01,599 - INFO - tqdm - f1: 0.9627, accuracy: 0.9627, batch_loss: 0.0356, loss: 0.1086 ||:  28%|##7       | 1983/7188 [04:32<08:28, 10.24it/s]
2022-03-21 10:45:11,710 - INFO - tqdm - f1: 0.9617, accuracy: 0.9617, batch_loss: 0.0108, loss: 0.1109 ||:  29%|##9       | 2093/7188 [04:42<07:46, 10.93it/s]
2022-03-21 10:45:21,735 - INFO - tqdm - f1: 0.9612, accuracy: 0.9612, batch_loss: 0.0646, loss: 0.1117 ||:  31%|###       | 2204/7188 [04:52<08:09, 10.19it/s]
2022-03-21 10:45:31,799 - INFO - tqdm - f1: 0.9608, accuracy: 0.9608, batch_loss: 0.1027, loss: 0.1128 ||:  32%|###2      | 2316/7188 [05:02<07:00, 11.60it/s]
2022-03-21 10:45:41,859 - INFO - tqdm - f1: 0.9609, accuracy: 0.9609, batch_loss: 0.0042, loss: 0.1124 ||:  34%|###3      | 2428/7188 [05:12<11:12,  7.08it/s]
2022-03-21 10:45:52,713 - INFO - tqdm - f1: 0.9608, accuracy: 0.9608, batch_loss: 0.0484, loss: 0.1129 ||:  35%|###5      | 2538/7188 [05:23<17:26,  4.45it/s]
2022-03-21 10:46:03,018 - INFO - tqdm - f1: 0.9605, accuracy: 0.9604, batch_loss: 0.0177, loss: 0.1132 ||:  37%|###6      | 2650/7188 [05:33<16:32,  4.57it/s]
2022-03-21 10:46:13,029 - INFO - tqdm - f1: 0.9604, accuracy: 0.9603, batch_loss: 0.0241, loss: 0.1129 ||:  38%|###8      | 2758/7188 [05:43<16:32,  4.46it/s]
2022-03-21 10:46:23,100 - INFO - tqdm - f1: 0.9604, accuracy: 0.9604, batch_loss: 0.0798, loss: 0.1127 ||:  40%|###9      | 2868/7188 [05:53<15:39,  4.60it/s]
2022-03-21 10:46:33,291 - INFO - tqdm - f1: 0.9606, accuracy: 0.9606, batch_loss: 0.0153, loss: 0.1122 ||:  41%|####1     | 2978/7188 [06:04<15:21,  4.57it/s]
2022-03-21 10:46:43,376 - INFO - tqdm - f1: 0.9604, accuracy: 0.9603, batch_loss: 0.0943, loss: 0.1128 ||:  43%|####2     | 3086/7188 [06:14<11:43,  5.83it/s]
2022-03-21 10:46:53,504 - INFO - tqdm - f1: 0.9605, accuracy: 0.9604, batch_loss: 0.0516, loss: 0.1130 ||:  44%|####4     | 3196/7188 [06:24<11:08,  5.97it/s]
2022-03-21 10:47:03,553 - INFO - tqdm - f1: 0.9606, accuracy: 0.9605, batch_loss: 0.0616, loss: 0.1127 ||:  46%|####5     | 3306/7188 [06:34<09:26,  6.86it/s]
2022-03-21 10:47:13,658 - INFO - tqdm - f1: 0.9608, accuracy: 0.9607, batch_loss: 0.2385, loss: 0.1125 ||:  47%|####7     | 3414/7188 [06:44<10:38,  5.92it/s]
2022-03-21 10:47:23,676 - INFO - tqdm - f1: 0.9609, accuracy: 0.9608, batch_loss: 0.0201, loss: 0.1119 ||:  49%|####8     | 3522/7188 [06:54<13:12,  4.63it/s]
2022-03-21 10:47:34,318 - INFO - tqdm - f1: 0.9609, accuracy: 0.9608, batch_loss: 0.0844, loss: 0.1117 ||:  51%|#####     | 3638/7188 [07:05<12:58,  4.56it/s]
2022-03-21 10:47:44,978 - INFO - tqdm - f1: 0.9609, accuracy: 0.9608, batch_loss: 0.0332, loss: 0.1117 ||:  52%|#####2    | 3754/7188 [07:15<12:13,  4.68it/s]
2022-03-21 10:47:55,081 - INFO - tqdm - f1: 0.9608, accuracy: 0.9608, batch_loss: 0.1589, loss: 0.1119 ||:  54%|#####3    | 3862/7188 [07:25<09:22,  5.91it/s]
2022-03-21 10:48:05,088 - INFO - tqdm - f1: 0.9608, accuracy: 0.9608, batch_loss: 0.0209, loss: 0.1118 ||:  55%|#####5    | 3970/7188 [07:35<07:36,  7.05it/s]
2022-03-21 10:48:15,201 - INFO - tqdm - f1: 0.9609, accuracy: 0.9608, batch_loss: 0.1994, loss: 0.1118 ||:  57%|#####6    | 4080/7188 [07:46<07:13,  7.17it/s]
2022-03-21 10:48:25,297 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0077, loss: 0.1117 ||:  58%|#####8    | 4190/7188 [07:56<05:58,  8.36it/s]
2022-03-21 10:48:35,324 - INFO - tqdm - f1: 0.9608, accuracy: 0.9607, batch_loss: 0.5574, loss: 0.1122 ||:  60%|#####9    | 4296/7188 [08:06<10:49,  4.46it/s]
2022-03-21 10:48:45,415 - INFO - tqdm - f1: 0.9609, accuracy: 0.9608, batch_loss: 0.0357, loss: 0.1122 ||:  61%|######1   | 4406/7188 [08:16<10:11,  4.55it/s]
2022-03-21 10:48:55,576 - INFO - tqdm - f1: 0.9609, accuracy: 0.9608, batch_loss: 0.0186, loss: 0.1121 ||:  63%|######2   | 4514/7188 [08:26<10:01,  4.44it/s]
2022-03-21 10:49:05,634 - INFO - tqdm - f1: 0.9611, accuracy: 0.9610, batch_loss: 0.0229, loss: 0.1116 ||:  64%|######4   | 4622/7188 [08:36<09:13,  4.63it/s]
2022-03-21 10:49:15,731 - INFO - tqdm - f1: 0.9611, accuracy: 0.9610, batch_loss: 0.1337, loss: 0.1120 ||:  66%|######5   | 4732/7188 [08:46<09:16,  4.41it/s]
2022-03-21 10:49:26,140 - INFO - tqdm - f1: 0.9612, accuracy: 0.9612, batch_loss: 0.0052, loss: 0.1115 ||:  67%|######7   | 4844/7188 [08:56<08:19,  4.69it/s]
2022-03-21 10:49:36,484 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.1104, loss: 0.1117 ||:  69%|######8   | 4956/7188 [09:07<08:22,  4.44it/s]
2022-03-21 10:49:47,183 - INFO - tqdm - f1: 0.9612, accuracy: 0.9611, batch_loss: 0.0222, loss: 0.1119 ||:  71%|#######   | 5070/7188 [09:18<07:55,  4.46it/s]
2022-03-21 10:49:57,214 - INFO - tqdm - f1: 0.9612, accuracy: 0.9612, batch_loss: 0.0234, loss: 0.1119 ||:  72%|#######2  | 5188/7188 [09:28<02:09, 15.43it/s]
2022-03-21 10:50:07,227 - INFO - tqdm - f1: 0.9612, accuracy: 0.9612, batch_loss: 0.0302, loss: 0.1117 ||:  74%|#######3  | 5294/7188 [09:38<02:04, 15.20it/s]
2022-03-21 10:50:17,346 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.4189, loss: 0.1120 ||:  75%|#######5  | 5404/7188 [09:48<02:09, 13.74it/s]
2022-03-21 10:50:27,396 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.1844, loss: 0.1122 ||:  77%|#######6  | 5512/7188 [09:58<02:07, 13.10it/s]
2022-03-21 10:50:37,475 - INFO - tqdm - f1: 0.9611, accuracy: 0.9611, batch_loss: 0.0395, loss: 0.1118 ||:  78%|#######8  | 5618/7188 [10:08<02:26, 10.74it/s]
2022-03-21 10:50:47,594 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0253, loss: 0.1119 ||:  80%|#######9  | 5730/7188 [10:18<01:58, 12.31it/s]
2022-03-21 10:50:57,625 - INFO - tqdm - f1: 0.9608, accuracy: 0.9608, batch_loss: 0.0663, loss: 0.1124 ||:  81%|########1 | 5836/7188 [10:28<02:00, 11.26it/s]
2022-03-21 10:51:07,668 - INFO - tqdm - f1: 0.9608, accuracy: 0.9608, batch_loss: 0.0064, loss: 0.1124 ||:  83%|########2 | 5942/7188 [10:38<02:34,  8.08it/s]
2022-03-21 10:51:17,751 - INFO - tqdm - f1: 0.9609, accuracy: 0.9609, batch_loss: 0.0106, loss: 0.1122 ||:  84%|########4 | 6048/7188 [10:48<03:17,  5.78it/s]
2022-03-21 10:51:27,887 - INFO - tqdm - f1: 0.9609, accuracy: 0.9609, batch_loss: 0.2718, loss: 0.1125 ||:  86%|########5 | 6162/7188 [10:58<02:18,  7.39it/s]
2022-03-21 10:51:38,408 - INFO - tqdm - f1: 0.9611, accuracy: 0.9610, batch_loss: 0.3688, loss: 0.1121 ||:  87%|########7 | 6268/7188 [11:09<03:17,  4.66it/s]
2022-03-21 10:51:48,763 - INFO - tqdm - f1: 0.9611, accuracy: 0.9610, batch_loss: 0.0158, loss: 0.1121 ||:  89%|########8 | 6380/7188 [11:19<03:04,  4.38it/s]
2022-03-21 10:51:58,958 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0799, loss: 0.1123 ||:  90%|######### | 6492/7188 [11:29<02:27,  4.73it/s]
2022-03-21 10:52:09,213 - INFO - tqdm - f1: 0.9610, accuracy: 0.9609, batch_loss: 0.0353, loss: 0.1125 ||:  92%|#########1| 6602/7188 [11:40<02:06,  4.64it/s]
2022-03-21 10:52:19,554 - INFO - tqdm - f1: 0.9610, accuracy: 0.9609, batch_loss: 0.2604, loss: 0.1124 ||:  93%|#########3| 6716/7188 [11:50<01:41,  4.63it/s]
2022-03-21 10:52:30,189 - INFO - tqdm - f1: 0.9610, accuracy: 0.9609, batch_loss: 0.0752, loss: 0.1125 ||:  95%|#########4| 6828/7188 [12:01<01:20,  4.49it/s]
2022-03-21 10:52:40,290 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.1274, loss: 0.1124 ||:  97%|#########6| 6938/7188 [12:11<00:53,  4.66it/s]
2022-03-21 10:52:50,382 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0770, loss: 0.1125 ||:  98%|#########8| 7046/7188 [12:21<00:24,  5.87it/s]
2022-03-21 10:53:00,506 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0513, loss: 0.1125 ||:  99%|#########9| 7152/7188 [12:31<00:06,  5.78it/s]
2022-03-21 10:53:00,675 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0468, loss: 0.1125 ||: 100%|#########9| 7154/7188 [12:31<00:04,  6.83it/s]
2022-03-21 10:53:00,857 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0412, loss: 0.1125 ||: 100%|#########9| 7156/7188 [12:31<00:04,  7.70it/s]
2022-03-21 10:53:01,001 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0086, loss: 0.1125 ||: 100%|#########9| 7158/7188 [12:31<00:03,  8.89it/s]
2022-03-21 10:53:01,130 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0711, loss: 0.1125 ||: 100%|#########9| 7160/7188 [12:31<00:02, 10.21it/s]
2022-03-21 10:53:01,259 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.3042, loss: 0.1125 ||: 100%|#########9| 7162/7188 [12:32<00:02, 11.36it/s]
2022-03-21 10:53:01,403 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0413, loss: 0.1125 ||: 100%|#########9| 7164/7188 [12:32<00:01, 12.02it/s]
2022-03-21 10:53:01,556 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.3846, loss: 0.1125 ||: 100%|#########9| 7166/7188 [12:32<00:01, 12.32it/s]
2022-03-21 10:53:01,703 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0557, loss: 0.1125 ||: 100%|#########9| 7168/7188 [12:32<00:01, 12.68it/s]
2022-03-21 10:53:01,854 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.1070, loss: 0.1125 ||: 100%|#########9| 7170/7188 [12:32<00:01, 12.84it/s]
2022-03-21 10:53:02,008 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.1929, loss: 0.1125 ||: 100%|#########9| 7172/7188 [12:32<00:01, 12.88it/s]
2022-03-21 10:53:02,149 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0721, loss: 0.1125 ||: 100%|#########9| 7174/7188 [12:32<00:01, 13.27it/s]
2022-03-21 10:53:02,283 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.1950, loss: 0.1125 ||: 100%|#########9| 7176/7188 [12:33<00:00, 13.70it/s]
2022-03-21 10:53:02,418 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.1459, loss: 0.1125 ||: 100%|#########9| 7178/7188 [12:33<00:00, 14.04it/s]
2022-03-21 10:53:02,547 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.1309, loss: 0.1126 ||: 100%|#########9| 7180/7188 [12:33<00:00, 14.43it/s]
2022-03-21 10:53:02,702 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.1830, loss: 0.1125 ||: 100%|#########9| 7182/7188 [12:33<00:00, 13.95it/s]
2022-03-21 10:53:02,864 - INFO - tqdm - f1: 0.9610, accuracy: 0.9609, batch_loss: 0.0152, loss: 0.1125 ||: 100%|#########9| 7184/7188 [12:33<00:00, 13.42it/s]
2022-03-21 10:53:04,011 - INFO - tqdm - f1: 0.9610, accuracy: 0.9609, batch_loss: 0.0830, loss: 0.1125 ||: 100%|#########9| 7186/7188 [12:34<00:00,  4.46it/s]
2022-03-21 10:53:04,175 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0051, loss: 0.1125 ||: 100%|##########| 7188/7188 [12:35<00:00,  5.51it/s]
2022-03-21 10:53:04,222 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.0051, loss: 0.1125 ||: 100%|##########| 7188/7188 [12:35<00:00,  9.52it/s]
2022-03-21 10:53:04,231 - INFO - allennlp.training.trainer - Validating
2022-03-21 10:53:04,249 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 10:53:14,117 - INFO - tqdm - f1: 0.9385, accuracy: 0.9384, batch_loss: 0.1438, loss: 0.2049 ||: 100%|##########| 313/313 [00:09<00:00, 31.79it/s]
2022-03-21 10:53:14,178 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 10:53:14,188 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.961  |     0.938
2022-03-21 10:53:14,216 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.961  |     0.938
2022-03-21 10:53:14,233 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 10:53:14,238 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.113  |     0.205
2022-03-21 10:53:14,256 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7574.758  |       N/A
2022-03-21 10:53:14,271 - INFO - allennlp.training.trainer - Epoch duration: 0:12:45.181129
2022-03-21 10:53:14,292 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:07:53
2022-03-21 10:53:14,312 - INFO - allennlp.training.trainer - Epoch 4/9
2022-03-21 10:53:14,328 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-21 10:53:14,330 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 10:53:14,334 - INFO - allennlp.training.trainer - Training
2022-03-21 10:53:14,358 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 10:53:24,465 - INFO - tqdm - f1: 0.9773, accuracy: 0.9771, batch_loss: 0.0069, loss: 0.0610 ||:   1%|1         | 101/7188 [00:10<25:59,  4.54it/s]
2022-03-21 10:53:34,836 - INFO - tqdm - f1: 0.9749, accuracy: 0.9750, batch_loss: 0.2281, loss: 0.0709 ||:   3%|2         | 215/7188 [00:20<25:15,  4.60it/s]
2022-03-21 10:53:45,332 - INFO - tqdm - f1: 0.9692, accuracy: 0.9696, batch_loss: 0.1594, loss: 0.0835 ||:   5%|4         | 325/7188 [00:30<25:16,  4.53it/s]
2022-03-21 10:53:55,419 - INFO - tqdm - f1: 0.9694, accuracy: 0.9698, batch_loss: 0.0988, loss: 0.0855 ||:   6%|5         | 431/7188 [00:41<24:30,  4.59it/s]
2022-03-21 10:54:05,495 - INFO - tqdm - f1: 0.9713, accuracy: 0.9715, batch_loss: 0.2448, loss: 0.0801 ||:   7%|7         | 539/7188 [00:51<23:49,  4.65it/s]
2022-03-21 10:54:15,584 - INFO - tqdm - f1: 0.9710, accuracy: 0.9711, batch_loss: 0.0886, loss: 0.0795 ||:   9%|9         | 647/7188 [01:01<23:53,  4.56it/s]
2022-03-21 10:54:25,709 - INFO - tqdm - f1: 0.9719, accuracy: 0.9721, batch_loss: 0.0282, loss: 0.0773 ||:  11%|#         | 755/7188 [01:11<23:46,  4.51it/s]
2022-03-21 10:54:35,954 - INFO - tqdm - f1: 0.9727, accuracy: 0.9729, batch_loss: 0.0191, loss: 0.0780 ||:  12%|#2        | 865/7188 [01:21<22:39,  4.65it/s]
2022-03-21 10:54:46,135 - INFO - tqdm - f1: 0.9728, accuracy: 0.9729, batch_loss: 0.0060, loss: 0.0782 ||:  14%|#3        | 975/7188 [01:31<22:11,  4.66it/s]
2022-03-21 10:54:56,205 - INFO - tqdm - f1: 0.9729, accuracy: 0.9730, batch_loss: 0.0237, loss: 0.0783 ||:  15%|#5        | 1083/7188 [01:41<22:39,  4.49it/s]
2022-03-21 10:55:06,343 - INFO - tqdm - f1: 0.9730, accuracy: 0.9731, batch_loss: 0.4037, loss: 0.0793 ||:  17%|#6        | 1193/7188 [01:51<22:13,  4.50it/s]
2022-03-21 10:55:16,481 - INFO - tqdm - f1: 0.9732, accuracy: 0.9732, batch_loss: 0.0512, loss: 0.0786 ||:  18%|#8        | 1303/7188 [02:02<20:59,  4.67it/s]
2022-03-21 10:55:26,525 - INFO - tqdm - f1: 0.9732, accuracy: 0.9732, batch_loss: 0.1570, loss: 0.0793 ||:  20%|#9        | 1413/7188 [02:12<13:03,  7.38it/s]
2022-03-21 10:55:36,576 - INFO - tqdm - f1: 0.9727, accuracy: 0.9727, batch_loss: 0.0266, loss: 0.0803 ||:  21%|##1       | 1523/7188 [02:22<11:05,  8.52it/s]
2022-03-21 10:55:46,693 - INFO - tqdm - f1: 0.9726, accuracy: 0.9726, batch_loss: 0.0821, loss: 0.0810 ||:  23%|##2       | 1631/7188 [02:32<09:38,  9.60it/s]
2022-03-21 10:55:56,782 - INFO - tqdm - f1: 0.9725, accuracy: 0.9725, batch_loss: 0.0118, loss: 0.0817 ||:  24%|##4       | 1737/7188 [02:42<12:56,  7.02it/s]
2022-03-21 10:56:06,872 - INFO - tqdm - f1: 0.9723, accuracy: 0.9723, batch_loss: 0.1055, loss: 0.0829 ||:  26%|##5       | 1847/7188 [02:52<12:21,  7.21it/s]
2022-03-21 10:56:17,000 - INFO - tqdm - f1: 0.9724, accuracy: 0.9724, batch_loss: 0.0041, loss: 0.0832 ||:  27%|##7       | 1955/7188 [03:02<10:32,  8.28it/s]
2022-03-21 10:56:27,122 - INFO - tqdm - f1: 0.9725, accuracy: 0.9724, batch_loss: 0.2761, loss: 0.0837 ||:  29%|##8       | 2065/7188 [03:12<08:46,  9.73it/s]
2022-03-21 10:56:37,161 - INFO - tqdm - f1: 0.9721, accuracy: 0.9721, batch_loss: 0.0131, loss: 0.0843 ||:  30%|###       | 2173/7188 [03:22<11:54,  7.02it/s]
2022-03-21 10:56:47,195 - INFO - tqdm - f1: 0.9721, accuracy: 0.9720, batch_loss: 0.0682, loss: 0.0840 ||:  32%|###1      | 2283/7188 [03:32<11:15,  7.26it/s]
2022-03-21 10:56:57,323 - INFO - tqdm - f1: 0.9716, accuracy: 0.9716, batch_loss: 0.0741, loss: 0.0852 ||:  33%|###3      | 2393/7188 [03:42<17:38,  4.53it/s]
2022-03-21 10:57:07,561 - INFO - tqdm - f1: 0.9715, accuracy: 0.9714, batch_loss: 0.1912, loss: 0.0850 ||:  35%|###4      | 2503/7188 [03:53<16:57,  4.61it/s]
2022-03-21 10:57:17,691 - INFO - tqdm - f1: 0.9717, accuracy: 0.9717, batch_loss: 0.0046, loss: 0.0840 ||:  36%|###6      | 2611/7188 [04:03<17:13,  4.43it/s]
2022-03-21 10:57:27,788 - INFO - tqdm - f1: 0.9715, accuracy: 0.9715, batch_loss: 0.0729, loss: 0.0849 ||:  38%|###7      | 2721/7188 [04:13<12:32,  5.94it/s]
2022-03-21 10:57:37,890 - INFO - tqdm - f1: 0.9714, accuracy: 0.9714, batch_loss: 0.0109, loss: 0.0853 ||:  39%|###9      | 2831/7188 [04:23<12:43,  5.71it/s]
2022-03-21 10:57:47,905 - INFO - tqdm - f1: 0.9712, accuracy: 0.9712, batch_loss: 0.0732, loss: 0.0858 ||:  41%|####      | 2939/7188 [04:33<15:16,  4.64it/s]
2022-03-21 10:57:57,993 - INFO - tqdm - f1: 0.9711, accuracy: 0.9710, batch_loss: 0.0690, loss: 0.0863 ||:  42%|####2     | 3047/7188 [04:43<14:56,  4.62it/s]
2022-03-21 10:58:08,385 - INFO - tqdm - f1: 0.9710, accuracy: 0.9710, batch_loss: 0.0247, loss: 0.0864 ||:  44%|####3     | 3159/7188 [04:54<14:35,  4.60it/s]
2022-03-21 10:58:18,412 - INFO - tqdm - f1: 0.9707, accuracy: 0.9706, batch_loss: 0.0274, loss: 0.0867 ||:  45%|####5     | 3267/7188 [05:04<14:36,  4.47it/s]
2022-03-21 10:58:28,498 - INFO - tqdm - f1: 0.9705, accuracy: 0.9705, batch_loss: 0.0684, loss: 0.0869 ||:  47%|####6     | 3375/7188 [05:14<11:06,  5.72it/s]
2022-03-21 10:58:38,548 - INFO - tqdm - f1: 0.9705, accuracy: 0.9704, batch_loss: 0.0817, loss: 0.0868 ||:  48%|####8     | 3483/7188 [05:24<08:37,  7.16it/s]
2022-03-21 10:58:48,631 - INFO - tqdm - f1: 0.9703, accuracy: 0.9703, batch_loss: 0.0016, loss: 0.0869 ||:  50%|####9     | 3589/7188 [05:34<07:12,  8.32it/s]
2022-03-21 10:58:58,737 - INFO - tqdm - f1: 0.9703, accuracy: 0.9703, batch_loss: 0.2626, loss: 0.0867 ||:  51%|#####1    | 3697/7188 [05:44<06:06,  9.53it/s]
2022-03-21 10:59:08,757 - INFO - tqdm - f1: 0.9705, accuracy: 0.9705, batch_loss: 0.0175, loss: 0.0865 ||:  53%|#####2    | 3805/7188 [05:54<05:09, 10.93it/s]
2022-03-21 10:59:18,786 - INFO - tqdm - f1: 0.9703, accuracy: 0.9703, batch_loss: 0.0092, loss: 0.0869 ||:  54%|#####4    | 3913/7188 [06:04<04:50, 11.28it/s]
2022-03-21 10:59:28,830 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0403, loss: 0.0871 ||:  56%|#####5    | 4023/7188 [06:14<04:16, 12.36it/s]
2022-03-21 10:59:38,928 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0652, loss: 0.0873 ||:  57%|#####7    | 4131/7188 [06:24<03:50, 13.26it/s]
2022-03-21 10:59:48,962 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0028, loss: 0.0870 ||:  59%|#####8    | 4239/7188 [06:34<03:30, 14.02it/s]
2022-03-21 10:59:59,080 - INFO - tqdm - f1: 0.9701, accuracy: 0.9701, batch_loss: 0.0337, loss: 0.0871 ||:  60%|######    | 4345/7188 [06:44<03:17, 14.36it/s]
2022-03-21 11:00:09,129 - INFO - tqdm - f1: 0.9702, accuracy: 0.9703, batch_loss: 0.0620, loss: 0.0871 ||:  62%|######1   | 4451/7188 [06:54<03:01, 15.09it/s]
2022-03-21 11:00:19,195 - INFO - tqdm - f1: 0.9703, accuracy: 0.9704, batch_loss: 0.0728, loss: 0.0870 ||:  63%|######3   | 4557/7188 [07:04<03:05, 14.20it/s]
2022-03-21 11:00:29,250 - INFO - tqdm - f1: 0.9703, accuracy: 0.9703, batch_loss: 0.1739, loss: 0.0874 ||:  65%|######4   | 4667/7188 [07:14<02:40, 15.69it/s]
2022-03-21 11:00:39,309 - INFO - tqdm - f1: 0.9703, accuracy: 0.9703, batch_loss: 0.0265, loss: 0.0872 ||:  66%|######6   | 4777/7188 [07:24<02:42, 14.80it/s]
2022-03-21 11:00:49,428 - INFO - tqdm - f1: 0.9702, accuracy: 0.9702, batch_loss: 0.0085, loss: 0.0876 ||:  68%|######7   | 4887/7188 [07:35<02:44, 13.96it/s]
2022-03-21 11:00:59,459 - INFO - tqdm - f1: 0.9703, accuracy: 0.9703, batch_loss: 0.5923, loss: 0.0874 ||:  69%|######9   | 4995/7188 [07:45<02:32, 14.41it/s]
2022-03-21 11:01:09,519 - INFO - tqdm - f1: 0.9701, accuracy: 0.9701, batch_loss: 0.1119, loss: 0.0879 ||:  71%|#######   | 5099/7188 [07:55<02:27, 14.13it/s]
2022-03-21 11:01:19,548 - INFO - tqdm - f1: 0.9700, accuracy: 0.9700, batch_loss: 0.0331, loss: 0.0881 ||:  72%|#######2  | 5203/7188 [08:05<02:21, 14.00it/s]
2022-03-21 11:01:29,668 - INFO - tqdm - f1: 0.9699, accuracy: 0.9699, batch_loss: 0.1956, loss: 0.0882 ||:  74%|#######3  | 5311/7188 [08:15<02:08, 14.58it/s]
2022-03-21 11:01:39,771 - INFO - tqdm - f1: 0.9699, accuracy: 0.9699, batch_loss: 0.0206, loss: 0.0883 ||:  75%|#######5  | 5419/7188 [08:25<01:56, 15.16it/s]
2022-03-21 11:01:49,871 - INFO - tqdm - f1: 0.9697, accuracy: 0.9697, batch_loss: 0.0978, loss: 0.0887 ||:  77%|#######6  | 5527/7188 [08:35<01:54, 14.47it/s]
2022-03-21 11:01:59,960 - INFO - tqdm - f1: 0.9697, accuracy: 0.9697, batch_loss: 0.0059, loss: 0.0889 ||:  78%|#######8  | 5635/7188 [08:45<01:45, 14.73it/s]
2022-03-21 11:02:10,005 - INFO - tqdm - f1: 0.9694, accuracy: 0.9694, batch_loss: 0.1543, loss: 0.0893 ||:  80%|#######9  | 5743/7188 [08:55<01:34, 15.32it/s]
2022-03-21 11:02:20,044 - INFO - tqdm - f1: 0.9695, accuracy: 0.9695, batch_loss: 0.0039, loss: 0.0893 ||:  81%|########1 | 5851/7188 [09:05<01:30, 14.79it/s]
2022-03-21 11:02:30,144 - INFO - tqdm - f1: 0.9694, accuracy: 0.9694, batch_loss: 0.0580, loss: 0.0894 ||:  83%|########2 | 5961/7188 [09:15<01:22, 14.90it/s]
2022-03-21 11:02:40,203 - INFO - tqdm - f1: 0.9693, accuracy: 0.9693, batch_loss: 0.0422, loss: 0.0897 ||:  84%|########4 | 6069/7188 [09:25<01:12, 15.36it/s]
2022-03-21 11:02:50,301 - INFO - tqdm - f1: 0.9693, accuracy: 0.9693, batch_loss: 0.3748, loss: 0.0898 ||:  86%|########5 | 6179/7188 [09:35<01:05, 15.44it/s]
2022-03-21 11:03:00,339 - INFO - tqdm - f1: 0.9692, accuracy: 0.9691, batch_loss: 0.7159, loss: 0.0904 ||:  87%|########7 | 6287/7188 [09:45<00:58, 15.38it/s]
2022-03-21 11:03:10,456 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0369, loss: 0.0903 ||:  89%|########8 | 6397/7188 [09:56<00:47, 16.67it/s]
2022-03-21 11:03:21,507 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.0174, loss: 0.0905 ||:  90%|######### | 6501/7188 [10:07<02:34,  4.45it/s]
2022-03-21 11:03:31,616 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.0124, loss: 0.0904 ||:  92%|#########1| 6611/7188 [10:17<02:05,  4.59it/s]
2022-03-21 11:03:41,703 - INFO - tqdm - f1: 0.9690, accuracy: 0.9690, batch_loss: 0.0282, loss: 0.0908 ||:  93%|#########3| 6719/7188 [10:27<01:42,  4.58it/s]
2022-03-21 11:03:51,779 - INFO - tqdm - f1: 0.9691, accuracy: 0.9690, batch_loss: 0.0189, loss: 0.0908 ||:  95%|#########4| 6825/7188 [10:37<01:02,  5.85it/s]
2022-03-21 11:04:01,799 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0146, loss: 0.0906 ||:  96%|#########6| 6933/7188 [10:47<00:43,  5.81it/s]
2022-03-21 11:04:11,889 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.2363, loss: 0.0910 ||:  98%|#########7| 7043/7188 [10:57<00:20,  7.11it/s]
2022-03-21 11:04:22,031 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.1275, loss: 0.0908 ||:  99%|#########9| 7149/7188 [11:07<00:04,  8.30it/s]
2022-03-21 11:04:22,328 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.2691, loss: 0.0908 ||: 100%|#########9| 7153/7188 [11:07<00:03, 10.40it/s]
2022-03-21 11:04:22,459 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0084, loss: 0.0908 ||: 100%|#########9| 7155/7188 [11:08<00:02, 11.50it/s]
2022-03-21 11:04:22,608 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0160, loss: 0.0908 ||: 100%|#########9| 7157/7188 [11:08<00:02, 12.02it/s]
2022-03-21 11:04:22,747 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0988, loss: 0.0908 ||: 100%|#########9| 7159/7188 [11:08<00:02, 12.64it/s]
2022-03-21 11:04:22,911 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0747, loss: 0.0908 ||: 100%|#########9| 7161/7188 [11:08<00:02, 12.49it/s]
2022-03-21 11:04:23,064 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.4305, loss: 0.0909 ||: 100%|#########9| 7163/7188 [11:08<00:01, 12.66it/s]
2022-03-21 11:04:23,226 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.3284, loss: 0.0909 ||: 100%|#########9| 7165/7188 [11:08<00:01, 12.58it/s]
2022-03-21 11:04:23,372 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0622, loss: 0.0909 ||: 100%|#########9| 7167/7188 [11:09<00:01, 12.87it/s]
2022-03-21 11:04:23,537 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.4420, loss: 0.0909 ||: 100%|#########9| 7169/7188 [11:09<00:01, 12.66it/s]
2022-03-21 11:04:23,676 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.0022, loss: 0.0909 ||: 100%|#########9| 7171/7188 [11:09<00:01, 13.12it/s]
2022-03-21 11:04:23,813 - INFO - tqdm - f1: 0.9692, accuracy: 0.9692, batch_loss: 0.1109, loss: 0.0909 ||: 100%|#########9| 7173/7188 [11:09<00:01, 13.54it/s]
2022-03-21 11:04:24,945 - INFO - tqdm - f1: 0.9692, accuracy: 0.9691, batch_loss: 0.1364, loss: 0.0909 ||: 100%|#########9| 7175/7188 [11:10<00:02,  4.52it/s]
2022-03-21 11:04:25,092 - INFO - tqdm - f1: 0.9692, accuracy: 0.9691, batch_loss: 0.1774, loss: 0.0909 ||: 100%|#########9| 7177/7188 [11:10<00:01,  5.64it/s]
2022-03-21 11:04:25,232 - INFO - tqdm - f1: 0.9692, accuracy: 0.9691, batch_loss: 0.0955, loss: 0.0909 ||: 100%|#########9| 7179/7188 [11:10<00:01,  6.90it/s]
2022-03-21 11:04:25,386 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.4047, loss: 0.0910 ||: 100%|#########9| 7181/7188 [11:11<00:00,  8.03it/s]
2022-03-21 11:04:25,529 - INFO - tqdm - f1: 0.9692, accuracy: 0.9691, batch_loss: 0.0170, loss: 0.0910 ||: 100%|#########9| 7183/7188 [11:11<00:00,  9.20it/s]
2022-03-21 11:04:25,668 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.0984, loss: 0.0910 ||: 100%|#########9| 7185/7188 [11:11<00:00, 10.32it/s]
2022-03-21 11:04:25,804 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.0173, loss: 0.0910 ||: 100%|#########9| 7187/7188 [11:11<00:00, 11.34it/s]
2022-03-21 11:04:25,922 - INFO - tqdm - f1: 0.9691, accuracy: 0.9691, batch_loss: 0.1616, loss: 0.0910 ||: 100%|##########| 7188/7188 [11:11<00:00, 10.70it/s]
2022-03-21 11:04:25,933 - INFO - allennlp.training.trainer - Validating
2022-03-21 11:04:25,945 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 11:04:35,812 - INFO - tqdm - f1: 0.9360, accuracy: 0.9360, batch_loss: 0.0832, loss: 0.2421 ||: 100%|##########| 313/313 [00:09<00:00, 31.73it/s]
2022-03-21 11:04:35,906 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-21 11:04:35,920 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.969  |     0.936
2022-03-21 11:04:35,936 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.969  |     0.936
2022-03-21 11:04:35,953 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-21 11:04:35,968 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.091  |     0.242
2022-03-21 11:04:35,984 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  7574.758  |       N/A
2022-03-21 11:04:36,000 - INFO - allennlp.training.trainer - Epoch duration: 0:11:21.688267
2022-03-21 11:04:36,016 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:56:37
2022-03-21 11:04:36,031 - INFO - allennlp.training.trainer - Epoch 5/9
2022-03-21 11:04:36,047 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.4G
2022-03-21 11:04:36,063 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-21 11:04:36,079 - INFO - allennlp.training.trainer - Training
2022-03-21 11:04:36,095 - INFO - tqdm - 0%|          | 0/7188 [00:00<?, ?it/s]
2022-03-21 11:04:46,201 - INFO - tqdm - f1: 0.9800, accuracy: 0.9802, batch_loss: 0.0349, loss: 0.0602 ||:   1%|1         | 101/7188 [00:10<08:02, 14.70it/s]
2022-03-21 11:04:56,263 - INFO - tqdm - f1: 0.9768, accuracy: 0.9770, batch_loss: 0.0206, loss: 0.0674 ||:   3%|2         | 209/7188 [00:20<07:47, 14.94it/s]
2022-03-21 11:05:06,329 - INFO - tqdm - f1: 0.9787, accuracy: 0.9787, batch_loss: 0.0017, loss: 0.0620 ||:   4%|4         | 317/7188 [00:30<07:52, 14.55it/s]
2022-03-21 11:05:16,379 - INFO - tqdm - f1: 0.9767, accuracy: 0.9767, batch_loss: 0.0040, loss: 0.0654 ||:   6%|5         | 427/7188 [00:40<07:37, 14.78it/s]
2022-03-21 11:05:26,452 - INFO - tqdm - f1: 0.9774, accuracy: 0.9776, batch_loss: 0.0081, loss: 0.0629 ||:   7%|7         | 535/7188 [00:50<07:25, 14.94it/s]
2022-03-21 11:05:36,477 - INFO - tqdm - f1: 0.9773, accuracy: 0.9774, batch_loss: 0.0052, loss: 0.0638 ||:   9%|8         | 643/7188 [01:00<07:11, 15.16it/s]
2022-03-21 11:05:46,504 - INFO - tqdm - f1: 0.9763, accuracy: 0.9764, batch_loss: 0.3125, loss: 0.0668 ||:  10%|#         | 753/7188 [01:10<07:05, 15.14it/s]
2022-03-21 11:05:56,591 - INFO - tqdm - f1: 0.9768, accuracy: 0.9769, batch_loss: 0.0075, loss: 0.0669 ||:  12%|#2        | 863/7188 [01:20<06:59, 15.07it/s]
2022-03-21 11:06:06,668 - INFO - tqdm - f1: 0.9772, accuracy: 0.9775, batch_loss: 0.0078, loss: 0.0650 ||:  14%|#3        | 973/7188 [01:30<06:39, 15.55it/s]
2022-03-21 11:06:16,734 - INFO - tqdm - f1: 0.9774, accuracy: 0.9776, batch_loss: 0.0236, loss: 0.0647 ||:  15%|#5        | 1083/7188 [01:40<06:24, 15.90it/s]
2022-03-21 11:06:26,745 - INFO - tqdm - f1: 0.9769, accuracy: 0.9771, batch_loss: 0.0227, loss: 0.0657 ||:  17%|#6        | 1191/7188 [01:50<06:34, 15.22it/s]
2022-03-21 11:06:36,782 - INFO - tqdm - f1: 0.9768, accuracy: 0.9769, batch_loss: 0.0589, loss: 0.0668 ||:  18%|#8        | 1303/7188 [02:00<06:07, 16.02it/s]
2022-03-21 11:06:46,825 - INFO - tqdm - f1: 0.9771, accuracy: 0.9771, batch_loss: 0.0035, loss: 0.0675 ||:  20%|#9        | 1413/7188 [02:10<06:23, 15.07it/s]
2022-03-21 11:06:56,861 - INFO - tqdm - f1: 0.9765, accuracy: 0.9765, batch_loss: 0.0033, loss: 0.0685 ||:  21%|##1       | 1522/7188 [02:20<06:28, 14.57it/s]
2022-03-21 11:07:06,971 - INFO - tqdm - f1: 0.9764, accuracy: 0.9765, batch_loss: 0.0888, loss: 0.0689 ||:  23%|##2       | 1628/7188 [02:30<06:38, 13.95it/s]
2022-03-21 11:07:17,342 - INFO - tqdm - f1: 0.9763, accuracy: 0.9763, batch_loss: 0.7824, loss: 0.0694 ||:  24%|##4       | 1734/7188 [02:41<21:36,  4.21it/s]
2022-03-21 11:07:27,369 - INFO - tqdm - f1: 0.9759, accuracy: 0.9760, batch_loss: 0.2418, loss: 0.0702 ||:  26%|##5       | 1852/7188 [02:51<06:06, 14.54it/s]
2022-03-21 11:07:37,377 - INFO - tqdm - f1: 0.9759, accuracy: 0.9759, batch_loss: 0.0797, loss: 0.0701 ||:  27%|##7       | 1954/7188 [03:01<08:15, 10.56it/s]
2022-03-21 11:07:48,249 - INFO - tqdm - f1: 0.9763, accuracy: 0.9763, batch_loss: 0.0111, loss: 0.0686 ||:  29%|##8       | 2068/7188 [03:12<19:39,  4.34it/s]
2022-03-21 11:07:58,276 - INFO - tqdm - f1: 0.9763, accuracy: 0.9763, batch_loss: 0.0599, loss: 0.0685 ||:  30%|###       | 2188/7188 [03:22<05:31, 15.07it/s]
2022-03-21 11:08:08,409 - INFO - tqdm - f1: 0.9758, accuracy: 0.9758, batch_loss: 0.1464, loss: 0.0699 ||:  32%|###1      | 2290/7188 [03:32<07:08, 11.43it/s]
2022-03-21 11:08:19,051 - INFO - tqdm - f1: 0.9759, accuracy: 0.9759, batch_loss: 0.0443, loss: 0.0701 ||:  33%|###3      | 2400/7188 [03:42<18:26,  4.33it/s]
2022-03-21 11:08:29,077 - INFO - tqdm - f1: 0.9756, accuracy: 0.9757, batch_loss: 0.1165, loss: 0.0708 ||:  35%|###5      | 2518/7188 [03:52<05:19, 14.61it/s]
2022-03-21 11:08:39,097 - INFO - tqdm - f1: 0.9756, accuracy: 0.9756, batch_loss: 0.0096, loss: 0.0709 ||:  36%|###6      | 2622/7188 [04:02<06:21, 11.96it/s]
2022-03-21 11:08:49,606 - INFO - tqdm - f1: 0.9755, accuracy: 0.9755, batch_loss: 0.0437, loss: 0.0711 ||:  38%|###7      | 2730/7188 [04:13<17:36,  4.22it/s]
2022-03-21 11:08:59,636 - INFO - tqdm - f1: 0.9756, accuracy: 0.9755, batch_loss: 0.0014, loss: 0.0711 ||:  40%|###9      | 2850/7188 [04:23<04:40, 15.44it/s]
2022-03-21 11:09:09,656 - INFO - tqdm - f1: 0.9756, accuracy: 0.9756, batch_loss: 0.2151, loss: 0.0712 ||:  41%|####1     | 2952/7188 [04:33<04:51, 14.55it/s]
2022-03-21 11:09:19,896 - INFO - tqdm - f1: 0.9754, accuracy: 0.9754, batch_loss: 0.0350, loss: 0.0718 ||:  43%|####2     | 3058/7188 [04:43<15:40,  4.39it/s]
2022-03-21 11:09:30,519 - INFO - tqdm - f1: 0.9754, accuracy: 0.9753, batch_loss: 0.0106, loss: 0.0720 ||:  44%|####4     | 3174/7188 [04:54<14:31,  4.61it/s]
2022-03-21 11:09:40,540 - INFO - tqdm - f1: 0.9753, accuracy: 0.9753, batch_loss: 0.0838, loss: 0.0720 ||:  46%|####5     | 3280/7188 [05:04<11:14,  5.80it/s]
2022-03-21 11:09:50,633 - INFO - tqdm - f1: 0.9752, accuracy: 0.9752, batch_loss: 0.0118, loss: 0.0723 ||:  47%|####7     | 3390/7188 [05:14<08:48,  7.19it/s]
2022-03-21 11:10:00,692 - INFO - tqdm - f1: 0.9753, accuracy: 0.9753, batch_loss: 0.3856, loss: 0.0722 ||:  49%|####8     | 3498/7188 [05:24<10:31,  5.84it/s]
2022-03-21 11:10:10,769 - INFO - tqdm - f1: 0.9751, accuracy: 0.9750, batch_loss: 0.0724, loss: 0.0727 ||:  50%|#####     | 3602/7188 [05:34<13:19,  4.48it/s]
2022-03-21 11:10:21,193 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.0098, loss: 0.0731 ||:  52%|#####1    | 3716/7188 [05:45<12:30,  4.62it/s]
2022-03-21 11:10:31,217 - INFO - tqdm - f1: 0.9749, accuracy: 0.9749, batch_loss: 0.0459, loss: 0.0735 ||:  53%|#####3    | 3824/7188 [05:55<12:10,  4.61it/s]
2022-03-21 11:10:41,244 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.0109, loss: 0.0733 ||:  55%|#####4    | 3930/7188 [06:05<09:27,  5.74it/s]
2022-03-21 11:10:51,249 - INFO - tqdm - f1: 0.9751, accuracy: 0.9751, batch_loss: 0.0268, loss: 0.0732 ||:  56%|#####6    | 4038/7188 [06:15<08:59,  5.84it/s]
2022-03-21 11:11:01,342 - INFO - tqdm - f1: 0.9751, accuracy: 0.9750, batch_loss: 0.1866, loss: 0.0733 ||:  58%|#####7    | 4146/7188 [06:25<07:11,  7.05it/s]
2022-03-21 11:11:11,463 - INFO - tqdm - f1: 0.9750, accuracy: 0.9749, batch_loss: 0.1125, loss: 0.0733 ||:  59%|#####9    | 4254/7188 [06:35<06:54,  7.07it/s]
2022-03-21 11:11:21,598 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.0073, loss: 0.0735 ||:  61%|######    | 4362/7188 [06:45<04:49,  9.76it/s]
2022-03-21 11:11:31,678 - INFO - tqdm - f1: 0.9751, accuracy: 0.9750, batch_loss: 0.0255, loss: 0.0733 ||:  62%|######2   | 4468/7188 [06:55<04:44,  9.56it/s]
2022-03-21 11:11:41,780 - INFO - tqdm - f1: 0.9750, accuracy: 0.9750, batch_loss: 0.0093, loss: 0.0733 ||:  64%|######3   | 4578/7188 [07:05<03:31, 12.32it/s]
2022-03-21 11:11:51,818 - INFO - tqdm - f1: 0.9750, accuracy: 0.9749, batch_loss: 0.0412, loss: 0.0736 ||:  65%|######5   | 4688/7188 [07:15<03:40, 11.35it/s]
2022-03-21 11:12:01,951 - INFO - tqdm - f1: 0.9749, accuracy: 0.9748, batch_loss: 0.2661, loss: 0.0740 ||:  67%|######6   | 4798/7188 [07:25<03:47, 10.48it/s]
2022-03-21 11:12:11,979 - INFO - tqdm - f1: 0.9747, accuracy: 0.9747, batch_loss: 0.0198, loss: 0.0743 ||:  68%|######8   | 4906/7188 [07:35<03:46, 10.07it/s]
2022-03-21 11:12:22,084 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.4910, loss: 0.0742 ||:  70%|######9   | 5016/7188 [07:45<04:17,  8.43it/s]
2022-03-21 11:12:32,338 - INFO - tqdm - f1: 0.9748, accuracy: 0.9748, batch_loss: 0.0129, loss: 0.0738 ||:  71%|#######1  | 5126/7188 [07:56<06:50,  5.02it/s]
2022-03-21 11:12:42,768 - INFO - tqdm - f1: 0.9747, accuracy: 0.9747, batch_loss: 0.0034, loss: 0.0740 ||:  73%|#######2  | 5236/7188 [08:06<07:13,  4.50it/s]
2022-03-21 11:12:52,818 - INFO - tqdm - f1: 0.9747, accuracy: 0.9747, batch_loss: 0.0092, loss: 0.0739 ||:  74%|#######4  | 5342/7188 [08:16<06:41,  4.60it/s]
2022-03-21 11:13:02,845 - INFO - tqdm - f1: 0.9747, accuracy: 0.9747, batch_loss: 0.0023, loss: 0.0739 ||:  76%|#######5  | 5448/7188 [08:26<06:22,  4.55it/s]
2022-03-21 11:13:12,866 - INFO - tqdm - f1: 0.9747, accuracy: 0.9747, batch_loss: 0.0145, loss: 0.0740 ||:  77%|#######7  | 5554/7188 [08:36<05:31,  4.93it/s]
2022-03-21 11:13:22,941 - INFO - tqdm - f1: 0.9746, accuracy: 0.9746, batch_loss: 0.0074, loss: 0.0741 ||:  79%|#######8  | 5662/7188 [08:46<04:24,  5.77it/s]
2022-03-21 11:13:32,983 - INFO - tqdm - f1: 0.9745, accuracy: 0.9745, batch_loss: 0.0387, loss: 0.0744 ||:  80%|########  | 5770/7188 [08:56<04:04,  5.81it/s]
2022-03-21 11:13:42,998 - INFO - tqdm - f1: 0.9745, accuracy: 0.9745, batch_loss: 0.1042, loss: 0.0747 ||:  82%|########1 | 5878/7188 [09:06<04:45,  4.59it/s]
2022-03-21 11:13:53,101 - INFO - tqdm - f1: 0.9746, accuracy: 0.9746, batch_loss: 0.0061, loss: 0.0745 ||:  83%|########3 | 5988/7188 [09:16<02:21,  8.50it/s]
2022-03-21 11:14:03,160 - INFO - tqdm - f1: 0.9746, accuracy: 0.9746, batch_loss: 0.0073, loss: 0.0746 ||:  85%|########4 | 6096/7188 [09:27<02:09,  8.43it/s]
2022-03-21 11:14:13,191 - INFO - tqdm - f1: 0.9746, accuracy: 0.9746, batch_loss: 0.0354, loss: 0.0745 ||:  86%|########6 | 6202/7188 [09:37<02:54,  5.65it/s]
2022-03-21 11:14:23,477 - INFO - tqdm - f1: 0.9746, accuracy: 0.9746, batch_loss: 0.0721, loss: 0.0747 ||:  88%|########7 | 6314/7188 [09:47<03:11,  4.56it/s]
2022-03-21 11:14:33,605 - INFO - tqdm - f1: 0.9745, accuracy: 0.9745, batch_loss: 0.0908, loss: 0.0750 ||:  89%|########9 | 6424/7188 [09:57<02:12,  5.79it/s]
2022-03-21 11:14:43,609 - INFO - tqdm - f1: 0.9745, accuracy: 0.9745, batch_loss: 0.3658, loss: 0.0750 ||:  91%|######### | 6530/7188 [10:07<01:33,  7.06it/s]
2022-03-21 11:14:53,632 - INFO - tqdm - f1: 0.9745, accuracy: 0.9745, batch_loss: 0.0523, loss: 0.0750 ||:  92%|#########2| 6638/7188 [10:17<01:18,  6.99it/s]
2022-03-21 11:15:03,719 - INFO - tqdm - f1: 0.9745, accuracy: 0.9745, batch_loss: 0.1969, loss: 0.0752 ||:  94%|#########3| 6746/7188 [10:27<00:52,  8.47it/s]
2022-03-21 11:15:13,735 - INFO - tqdm - f1: 0.9745, accuracy: 0.9745, batch_loss: 0.0405, loss: 0.0753 ||:  95%|#########5| 6852/7188 [10:37<00:40,  8.22it/s]
2022-03-21 11:15:23,852 - INFO - tqdm - f1: 0.9745, accuracy: 0.9745, batch_loss: 0.0892, loss: 0.0754 ||:  97%|#########6| 6958/7188 [10:47<00:23,  9.62it/s]
2022-03-21 11:15:33,958 - INFO - tqdm - f1: 0.9745, accuracy: 0.9745, batch_loss: 0.0285, loss: 0.0756 ||:  98%|#########8| 7064/7188 [10:57<00:14,  8.29it/s]
2022-03-21 11:15:42,045 - INFO - tqdm - f1: 0.9744, accuracy: 0.9744, batch_loss: 0.0048, loss: 0.0758 ||: 100%|#########9| 7154/7188 [11:05<00:02, 14.73it/s]
2022-03-21 11:15:42,189 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.2579, loss: 0.0759 ||: 100%|#########9| 7156/7188 [11:06<00:02, 14.45it/s]
2022-03-21 11:15:42,317 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.0869, loss: 0.0759 ||: 100%|#########9| 7158/7188 [11:06<00:02, 14.79it/s]
2022-03-21 11:15:42,453 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.1216, loss: 0.0759 ||: 100%|#########9| 7160/7188 [11:06<00:01, 14.75it/s]
2022-03-21 11:15:43,652 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.1362, loss: 0.0759 ||: 100%|#########9| 7162/7188 [11:07<00:05,  4.40it/s]
2022-03-21 11:15:43,798 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.0568, loss: 0.0758 ||: 100%|#########9| 7164/7188 [11:07<00:04,  5.53it/s]
2022-03-21 11:15:43,923 - INFO - tqdm - f1: 0.9744, accuracy: 0.9743, batch_loss: 0.2922, loss: 0.0759 ||: 100%|#########9| 7166/7188 [11:07<00:03,  6.87it/s]
2022-03-21 11:15:44,051 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.2738, loss: 0.0759 ||: 100%|#########9| 7168/7188 [11:07<00:02,  8.26it/s]
2022-03-21 11:15:44,189 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.0562, loss: 0.0759 ||: 100%|#########9| 7170/7188 [11:08<00:01,  9.49it/s]
2022-03-21 11:15:44,344 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.0149, loss: 0.0759 ||: 100%|#########9| 7172/7188 [11:08<00:01, 10.32it/s]
2022-03-21 11:15:44,500 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.2034, loss: 0.0759 ||: 100%|#########9| 7174/7188 [11:08<00:01, 10.94it/s]
2022-03-21 11:15:44,626 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.0030, loss: 0.0759 ||: 100%|#########9| 7176/7188 [11:08<00:00, 12.06it/s]
2022-03-21 11:15:44,767 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.1453, loss: 0.0759 ||: 100%|#########9| 7178/7188 [11:08<00:00, 12.65it/s]
2022-03-21 11:15:44,918 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.0066, loss: 0.0759 ||: 100%|#########9| 7180/7188 [11:08<00:00, 12.80it/s]
2022-03-21 11:15:45,083 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.0497, loss: 0.0759 ||: 100%|#########9| 7182/7188 [11:08<00:00, 12.61it/s]
2022-03-21 11:15:45,227 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.1085, loss: 0.0759 ||: 100%|#########9| 7184/7188 [11:09<00:00, 12.95it/s]
2022-03-21 11:15:45,367 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.1058, loss: 0.0759 ||: 100%|#########9| 7186/7188 [11:09<00:00, 13.33it/s]
2022-03-21 11:15:45,520 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.0328, loss: 0.0759 ||: 100%|##########| 7188/7188 [11:09<00:00, 13.27it/s]
2022-03-21 11:15:45,569 - INFO - tqdm - f1: 0.9743, accuracy: 0.9743, batch_loss: 0.0328, loss: 0.0759 ||: 100%|##########| 7188/7188 [11:09<00:00, 10.74it/s]
2022-03-21 11:15:45,574 - INFO - allennlp.training.trainer - Validating
2022-03-21 11:15:45,589 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-21 11:15:55,643 - INFO - tqdm - f1: 0.9393, accuracy: 0.9392, batch_loss: 0.1735, loss: 0.2337 ||: 100%|##########| 313/313 [00:10<00:00, 31.19it/s]
2022-03-21 11:15:55,661 - INFO - allennlp.training.trainer - Ran out of patience.  Stopping training.
2022-03-21 11:15:55,674 - INFO - dont_stop_pretraining.training.roberta_checkpointer - loading best weights
2022-03-21 11:15:56,094 - INFO - allennlp.commands.train - The model will be evaluated using the best epoch weights.
2022-03-21 11:15:56,116 - INFO - allennlp.training.util - Iterating over dataset
2022-03-21 11:15:56,129 - INFO - tqdm - 0it [00:00, ?it/s]
2022-03-21 11:15:56,159 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-21 11:15:56,166 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-21 11:16:06,145 - INFO - tqdm - f1: 0.94, accuracy: 0.94, loss: 0.22 ||: : 310it [00:09, 50.70it/s]
2022-03-21 11:16:11,971 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 2,
  "peak_worker_0_memory_MB": 7574.7578125,
  "peak_gpu_0_memory_MB": 0,
  "training_duration": "0:56:36.965157",
  "training_start_epoch": 0,
  "training_epochs": 4,
  "epoch": 4,
  "training_f1": 0.9691427201032639,
  "training_accuracy": 0.9691217391304348,
  "training_loss": 0.09097449729746025,
  "training_worker_0_memory_MB": 7574.7578125,
  "training_gpu_0_memory_MB": 0.0,
  "validation_f1": 0.9359936565160751,
  "validation_accuracy": 0.936,
  "validation_loss": 0.2420715901244896,
  "best_validation_f1": 0.9422508478164673,
  "best_validation_accuracy": 0.9422,
  "best_validation_loss": 0.20158322351856733,
  "test_f1": 0.9381488114595413,
  "test_accuracy": 0.9381578947368421,
  "test_loss": 0.21176587892377652
}
2022-03-21 11:16:12,038 - INFO - allennlp.models.archival - archiving weights and vocabulary to model_logs/ag_base_hyper_small_seed_47/model.tar.gz
