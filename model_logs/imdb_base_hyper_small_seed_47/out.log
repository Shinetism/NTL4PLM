2022-03-22 12:09:45,028 - INFO - allennlp.common.params - random_seed = 47
2022-03-22 12:09:45,048 - INFO - allennlp.common.params - numpy_seed = 47
2022-03-22 12:09:45,048 - INFO - allennlp.common.params - pytorch_seed = 47
2022-03-22 12:09:45,087 - INFO - allennlp.common.checks - Pytorch version: 1.8.0+cu111
2022-03-22 12:09:45,105 - INFO - allennlp.common.params - type = default
2022-03-22 12:09:45,106 - INFO - allennlp.common.params - dataset_reader.type = text_classification_json_with_sampling
2022-03-22 12:09:45,125 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer
2022-03-22 12:09:45,147 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2022-03-22 12:09:45,147 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = roberta-base
2022-03-22 12:09:45,166 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags
2022-03-22 12:09:45,166 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512
2022-03-22 12:09:45,166 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None
2022-03-22 12:10:01,783 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer
2022-03-22 12:10:01,784 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = roberta-base
2022-03-22 12:10:01,784 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True
2022-03-22 12:10:01,784 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = 512
2022-03-22 12:10:01,784 - INFO - allennlp.common.params - dataset_reader.tokenizer.stride = 0
2022-03-22 12:10:01,784 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None
2022-03-22 12:10:01,785 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = 512
2022-03-22 12:10:01,798 - INFO - allennlp.common.params - dataset_reader.sample = None
2022-03-22 12:10:01,798 - INFO - allennlp.common.params - dataset_reader.skip_label_indexing = False
2022-03-22 12:10:01,798 - INFO - allennlp.common.params - train_data_path = datasets/imdb/train.jsonl
2022-03-22 12:10:01,799 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7fcaef752150>
2022-03-22 12:10:01,799 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-03-22 12:10:01,799 - INFO - allennlp.common.params - validation_dataset_reader.type = text_classification_json_with_sampling
2022-03-22 12:10:01,800 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer
2022-03-22 12:10:01,800 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2022-03-22 12:10:01,800 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = roberta-base
2022-03-22 12:10:01,800 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags
2022-03-22 12:10:01,800 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512
2022-03-22 12:10:01,800 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None
2022-03-22 12:10:01,801 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.type = pretrained_transformer
2022-03-22 12:10:01,801 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.model_name = roberta-base
2022-03-22 12:10:01,801 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.add_special_tokens = True
2022-03-22 12:10:01,801 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.max_length = 512
2022-03-22 12:10:01,801 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.stride = 0
2022-03-22 12:10:01,801 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer.tokenizer_kwargs = None
2022-03-22 12:10:01,802 - INFO - allennlp.common.params - validation_dataset_reader.max_sequence_length = 512
2022-03-22 12:10:01,802 - INFO - allennlp.common.params - validation_dataset_reader.sample = None
2022-03-22 12:10:01,802 - INFO - allennlp.common.params - validation_dataset_reader.skip_label_indexing = False
2022-03-22 12:10:01,802 - INFO - allennlp.common.params - validation_data_path = datasets/imdb/dev.jsonl
2022-03-22 12:10:01,802 - INFO - allennlp.common.params - validation_data_loader = None
2022-03-22 12:10:01,802 - INFO - allennlp.common.params - test_data_path = datasets/imdb/test.jsonl
2022-03-22 12:10:01,802 - INFO - allennlp.common.params - evaluate_on_test = True
2022-03-22 12:10:01,802 - INFO - allennlp.common.params - batch_weight_key = 
2022-03-22 12:10:01,803 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-22 12:10:01,803 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-22 12:10:01,803 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-22 12:10:01,803 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-22 12:10:01,803 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-22 12:10:01,803 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-22 12:10:01,804 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-22 12:10:01,804 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-22 12:10:01,804 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-22 12:10:01,804 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-22 12:10:01,804 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-22 12:10:01,804 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-22 12:10:01,804 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-22 12:10:01,804 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-22 12:10:01,807 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-22 12:10:12,211 - INFO - tqdm - loading instances: 5916it [00:10, 326.70it/s]
2022-03-22 12:10:22,766 - INFO - tqdm - loading instances: 12503it [00:20, 248.41it/s]
2022-03-22 12:10:32,795 - INFO - tqdm - loading instances: 19265it [00:30, 740.73it/s]
2022-03-22 12:10:33,743 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-22 12:10:33,750 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-22 12:10:33,750 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-22 12:10:33,750 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-22 12:10:33,750 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-22 12:10:33,766 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-22 12:10:33,766 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-22 12:10:33,766 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-22 12:10:33,779 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-22 12:10:33,779 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-22 12:10:33,779 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-22 12:10:33,779 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-22 12:10:33,779 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-22 12:10:33,779 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-22 12:10:33,779 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-22 12:10:41,838 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-03-22 12:10:41,841 - INFO - allennlp.common.params - data_loader.batch_size = None
2022-03-22 12:10:41,841 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-22 12:10:41,855 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-22 12:10:41,869 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-22 12:10:41,869 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 16
2022-03-22 12:10:41,869 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-22 12:10:41,882 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-22 12:10:41,882 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-22 12:10:41,882 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-22 12:10:41,882 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-22 12:10:41,882 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-03-22 12:10:41,895 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-03-22 12:10:41,895 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-03-22 12:10:41,895 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-03-22 12:10:51,961 - INFO - tqdm - loading instances: 6045it [00:10, 696.07it/s]
2022-03-22 12:11:01,999 - INFO - tqdm - loading instances: 11409it [00:20, 698.12it/s]
2022-03-22 12:11:12,023 - INFO - tqdm - loading instances: 16833it [00:30, 741.27it/s]
2022-03-22 12:11:22,103 - INFO - tqdm - loading instances: 24028it [00:40, 703.19it/s]
2022-03-22 12:11:23,505 - INFO - allennlp.common.params - type = from_instances
2022-03-22 12:11:23,509 - INFO - allennlp.common.params - min_count = None
2022-03-22 12:11:23,509 - INFO - allennlp.common.params - max_vocab_size = None
2022-03-22 12:11:23,509 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-03-22 12:11:23,522 - INFO - allennlp.common.params - pretrained_files = None
2022-03-22 12:11:23,522 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-03-22 12:11:23,522 - INFO - allennlp.common.params - tokens_to_add = None
2022-03-22 12:11:23,522 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-03-22 12:11:23,522 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-03-22 12:11:23,535 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-03-22 12:11:23,535 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-03-22 12:11:23,548 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-03-22 12:11:24,846 - INFO - allennlp.common.params - model.type = basic_classifier_with_f1
2022-03-22 12:11:24,858 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2022-03-22 12:11:24,858 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer
2022-03-22 12:11:24,859 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = roberta-base
2022-03-22 12:11:24,871 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512
2022-03-22 12:11:24,871 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_module = None
2022-03-22 12:11:24,871 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True
2022-03-22 12:11:24,871 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True
2022-03-22 12:11:24,871 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None
2022-03-22 12:11:24,872 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None
2022-03-22 12:11:24,872 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None
2022-03-22 12:11:24,872 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None
2022-03-22 12:11:24,872 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None
2022-03-22 12:11:31,295 - INFO - allennlp.common.params - model.seq2vec_encoder.type = cls_pooler
2022-03-22 12:11:31,305 - INFO - allennlp.common.params - model.seq2vec_encoder.embedding_dim = 768
2022-03-22 12:11:31,305 - INFO - allennlp.common.params - model.seq2vec_encoder.cls_is_last_token = False
2022-03-22 12:11:31,305 - INFO - allennlp.common.params - model.feedforward_layer.input_dim = 768
2022-03-22 12:11:31,318 - INFO - allennlp.common.params - model.feedforward_layer.num_layers = 1
2022-03-22 12:11:31,318 - INFO - allennlp.common.params - model.feedforward_layer.hidden_dims = 768
2022-03-22 12:11:31,331 - INFO - allennlp.common.params - model.feedforward_layer.activations = tanh
2022-03-22 12:11:31,332 - INFO - allennlp.common.params - type = tanh
2022-03-22 12:11:31,344 - INFO - allennlp.common.params - model.feedforward_layer.dropout = 0.0
2022-03-22 12:11:31,348 - INFO - allennlp.common.params - model.seq2seq_encoder = None
2022-03-22 12:11:31,358 - INFO - allennlp.common.params - model.dropout = 0.1
2022-03-22 12:11:31,358 - INFO - allennlp.common.params - model.num_labels = None
2022-03-22 12:11:31,358 - INFO - allennlp.common.params - model.label_namespace = labels
2022-03-22 12:11:31,371 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7fcaef766490>
2022-03-22 12:11:31,372 - INFO - allennlp.common.params - model.regularizer = None
2022-03-22 12:11:31,384 - INFO - allennlp.common.params - model.track_weights = False
2022-03-22 12:11:31,385 - INFO - allennlp.common.params - model.disable_layers = []
2022-03-22 12:11:31,385 - INFO - allennlp.nn.initializers - Initializing parameters
2022-03-22 12:11:31,399 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-03-22 12:11:31,411 - INFO - allennlp.nn.initializers -    _classification_layer.bias
2022-03-22 12:11:31,411 - INFO - allennlp.nn.initializers -    _classification_layer.weight
2022-03-22 12:11:31,411 - INFO - allennlp.nn.initializers -    _feedforward_layer._linear_layers.0.bias
2022-03-22 12:11:31,411 - INFO - allennlp.nn.initializers -    _feedforward_layer._linear_layers.0.weight
2022-03-22 12:11:31,411 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias
2022-03-22 12:11:31,411 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight
2022-03-22 12:11:31,412 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight
2022-03-22 12:11:31,424 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight
2022-03-22 12:11:31,425 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight
2022-03-22 12:11:31,425 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2022-03-22 12:11:31,425 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2022-03-22 12:11:31,438 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias
2022-03-22 12:11:31,438 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight
2022-03-22 12:11:31,451 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias
2022-03-22 12:11:31,451 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight
2022-03-22 12:11:31,451 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias
2022-03-22 12:11:31,451 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight
2022-03-22 12:11:31,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias
2022-03-22 12:11:31,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight
2022-03-22 12:11:31,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias
2022-03-22 12:11:31,477 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight
2022-03-22 12:11:31,478 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias
2022-03-22 12:11:31,478 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight
2022-03-22 12:11:31,478 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias
2022-03-22 12:11:31,478 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight
2022-03-22 12:11:31,491 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2022-03-22 12:11:31,491 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2022-03-22 12:11:31,491 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias
2022-03-22 12:11:31,492 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight
2022-03-22 12:11:31,492 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias
2022-03-22 12:11:31,492 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight
2022-03-22 12:11:31,492 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias
2022-03-22 12:11:31,492 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight
2022-03-22 12:11:31,493 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias
2022-03-22 12:11:31,493 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight
2022-03-22 12:11:31,493 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias
2022-03-22 12:11:31,493 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight
2022-03-22 12:11:31,493 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias
2022-03-22 12:11:31,493 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight
2022-03-22 12:11:31,494 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias
2022-03-22 12:11:31,494 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight
2022-03-22 12:11:31,494 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2022-03-22 12:11:31,494 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2022-03-22 12:11:31,494 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias
2022-03-22 12:11:31,495 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight
2022-03-22 12:11:31,495 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias
2022-03-22 12:11:31,495 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight
2022-03-22 12:11:31,495 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias
2022-03-22 12:11:31,495 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight
2022-03-22 12:11:31,495 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias
2022-03-22 12:11:31,496 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight
2022-03-22 12:11:31,496 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias
2022-03-22 12:11:31,496 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight
2022-03-22 12:11:31,496 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias
2022-03-22 12:11:31,496 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight
2022-03-22 12:11:31,497 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias
2022-03-22 12:11:31,497 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight
2022-03-22 12:11:31,497 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2022-03-22 12:11:31,497 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2022-03-22 12:11:31,497 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias
2022-03-22 12:11:31,497 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight
2022-03-22 12:11:31,498 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias
2022-03-22 12:11:31,498 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight
2022-03-22 12:11:31,498 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias
2022-03-22 12:11:31,498 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight
2022-03-22 12:11:31,498 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias
2022-03-22 12:11:31,499 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight
2022-03-22 12:11:31,499 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias
2022-03-22 12:11:31,499 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight
2022-03-22 12:11:31,499 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias
2022-03-22 12:11:31,499 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight
2022-03-22 12:11:31,499 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias
2022-03-22 12:11:31,499 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight
2022-03-22 12:11:31,499 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2022-03-22 12:11:31,499 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2022-03-22 12:11:31,499 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias
2022-03-22 12:11:31,500 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight
2022-03-22 12:11:31,500 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias
2022-03-22 12:11:31,500 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight
2022-03-22 12:11:31,500 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias
2022-03-22 12:11:31,500 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight
2022-03-22 12:11:31,500 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias
2022-03-22 12:11:31,500 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight
2022-03-22 12:11:31,500 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias
2022-03-22 12:11:31,500 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight
2022-03-22 12:11:31,500 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias
2022-03-22 12:11:31,500 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight
2022-03-22 12:11:31,501 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias
2022-03-22 12:11:31,501 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight
2022-03-22 12:11:31,501 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2022-03-22 12:11:31,501 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2022-03-22 12:11:31,501 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias
2022-03-22 12:11:31,501 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight
2022-03-22 12:11:31,501 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias
2022-03-22 12:11:31,501 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight
2022-03-22 12:11:31,501 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias
2022-03-22 12:11:31,501 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight
2022-03-22 12:11:31,501 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias
2022-03-22 12:11:31,502 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight
2022-03-22 12:11:31,502 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias
2022-03-22 12:11:31,502 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight
2022-03-22 12:11:31,502 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias
2022-03-22 12:11:31,502 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight
2022-03-22 12:11:31,502 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias
2022-03-22 12:11:31,502 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight
2022-03-22 12:11:31,502 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2022-03-22 12:11:31,502 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2022-03-22 12:11:31,502 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias
2022-03-22 12:11:31,502 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight
2022-03-22 12:11:31,503 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias
2022-03-22 12:11:31,503 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight
2022-03-22 12:11:31,503 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias
2022-03-22 12:11:31,503 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight
2022-03-22 12:11:31,503 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias
2022-03-22 12:11:31,503 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight
2022-03-22 12:11:31,503 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias
2022-03-22 12:11:31,503 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight
2022-03-22 12:11:31,503 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias
2022-03-22 12:11:31,503 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight
2022-03-22 12:11:31,503 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias
2022-03-22 12:11:31,504 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight
2022-03-22 12:11:31,504 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2022-03-22 12:11:31,504 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2022-03-22 12:11:31,504 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias
2022-03-22 12:11:31,504 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight
2022-03-22 12:11:31,504 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias
2022-03-22 12:11:31,504 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight
2022-03-22 12:11:31,504 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias
2022-03-22 12:11:31,504 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight
2022-03-22 12:11:31,504 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias
2022-03-22 12:11:31,504 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight
2022-03-22 12:11:31,505 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias
2022-03-22 12:11:31,505 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight
2022-03-22 12:11:31,505 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias
2022-03-22 12:11:31,505 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight
2022-03-22 12:11:31,505 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias
2022-03-22 12:11:31,505 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight
2022-03-22 12:11:31,505 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2022-03-22 12:11:31,505 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2022-03-22 12:11:31,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias
2022-03-22 12:11:31,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight
2022-03-22 12:11:31,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias
2022-03-22 12:11:31,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight
2022-03-22 12:11:31,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias
2022-03-22 12:11:31,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight
2022-03-22 12:11:31,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias
2022-03-22 12:11:31,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight
2022-03-22 12:11:31,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias
2022-03-22 12:11:31,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight
2022-03-22 12:11:31,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias
2022-03-22 12:11:31,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight
2022-03-22 12:11:31,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias
2022-03-22 12:11:31,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight
2022-03-22 12:11:31,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2022-03-22 12:11:31,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2022-03-22 12:11:31,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias
2022-03-22 12:11:31,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight
2022-03-22 12:11:31,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias
2022-03-22 12:11:31,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight
2022-03-22 12:11:31,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias
2022-03-22 12:11:31,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight
2022-03-22 12:11:31,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias
2022-03-22 12:11:31,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight
2022-03-22 12:11:31,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias
2022-03-22 12:11:31,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight
2022-03-22 12:11:31,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias
2022-03-22 12:11:31,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight
2022-03-22 12:11:31,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias
2022-03-22 12:11:31,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight
2022-03-22 12:11:31,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias
2022-03-22 12:11:31,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight
2022-03-22 12:11:31,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias
2022-03-22 12:11:31,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight
2022-03-22 12:11:31,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2022-03-22 12:11:31,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2022-03-22 12:11:31,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias
2022-03-22 12:11:31,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight
2022-03-22 12:11:31,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias
2022-03-22 12:11:31,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight
2022-03-22 12:11:31,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias
2022-03-22 12:11:31,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight
2022-03-22 12:11:31,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias
2022-03-22 12:11:31,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight
2022-03-22 12:11:31,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias
2022-03-22 12:11:31,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight
2022-03-22 12:11:31,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias
2022-03-22 12:11:31,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight
2022-03-22 12:11:31,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias
2022-03-22 12:11:31,511 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight
2022-03-22 12:11:31,511 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias
2022-03-22 12:11:31,511 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight
2022-03-22 12:11:40,013 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-03-22 12:11:40,014 - INFO - allennlp.common.params - trainer.patience = 3
2022-03-22 12:11:40,031 - INFO - allennlp.common.params - trainer.validation_metric = +f1
2022-03-22 12:11:40,031 - INFO - allennlp.common.params - trainer.num_epochs = 10
2022-03-22 12:11:40,031 - INFO - allennlp.common.params - trainer.cuda_device = 2
2022-03-22 12:11:40,031 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-03-22 12:11:40,031 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-03-22 12:11:40,031 - INFO - allennlp.common.params - trainer.distributed = False
2022-03-22 12:11:40,048 - INFO - allennlp.common.params - trainer.world_size = 1
2022-03-22 12:11:40,048 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1
2022-03-22 12:11:40,048 - INFO - allennlp.common.params - trainer.use_amp = False
2022-03-22 12:11:40,048 - INFO - allennlp.common.params - trainer.no_grad = None
2022-03-22 12:11:40,048 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2022-03-22 12:11:40,065 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-03-22 12:11:40,065 - INFO - allennlp.common.params - trainer.moving_average = None
2022-03-22 12:11:40,065 - INFO - allennlp.common.params - trainer.callbacks = None
2022-03-22 12:11:40,082 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True
2022-03-22 12:11:48,233 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw_str_lr
2022-03-22 12:11:48,234 - INFO - allennlp.common.params - trainer.optimizer.lr = 2e-05
2022-03-22 12:11:48,234 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-03-22 12:11:48,234 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-03-22 12:11:48,234 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-03-22 12:11:48,235 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = False
2022-03-22 12:11:48,236 - INFO - allennlp.training.optimizers - Done constructing parameter groups.
2022-03-22 12:11:48,236 - INFO - allennlp.training.optimizers - Group 0: ['_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias', '_feedforward_layer._linear_layers.0.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias', '_classification_layer.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight'], {'weight_decay': 0}
2022-03-22 12:11:48,237 - INFO - allennlp.training.optimizers - Group 1: ['_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight', '_classification_layer.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight', '_feedforward_layer._linear_layers.0.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight'], {}
2022-03-22 12:11:48,237 - WARNING - allennlp.training.optimizers - When constructing parameter groups, layer_norm.weight does not match any parameter name
2022-03-22 12:11:48,237 - INFO - allennlp.training.optimizers - Number of trainable parameters: 125237762
2022-03-22 12:11:48,239 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias
2022-03-22 12:11:48,240 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight
2022-03-22 12:11:48,241 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias
2022-03-22 12:11:48,242 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2022-03-22 12:11:48,243 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight
2022-03-22 12:11:48,244 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight
2022-03-22 12:11:48,245 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight
2022-03-22 12:11:48,246 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2022-03-22 12:11:48,247 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias
2022-03-22 12:11:48,248 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight
2022-03-22 12:11:48,249 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _feedforward_layer._linear_layers.0.weight
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _feedforward_layer._linear_layers.0.bias
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _classification_layer.weight
2022-03-22 12:11:48,250 - INFO - allennlp.common.util - _classification_layer.bias
2022-03-22 12:11:48,250 - INFO - allennlp.common.params - trainer.checkpointer.type = roberta_default
2022-03-22 12:11:48,250 - INFO - allennlp.common.params - trainer.checkpointer.keep_serialized_model_every_num_seconds = None
2022-03-22 12:11:48,250 - INFO - allennlp.common.params - trainer.checkpointer.num_serialized_models_to_keep = 0
2022-03-22 12:11:48,250 - INFO - allennlp.common.params - trainer.checkpointer.model_save_interval = None
2022-03-22 12:11:48,251 - INFO - allennlp.common.params - trainer.checkpointer.num_epochs = 10
2022-03-22 12:11:48,251 - INFO - allennlp.common.params - trainer.checkpointer.skip_early_stopping = False
2022-03-22 12:11:48,252 - INFO - allennlp.training.trainer - Beginning training.
2022-03-22 12:11:48,252 - INFO - allennlp.training.trainer - Epoch 0/9
2022-03-22 12:11:48,253 - INFO - allennlp.training.trainer - Worker 0 memory usage: 9.6G
2022-03-22 12:11:48,253 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-22 12:11:48,253 - INFO - allennlp.training.trainer - Training
2022-03-22 12:11:48,254 - INFO - tqdm - 0%|          | 0/1250 [00:00<?, ?it/s]
2022-03-22 12:11:48,281 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-22 12:11:48,282 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-22 12:11:58,393 - INFO - tqdm - f1: 0.4972, accuracy: 0.5337, batch_loss: 0.6901, loss: 0.6927 ||:   2%|2         | 26/1250 [00:10<04:50,  4.21it/s]
2022-03-22 12:12:08,584 - INFO - tqdm - f1: 0.6886, accuracy: 0.6925, batch_loss: 0.3515, loss: 0.5979 ||:   5%|5         | 63/1250 [00:20<06:10,  3.20it/s]
2022-03-22 12:12:18,812 - INFO - tqdm - f1: 0.7637, accuracy: 0.7658, batch_loss: 0.1712, loss: 0.4812 ||:   8%|7         | 99/1250 [00:30<05:31,  3.47it/s]
2022-03-22 12:12:28,816 - INFO - tqdm - f1: 0.8114, accuracy: 0.8120, batch_loss: 0.0816, loss: 0.4040 ||:  11%|#         | 135/1250 [00:40<05:23,  3.45it/s]
2022-03-22 12:12:38,921 - INFO - tqdm - f1: 0.8323, accuracy: 0.8326, batch_loss: 0.3222, loss: 0.3671 ||:  14%|#3        | 171/1250 [00:50<04:34,  3.93it/s]
2022-03-22 12:12:48,926 - INFO - tqdm - f1: 0.8519, accuracy: 0.8521, batch_loss: 0.1647, loss: 0.3332 ||:  17%|#7        | 218/1250 [01:00<01:37, 10.63it/s]
2022-03-22 12:12:59,014 - INFO - tqdm - f1: 0.8677, accuracy: 0.8678, batch_loss: 0.3440, loss: 0.3075 ||:  23%|##2       | 286/1250 [01:10<04:25,  3.63it/s]
2022-03-22 12:13:09,207 - INFO - tqdm - f1: 0.8733, accuracy: 0.8733, batch_loss: 0.2942, loss: 0.2977 ||:  27%|##6       | 335/1250 [01:20<03:16,  4.65it/s]
2022-03-22 12:13:19,226 - INFO - tqdm - f1: 0.8797, accuracy: 0.8797, batch_loss: 0.0763, loss: 0.2878 ||:  31%|###       | 383/1250 [01:30<02:51,  5.04it/s]
2022-03-22 12:13:29,382 - INFO - tqdm - f1: 0.8846, accuracy: 0.8846, batch_loss: 0.1442, loss: 0.2776 ||:  34%|###4      | 428/1250 [01:41<04:25,  3.09it/s]
2022-03-22 12:13:39,769 - INFO - tqdm - f1: 0.8872, accuracy: 0.8872, batch_loss: 0.5796, loss: 0.2718 ||:  37%|###7      | 466/1250 [01:51<04:39,  2.81it/s]
2022-03-22 12:13:49,944 - INFO - tqdm - f1: 0.8907, accuracy: 0.8907, batch_loss: 0.5018, loss: 0.2656 ||:  40%|####      | 504/1250 [02:01<02:53,  4.30it/s]
2022-03-22 12:14:00,265 - INFO - tqdm - f1: 0.8944, accuracy: 0.8944, batch_loss: 0.0867, loss: 0.2582 ||:  43%|####3     | 542/1250 [02:12<04:10,  2.83it/s]
2022-03-22 12:14:10,309 - INFO - tqdm - f1: 0.8968, accuracy: 0.8968, batch_loss: 0.0631, loss: 0.2539 ||:  46%|####6     | 578/1250 [02:22<02:41,  4.16it/s]
2022-03-22 12:14:20,534 - INFO - tqdm - f1: 0.8989, accuracy: 0.8989, batch_loss: 0.1436, loss: 0.2491 ||:  49%|####9     | 616/1250 [02:32<02:37,  4.03it/s]
2022-03-22 12:14:30,596 - INFO - tqdm - f1: 0.9010, accuracy: 0.9010, batch_loss: 0.0261, loss: 0.2448 ||:  52%|#####2    | 653/1250 [02:42<02:58,  3.35it/s]
2022-03-22 12:14:40,809 - INFO - tqdm - f1: 0.9028, accuracy: 0.9028, batch_loss: 0.2966, loss: 0.2414 ||:  55%|#####5    | 689/1250 [02:52<02:40,  3.49it/s]
2022-03-22 12:14:50,888 - INFO - tqdm - f1: 0.9038, accuracy: 0.9038, batch_loss: 0.1940, loss: 0.2392 ||:  58%|#####8    | 726/1250 [03:02<02:27,  3.56it/s]
2022-03-22 12:15:01,043 - INFO - tqdm - f1: 0.9053, accuracy: 0.9053, batch_loss: 0.0378, loss: 0.2363 ||:  61%|######1   | 763/1250 [03:12<02:16,  3.57it/s]
2022-03-22 12:15:11,167 - INFO - tqdm - f1: 0.9064, accuracy: 0.9064, batch_loss: 0.2319, loss: 0.2343 ||:  64%|######3   | 798/1250 [03:22<01:49,  4.12it/s]
2022-03-22 12:15:21,390 - INFO - tqdm - f1: 0.9072, accuracy: 0.9072, batch_loss: 0.1370, loss: 0.2331 ||:  67%|######6   | 833/1250 [03:33<02:09,  3.22it/s]
2022-03-22 12:15:31,614 - INFO - tqdm - f1: 0.9087, accuracy: 0.9087, batch_loss: 0.0936, loss: 0.2302 ||:  69%|######9   | 868/1250 [03:43<01:53,  3.37it/s]
2022-03-22 12:15:41,693 - INFO - tqdm - f1: 0.9091, accuracy: 0.9091, batch_loss: 0.3597, loss: 0.2298 ||:  72%|#######2  | 902/1250 [03:53<01:37,  3.57it/s]
2022-03-22 12:15:51,858 - INFO - tqdm - f1: 0.9102, accuracy: 0.9102, batch_loss: 0.0995, loss: 0.2275 ||:  75%|#######5  | 939/1250 [04:03<01:42,  3.03it/s]
2022-03-22 12:16:02,042 - INFO - tqdm - f1: 0.9108, accuracy: 0.9108, batch_loss: 0.1112, loss: 0.2264 ||:  78%|#######8  | 975/1250 [04:13<01:29,  3.07it/s]
2022-03-22 12:16:12,110 - INFO - tqdm - f1: 0.9114, accuracy: 0.9114, batch_loss: 0.2343, loss: 0.2252 ||:  81%|########  | 1010/1250 [04:23<01:03,  3.78it/s]
2022-03-22 12:16:22,143 - INFO - tqdm - f1: 0.9123, accuracy: 0.9123, batch_loss: 0.0875, loss: 0.2241 ||:  84%|########3 | 1046/1250 [04:33<00:59,  3.45it/s]
2022-03-22 12:16:32,287 - INFO - tqdm - f1: 0.9133, accuracy: 0.9133, batch_loss: 0.3400, loss: 0.2222 ||:  87%|########6 | 1083/1250 [04:44<00:51,  3.27it/s]
2022-03-22 12:16:42,340 - INFO - tqdm - f1: 0.9136, accuracy: 0.9136, batch_loss: 0.1734, loss: 0.2218 ||:  90%|########9 | 1119/1250 [04:54<00:37,  3.47it/s]
2022-03-22 12:16:52,618 - INFO - tqdm - f1: 0.9144, accuracy: 0.9144, batch_loss: 0.3076, loss: 0.2205 ||:  92%|#########2| 1156/1250 [05:04<00:24,  3.91it/s]
2022-03-22 12:17:02,876 - INFO - tqdm - f1: 0.9150, accuracy: 0.9150, batch_loss: 0.2653, loss: 0.2195 ||:  96%|#########5| 1195/1250 [05:14<00:15,  3.52it/s]
2022-03-22 12:17:13,076 - INFO - tqdm - f1: 0.9159, accuracy: 0.9159, batch_loss: 0.0311, loss: 0.2177 ||:  98%|#########8| 1231/1250 [05:24<00:05,  3.44it/s]
2022-03-22 12:17:16,875 - INFO - tqdm - f1: 0.9161, accuracy: 0.9161, batch_loss: 0.0823, loss: 0.2170 ||: 100%|#########9| 1244/1250 [05:28<00:01,  3.21it/s]
2022-03-22 12:17:17,079 - INFO - tqdm - f1: 0.9162, accuracy: 0.9162, batch_loss: 0.2542, loss: 0.2171 ||: 100%|#########9| 1245/1250 [05:28<00:01,  3.58it/s]
2022-03-22 12:17:17,393 - INFO - tqdm - f1: 0.9161, accuracy: 0.9161, batch_loss: 0.2101, loss: 0.2171 ||: 100%|#########9| 1246/1250 [05:29<00:01,  3.45it/s]
2022-03-22 12:17:17,728 - INFO - tqdm - f1: 0.9161, accuracy: 0.9161, batch_loss: 0.4223, loss: 0.2172 ||: 100%|#########9| 1247/1250 [05:29<00:00,  3.30it/s]
2022-03-22 12:17:17,995 - INFO - tqdm - f1: 0.9161, accuracy: 0.9161, batch_loss: 0.2175, loss: 0.2172 ||: 100%|#########9| 1248/1250 [05:29<00:00,  3.42it/s]
2022-03-22 12:17:18,278 - INFO - tqdm - f1: 0.9161, accuracy: 0.9161, batch_loss: 0.1327, loss: 0.2172 ||: 100%|#########9| 1249/1250 [05:30<00:00,  3.45it/s]
2022-03-22 12:17:18,456 - INFO - tqdm - f1: 0.9161, accuracy: 0.9162, batch_loss: 0.0307, loss: 0.2170 ||: 100%|##########| 1250/1250 [05:30<00:00,  3.90it/s]
2022-03-22 12:17:18,472 - INFO - tqdm - f1: 0.9161, accuracy: 0.9162, batch_loss: 0.0307, loss: 0.2170 ||: 100%|##########| 1250/1250 [05:30<00:00,  3.79it/s]
2022-03-22 12:17:18,520 - INFO - allennlp.training.trainer - Validating
2022-03-22 12:17:18,528 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-22 12:17:18,553 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-22 12:17:18,556 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-22 12:17:28,618 - INFO - tqdm - f1: 0.9390, accuracy: 0.9390, batch_loss: 0.0730, loss: 0.1578 ||:  27%|##6       | 84/313 [00:10<00:26,  8.76it/s]
2022-03-22 12:17:38,713 - INFO - tqdm - f1: 0.9446, accuracy: 0.9447, batch_loss: 0.1148, loss: 0.1430 ||:  53%|#####3    | 166/313 [00:20<00:21,  6.90it/s]
2022-03-22 12:17:48,743 - INFO - tqdm - f1: 0.9447, accuracy: 0.9447, batch_loss: 0.0356, loss: 0.1442 ||:  80%|#######9  | 249/313 [00:30<00:08,  7.29it/s]
2022-03-22 12:17:56,819 - INFO - tqdm - f1: 0.9456, accuracy: 0.9456, batch_loss: 0.2042, loss: 0.1429 ||: 100%|#########9| 312/313 [00:38<00:00,  7.04it/s]
2022-03-22 12:17:56,962 - INFO - tqdm - f1: 0.9458, accuracy: 0.9458, batch_loss: 0.0245, loss: 0.1425 ||: 100%|##########| 313/313 [00:38<00:00,  7.03it/s]
2022-03-22 12:17:56,976 - INFO - tqdm - f1: 0.9458, accuracy: 0.9458, batch_loss: 0.0245, loss: 0.1425 ||: 100%|##########| 313/313 [00:38<00:00,  8.14it/s]
2022-03-22 12:17:57,027 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/imdb_base_hyper_small_seed_47/best.th'.
2022-03-22 12:17:58,759 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-22 12:17:58,759 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.916  |     0.946
2022-03-22 12:17:58,759 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.916  |     0.946
2022-03-22 12:17:58,759 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-22 12:17:58,759 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.217  |     0.143
2022-03-22 12:17:58,759 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  9851.039  |       N/A
2022-03-22 12:17:58,759 - INFO - allennlp.training.trainer - Epoch duration: 0:06:10.507007
2022-03-22 12:17:58,773 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:55:34
2022-03-22 12:17:58,774 - INFO - allennlp.training.trainer - Epoch 1/9
2022-03-22 12:17:58,774 - INFO - allennlp.training.trainer - Worker 0 memory usage: 9.8G
2022-03-22 12:17:58,774 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-22 12:17:58,789 - INFO - allennlp.training.trainer - Training
2022-03-22 12:17:58,789 - INFO - tqdm - 0%|          | 0/1250 [00:00<?, ?it/s]
2022-03-22 12:18:08,916 - INFO - tqdm - f1: 0.9642, accuracy: 0.9643, batch_loss: 0.0475, loss: 0.1094 ||:   3%|2         | 35/1250 [00:10<05:25,  3.74it/s]
2022-03-22 12:18:18,975 - INFO - tqdm - f1: 0.9642, accuracy: 0.9643, batch_loss: 0.2674, loss: 0.1192 ||:   6%|5         | 70/1250 [00:20<05:26,  3.62it/s]
2022-03-22 12:18:29,019 - INFO - tqdm - f1: 0.9625, accuracy: 0.9625, batch_loss: 0.0442, loss: 0.1263 ||:   8%|8         | 105/1250 [00:30<05:05,  3.75it/s]
2022-03-22 12:18:39,156 - INFO - tqdm - f1: 0.9610, accuracy: 0.9611, batch_loss: 0.0565, loss: 0.1224 ||:  11%|#1        | 143/1250 [00:40<04:26,  4.16it/s]
2022-03-22 12:18:49,177 - INFO - tqdm - f1: 0.9610, accuracy: 0.9610, batch_loss: 0.1524, loss: 0.1203 ||:  14%|#4        | 178/1250 [00:50<04:27,  4.00it/s]
2022-03-22 12:18:59,392 - INFO - tqdm - f1: 0.9617, accuracy: 0.9617, batch_loss: 0.2188, loss: 0.1209 ||:  17%|#7        | 214/1250 [01:00<03:53,  4.44it/s]
2022-03-22 12:19:09,566 - INFO - tqdm - f1: 0.9619, accuracy: 0.9619, batch_loss: 0.0407, loss: 0.1207 ||:  20%|##        | 251/1250 [01:10<04:36,  3.61it/s]
2022-03-22 12:19:19,682 - INFO - tqdm - f1: 0.9595, accuracy: 0.9595, batch_loss: 0.0498, loss: 0.1253 ||:  23%|##2       | 287/1250 [01:20<04:25,  3.62it/s]
2022-03-22 12:19:30,043 - INFO - tqdm - f1: 0.9565, accuracy: 0.9565, batch_loss: 0.1773, loss: 0.1325 ||:  26%|##6       | 325/1250 [01:31<04:59,  3.09it/s]
2022-03-22 12:19:40,278 - INFO - tqdm - f1: 0.9571, accuracy: 0.9571, batch_loss: 0.0603, loss: 0.1312 ||:  29%|##9       | 363/1250 [01:41<04:32,  3.25it/s]
2022-03-22 12:19:50,372 - INFO - tqdm - f1: 0.9572, accuracy: 0.9572, batch_loss: 0.0061, loss: 0.1280 ||:  32%|###2      | 400/1250 [01:51<03:21,  4.21it/s]
2022-03-22 12:20:00,401 - INFO - tqdm - f1: 0.9581, accuracy: 0.9581, batch_loss: 0.0966, loss: 0.1246 ||:  35%|###4      | 436/1250 [02:01<04:06,  3.31it/s]
2022-03-22 12:20:10,493 - INFO - tqdm - f1: 0.9577, accuracy: 0.9577, batch_loss: 0.1037, loss: 0.1279 ||:  38%|###7      | 474/1250 [02:11<03:28,  3.72it/s]
2022-03-22 12:20:20,526 - INFO - tqdm - f1: 0.9571, accuracy: 0.9571, batch_loss: 0.0384, loss: 0.1284 ||:  41%|####      | 511/1250 [02:21<03:23,  3.63it/s]
2022-03-22 12:20:30,612 - INFO - tqdm - f1: 0.9569, accuracy: 0.9569, batch_loss: 0.0107, loss: 0.1269 ||:  44%|####3     | 547/1250 [02:31<03:01,  3.88it/s]
2022-03-22 12:20:40,816 - INFO - tqdm - f1: 0.9567, accuracy: 0.9567, batch_loss: 0.3380, loss: 0.1270 ||:  47%|####6     | 584/1250 [02:42<03:23,  3.28it/s]
2022-03-22 12:20:50,994 - INFO - tqdm - f1: 0.9565, accuracy: 0.9565, batch_loss: 0.0286, loss: 0.1279 ||:  50%|####9     | 621/1250 [02:52<03:25,  3.06it/s]
2022-03-22 12:21:01,033 - INFO - tqdm - f1: 0.9570, accuracy: 0.9570, batch_loss: 0.1173, loss: 0.1263 ||:  53%|#####2    | 657/1250 [03:02<03:16,  3.01it/s]
2022-03-22 12:21:11,466 - INFO - tqdm - f1: 0.9574, accuracy: 0.9574, batch_loss: 0.0713, loss: 0.1249 ||:  56%|#####5    | 694/1250 [03:12<03:05,  3.00it/s]
2022-03-22 12:21:21,468 - INFO - tqdm - f1: 0.9576, accuracy: 0.9576, batch_loss: 0.0277, loss: 0.1252 ||:  58%|#####8    | 731/1250 [03:22<02:36,  3.32it/s]
2022-03-22 12:21:31,605 - INFO - tqdm - f1: 0.9575, accuracy: 0.9575, batch_loss: 0.0481, loss: 0.1245 ||:  61%|######1   | 765/1250 [03:32<02:11,  3.68it/s]
2022-03-22 12:21:41,974 - INFO - tqdm - f1: 0.9574, accuracy: 0.9575, batch_loss: 0.0450, loss: 0.1245 ||:  64%|######4   | 802/1250 [03:43<02:09,  3.47it/s]
2022-03-22 12:21:52,107 - INFO - tqdm - f1: 0.9576, accuracy: 0.9576, batch_loss: 0.0956, loss: 0.1233 ||:  67%|######7   | 838/1250 [03:53<02:18,  2.98it/s]
2022-03-22 12:22:02,318 - INFO - tqdm - f1: 0.9571, accuracy: 0.9571, batch_loss: 0.1991, loss: 0.1242 ||:  70%|#######   | 875/1250 [04:03<01:40,  3.74it/s]
2022-03-22 12:22:12,443 - INFO - tqdm - f1: 0.9571, accuracy: 0.9571, batch_loss: 0.1163, loss: 0.1235 ||:  73%|#######2  | 911/1250 [04:13<01:42,  3.32it/s]
2022-03-22 12:22:22,503 - INFO - tqdm - f1: 0.9574, accuracy: 0.9574, batch_loss: 0.0132, loss: 0.1235 ||:  76%|#######5  | 947/1250 [04:23<01:14,  4.06it/s]
2022-03-22 12:22:32,509 - INFO - tqdm - f1: 0.9574, accuracy: 0.9574, batch_loss: 0.0180, loss: 0.1238 ||:  79%|#######8  | 982/1250 [04:33<01:26,  3.11it/s]
2022-03-22 12:22:42,520 - INFO - tqdm - f1: 0.9576, accuracy: 0.9576, batch_loss: 0.1039, loss: 0.1231 ||:  81%|########1 | 1017/1250 [04:43<01:13,  3.15it/s]
2022-03-22 12:22:52,544 - INFO - tqdm - f1: 0.9580, accuracy: 0.9580, batch_loss: 0.0590, loss: 0.1223 ||:  84%|########4 | 1055/1250 [04:53<01:04,  3.02it/s]
2022-03-22 12:23:02,658 - INFO - tqdm - f1: 0.9580, accuracy: 0.9580, batch_loss: 0.0467, loss: 0.1224 ||:  87%|########7 | 1091/1250 [05:03<00:52,  3.01it/s]
2022-03-22 12:23:12,771 - INFO - tqdm - f1: 0.9580, accuracy: 0.9580, batch_loss: 0.0934, loss: 0.1228 ||:  90%|######### | 1127/1250 [05:13<00:40,  3.00it/s]
2022-03-22 12:23:22,877 - INFO - tqdm - f1: 0.9578, accuracy: 0.9578, batch_loss: 0.1395, loss: 0.1235 ||:  93%|#########3| 1164/1250 [05:24<00:23,  3.68it/s]
2022-03-22 12:23:32,988 - INFO - tqdm - f1: 0.9578, accuracy: 0.9578, batch_loss: 0.0945, loss: 0.1228 ||:  96%|#########6| 1201/1250 [05:34<00:14,  3.45it/s]
2022-03-22 12:23:43,073 - INFO - tqdm - f1: 0.9579, accuracy: 0.9579, batch_loss: 0.4701, loss: 0.1234 ||:  99%|#########9| 1238/1250 [05:44<00:03,  3.26it/s]
2022-03-22 12:23:44,865 - INFO - tqdm - f1: 0.9578, accuracy: 0.9578, batch_loss: 0.0107, loss: 0.1237 ||: 100%|#########9| 1244/1250 [05:46<00:01,  3.61it/s]
2022-03-22 12:23:45,095 - INFO - tqdm - f1: 0.9578, accuracy: 0.9578, batch_loss: 0.0913, loss: 0.1237 ||: 100%|#########9| 1245/1250 [05:46<00:01,  3.81it/s]
2022-03-22 12:23:45,329 - INFO - tqdm - f1: 0.9578, accuracy: 0.9578, batch_loss: 0.1447, loss: 0.1237 ||: 100%|#########9| 1246/1250 [05:46<00:01,  3.94it/s]
2022-03-22 12:23:45,600 - INFO - tqdm - f1: 0.9578, accuracy: 0.9578, batch_loss: 0.0378, loss: 0.1236 ||: 100%|#########9| 1247/1250 [05:46<00:00,  3.86it/s]
2022-03-22 12:23:45,906 - INFO - tqdm - f1: 0.9578, accuracy: 0.9578, batch_loss: 0.0253, loss: 0.1236 ||: 100%|#########9| 1248/1250 [05:47<00:00,  3.66it/s]
2022-03-22 12:23:46,116 - INFO - tqdm - f1: 0.9579, accuracy: 0.9579, batch_loss: 0.0217, loss: 0.1235 ||: 100%|#########9| 1249/1250 [05:47<00:00,  3.93it/s]
2022-03-22 12:23:46,421 - INFO - tqdm - f1: 0.9578, accuracy: 0.9578, batch_loss: 0.1304, loss: 0.1235 ||: 100%|##########| 1250/1250 [05:47<00:00,  3.71it/s]
2022-03-22 12:23:46,445 - INFO - tqdm - f1: 0.9578, accuracy: 0.9578, batch_loss: 0.1304, loss: 0.1235 ||: 100%|##########| 1250/1250 [05:47<00:00,  3.60it/s]
2022-03-22 12:23:46,482 - INFO - allennlp.training.trainer - Validating
2022-03-22 12:23:46,483 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-22 12:23:56,487 - INFO - tqdm - f1: 0.9509, accuracy: 0.9509, batch_loss: 0.2041, loss: 0.1408 ||:  25%|##5       | 79/313 [00:10<00:30,  7.71it/s]
2022-03-22 12:24:06,488 - INFO - tqdm - f1: 0.9448, accuracy: 0.9449, batch_loss: 0.3246, loss: 0.1485 ||:  51%|#####1    | 161/313 [00:20<00:19,  7.69it/s]
2022-03-22 12:24:16,664 - INFO - tqdm - f1: 0.9421, accuracy: 0.9422, batch_loss: 0.0864, loss: 0.1623 ||:  79%|#######8  | 246/313 [00:30<00:08,  7.45it/s]
2022-03-22 12:24:25,060 - INFO - tqdm - f1: 0.9430, accuracy: 0.9430, batch_loss: 0.0582, loss: 0.1600 ||: 100%|#########9| 312/313 [00:38<00:00,  8.01it/s]
2022-03-22 12:24:25,210 - INFO - tqdm - f1: 0.9429, accuracy: 0.9430, batch_loss: 0.1359, loss: 0.1600 ||: 100%|##########| 313/313 [00:38<00:00,  7.65it/s]
2022-03-22 12:24:25,221 - INFO - tqdm - f1: 0.9429, accuracy: 0.9430, batch_loss: 0.1359, loss: 0.1600 ||: 100%|##########| 313/313 [00:38<00:00,  8.08it/s]
2022-03-22 12:24:25,267 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-22 12:24:25,280 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.958  |     0.943
2022-03-22 12:24:25,280 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.958  |     0.943
2022-03-22 12:24:25,280 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-22 12:24:25,280 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.123  |     0.160
2022-03-22 12:24:25,280 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  10048.641  |       N/A
2022-03-22 12:24:25,280 - INFO - allennlp.training.trainer - Epoch duration: 0:06:26.506447
2022-03-22 12:24:25,280 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:50:28
2022-03-22 12:24:25,280 - INFO - allennlp.training.trainer - Epoch 2/9
2022-03-22 12:24:25,295 - INFO - allennlp.training.trainer - Worker 0 memory usage: 9.8G
2022-03-22 12:24:25,296 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-22 12:24:25,312 - INFO - allennlp.training.trainer - Training
2022-03-22 12:24:25,312 - INFO - tqdm - 0%|          | 0/1250 [00:00<?, ?it/s]
2022-03-22 12:24:35,502 - INFO - tqdm - f1: 0.9765, accuracy: 0.9766, batch_loss: 0.0325, loss: 0.0698 ||:   3%|3         | 40/1250 [00:10<04:53,  4.13it/s]
2022-03-22 12:24:45,660 - INFO - tqdm - f1: 0.9792, accuracy: 0.9792, batch_loss: 0.0624, loss: 0.0706 ||:   6%|6         | 75/1250 [00:20<06:58,  2.81it/s]
2022-03-22 12:24:55,696 - INFO - tqdm - f1: 0.9814, accuracy: 0.9814, batch_loss: 0.2949, loss: 0.0654 ||:   9%|8         | 111/1250 [00:30<04:58,  3.81it/s]
2022-03-22 12:25:05,798 - INFO - tqdm - f1: 0.9820, accuracy: 0.9820, batch_loss: 0.0066, loss: 0.0628 ||:  12%|#1        | 149/1250 [00:40<05:04,  3.62it/s]
2022-03-22 12:25:15,924 - INFO - tqdm - f1: 0.9807, accuracy: 0.9807, batch_loss: 0.3227, loss: 0.0684 ||:  15%|#5        | 188/1250 [00:50<05:31,  3.20it/s]
2022-03-22 12:25:25,993 - INFO - tqdm - f1: 0.9792, accuracy: 0.9792, batch_loss: 0.1681, loss: 0.0686 ||:  18%|#8        | 225/1250 [01:00<04:59,  3.43it/s]
2022-03-22 12:25:36,269 - INFO - tqdm - f1: 0.9788, accuracy: 0.9788, batch_loss: 0.0112, loss: 0.0704 ||:  21%|##        | 262/1250 [01:10<05:14,  3.14it/s]
2022-03-22 12:25:46,458 - INFO - tqdm - f1: 0.9785, accuracy: 0.9785, batch_loss: 0.0090, loss: 0.0688 ||:  24%|##3       | 297/1250 [01:21<04:34,  3.48it/s]
2022-03-22 12:25:56,709 - INFO - tqdm - f1: 0.9784, accuracy: 0.9784, batch_loss: 0.3773, loss: 0.0690 ||:  27%|##6       | 335/1250 [01:31<04:06,  3.71it/s]
2022-03-22 12:26:06,775 - INFO - tqdm - f1: 0.9781, accuracy: 0.9781, batch_loss: 0.1385, loss: 0.0694 ||:  30%|##9       | 371/1250 [01:41<04:03,  3.61it/s]
2022-03-22 12:26:16,809 - INFO - tqdm - f1: 0.9778, accuracy: 0.9778, batch_loss: 0.0191, loss: 0.0703 ||:  32%|###2      | 405/1250 [01:51<03:42,  3.80it/s]
2022-03-22 12:26:26,865 - INFO - tqdm - f1: 0.9776, accuracy: 0.9776, batch_loss: 0.1444, loss: 0.0707 ||:  35%|###5      | 440/1250 [02:01<04:25,  3.05it/s]
2022-03-22 12:26:37,219 - INFO - tqdm - f1: 0.9766, accuracy: 0.9766, batch_loss: 0.4505, loss: 0.0724 ||:  38%|###8      | 476/1250 [02:11<03:49,  3.37it/s]
2022-03-22 12:26:47,314 - INFO - tqdm - f1: 0.9755, accuracy: 0.9755, batch_loss: 0.3492, loss: 0.0756 ||:  41%|####      | 510/1250 [02:22<03:52,  3.19it/s]
2022-03-22 12:26:57,418 - INFO - tqdm - f1: 0.9757, accuracy: 0.9757, batch_loss: 0.0171, loss: 0.0756 ||:  44%|####3     | 547/1250 [02:32<03:26,  3.40it/s]
2022-03-22 12:27:07,566 - INFO - tqdm - f1: 0.9759, accuracy: 0.9759, batch_loss: 0.1186, loss: 0.0751 ||:  46%|####6     | 581/1250 [02:42<03:32,  3.15it/s]
2022-03-22 12:27:17,590 - INFO - tqdm - f1: 0.9755, accuracy: 0.9755, batch_loss: 0.0320, loss: 0.0760 ||:  50%|####9     | 619/1250 [02:52<02:59,  3.52it/s]
2022-03-22 12:27:27,721 - INFO - tqdm - f1: 0.9761, accuracy: 0.9761, batch_loss: 0.0083, loss: 0.0751 ||:  52%|#####2    | 655/1250 [03:02<02:40,  3.70it/s]
2022-03-22 12:27:37,915 - INFO - tqdm - f1: 0.9763, accuracy: 0.9763, batch_loss: 0.0190, loss: 0.0753 ||:  55%|#####5    | 690/1250 [03:12<02:41,  3.46it/s]
2022-03-22 12:27:48,228 - INFO - tqdm - f1: 0.9763, accuracy: 0.9763, batch_loss: 0.0214, loss: 0.0747 ||:  58%|#####8    | 727/1250 [03:22<02:41,  3.25it/s]
2022-03-22 12:27:58,547 - INFO - tqdm - f1: 0.9759, accuracy: 0.9759, batch_loss: 0.0515, loss: 0.0757 ||:  61%|######    | 761/1250 [03:33<02:42,  3.00it/s]
2022-03-22 12:28:08,638 - INFO - tqdm - f1: 0.9758, accuracy: 0.9758, batch_loss: 0.0708, loss: 0.0758 ||:  64%|######3   | 798/1250 [03:43<02:00,  3.75it/s]
2022-03-22 12:28:18,805 - INFO - tqdm - f1: 0.9757, accuracy: 0.9757, batch_loss: 0.0293, loss: 0.0755 ||:  67%|######6   | 836/1250 [03:53<01:35,  4.32it/s]
2022-03-22 12:28:29,030 - INFO - tqdm - f1: 0.9759, accuracy: 0.9759, batch_loss: 0.0020, loss: 0.0753 ||:  70%|######9   | 873/1250 [04:03<02:00,  3.13it/s]
2022-03-22 12:28:39,245 - INFO - tqdm - f1: 0.9762, accuracy: 0.9762, batch_loss: 0.4306, loss: 0.0751 ||:  73%|#######2  | 910/1250 [04:13<01:53,  2.99it/s]
2022-03-22 12:28:49,429 - INFO - tqdm - f1: 0.9762, accuracy: 0.9762, batch_loss: 0.0272, loss: 0.0757 ||:  76%|#######5  | 947/1250 [04:24<01:13,  4.14it/s]
2022-03-22 12:28:59,632 - INFO - tqdm - f1: 0.9760, accuracy: 0.9760, batch_loss: 0.1885, loss: 0.0753 ||:  79%|#######8  | 985/1250 [04:34<01:03,  4.16it/s]
2022-03-22 12:29:10,049 - INFO - tqdm - f1: 0.9758, accuracy: 0.9758, batch_loss: 0.4006, loss: 0.0758 ||:  82%|########1 | 1020/1250 [04:44<01:08,  3.38it/s]
2022-03-22 12:29:20,051 - INFO - tqdm - f1: 0.9758, accuracy: 0.9758, batch_loss: 0.0474, loss: 0.0758 ||:  85%|########4 | 1057/1250 [04:54<00:59,  3.25it/s]
2022-03-22 12:29:30,070 - INFO - tqdm - f1: 0.9756, accuracy: 0.9756, batch_loss: 0.0060, loss: 0.0763 ||:  87%|########7 | 1093/1250 [05:04<00:50,  3.09it/s]
2022-03-22 12:29:40,096 - INFO - tqdm - f1: 0.9759, accuracy: 0.9759, batch_loss: 0.0252, loss: 0.0760 ||:  90%|######### | 1130/1250 [05:14<00:33,  3.57it/s]
2022-03-22 12:29:50,257 - INFO - tqdm - f1: 0.9759, accuracy: 0.9759, batch_loss: 0.0573, loss: 0.0761 ||:  93%|#########3| 1167/1250 [05:24<00:23,  3.60it/s]
2022-03-22 12:30:00,408 - INFO - tqdm - f1: 0.9762, accuracy: 0.9762, batch_loss: 0.4156, loss: 0.0758 ||:  96%|#########6| 1206/1250 [05:35<00:13,  3.29it/s]
2022-03-22 12:30:10,421 - INFO - tqdm - f1: 0.9758, accuracy: 0.9758, batch_loss: 0.0180, loss: 0.0763 ||:  99%|#########9| 1241/1250 [05:45<00:02,  3.40it/s]
2022-03-22 12:30:11,037 - INFO - tqdm - f1: 0.9758, accuracy: 0.9758, batch_loss: 0.0457, loss: 0.0762 ||: 100%|#########9| 1244/1250 [05:45<00:01,  4.27it/s]
2022-03-22 12:30:11,380 - INFO - tqdm - f1: 0.9759, accuracy: 0.9759, batch_loss: 0.0149, loss: 0.0762 ||: 100%|#########9| 1245/1250 [05:46<00:01,  3.75it/s]
2022-03-22 12:30:11,583 - INFO - tqdm - f1: 0.9758, accuracy: 0.9758, batch_loss: 0.0763, loss: 0.0762 ||: 100%|#########9| 1246/1250 [05:46<00:00,  4.04it/s]
2022-03-22 12:30:12,037 - INFO - tqdm - f1: 0.9757, accuracy: 0.9757, batch_loss: 0.2722, loss: 0.0764 ||: 100%|#########9| 1247/1250 [05:46<00:00,  3.23it/s]
2022-03-22 12:30:12,323 - INFO - tqdm - f1: 0.9758, accuracy: 0.9758, batch_loss: 0.0055, loss: 0.0763 ||: 100%|#########9| 1248/1250 [05:47<00:00,  3.30it/s]
2022-03-22 12:30:12,736 - INFO - tqdm - f1: 0.9757, accuracy: 0.9757, batch_loss: 0.2767, loss: 0.0765 ||: 100%|#########9| 1249/1250 [05:47<00:00,  2.98it/s]
2022-03-22 12:30:13,041 - INFO - tqdm - f1: 0.9757, accuracy: 0.9758, batch_loss: 0.0617, loss: 0.0764 ||: 100%|##########| 1250/1250 [05:47<00:00,  3.06it/s]
2022-03-22 12:30:13,052 - INFO - tqdm - f1: 0.9757, accuracy: 0.9758, batch_loss: 0.0617, loss: 0.0764 ||: 100%|##########| 1250/1250 [05:47<00:00,  3.59it/s]
2022-03-22 12:30:13,055 - INFO - allennlp.training.trainer - Validating
2022-03-22 12:30:13,056 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-22 12:30:23,078 - INFO - tqdm - f1: 0.9391, accuracy: 0.9391, batch_loss: 0.0196, loss: 0.1856 ||:  25%|##5       | 79/313 [00:10<00:35,  6.53it/s]
2022-03-22 12:30:33,320 - INFO - tqdm - f1: 0.9472, accuracy: 0.9472, batch_loss: 0.3352, loss: 0.1640 ||:  53%|#####3    | 167/313 [00:20<00:16,  8.64it/s]
2022-03-22 12:30:43,380 - INFO - tqdm - f1: 0.9472, accuracy: 0.9472, batch_loss: 0.0045, loss: 0.1610 ||:  80%|#######9  | 249/313 [00:30<00:07,  8.16it/s]
2022-03-22 12:30:51,092 - INFO - tqdm - f1: 0.9462, accuracy: 0.9462, batch_loss: 0.1592, loss: 0.1676 ||: 100%|#########9| 312/313 [00:38<00:00,  8.41it/s]
2022-03-22 12:30:51,152 - INFO - tqdm - f1: 0.9460, accuracy: 0.9460, batch_loss: 0.3351, loss: 0.1681 ||: 100%|##########| 313/313 [00:38<00:00,  8.22it/s]
2022-03-22 12:30:51,187 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/imdb_base_hyper_small_seed_47/best.th'.
2022-03-22 12:30:52,185 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-22 12:30:52,186 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.976  |     0.946
2022-03-22 12:30:52,186 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.976  |     0.946
2022-03-22 12:30:52,186 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-22 12:30:52,186 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.076  |     0.168
2022-03-22 12:30:52,186 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  10048.641  |       N/A
2022-03-22 12:30:52,186 - INFO - allennlp.training.trainer - Epoch duration: 0:06:26.905629
2022-03-22 12:30:52,186 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:44:29
2022-03-22 12:30:52,186 - INFO - allennlp.training.trainer - Epoch 3/9
2022-03-22 12:30:52,186 - INFO - allennlp.training.trainer - Worker 0 memory usage: 9.8G
2022-03-22 12:30:52,186 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-22 12:30:52,187 - INFO - allennlp.training.trainer - Training
2022-03-22 12:30:52,187 - INFO - tqdm - 0%|          | 0/1250 [00:00<?, ?it/s]
2022-03-22 12:31:02,350 - INFO - tqdm - f1: 0.9889, accuracy: 0.9890, batch_loss: 0.0042, loss: 0.0515 ||:   3%|2         | 34/1250 [00:10<06:04,  3.33it/s]
2022-03-22 12:31:12,589 - INFO - tqdm - f1: 0.9877, accuracy: 0.9877, batch_loss: 0.0031, loss: 0.0471 ||:   6%|5         | 71/1250 [00:20<05:39,  3.47it/s]
2022-03-22 12:31:22,907 - INFO - tqdm - f1: 0.9871, accuracy: 0.9871, batch_loss: 0.0037, loss: 0.0440 ||:   9%|8         | 107/1250 [00:30<06:01,  3.17it/s]
2022-03-22 12:31:33,054 - INFO - tqdm - f1: 0.9858, accuracy: 0.9858, batch_loss: 0.0694, loss: 0.0440 ||:  12%|#1        | 145/1250 [00:40<04:31,  4.07it/s]
2022-03-22 12:31:43,063 - INFO - tqdm - f1: 0.9872, accuracy: 0.9872, batch_loss: 0.0151, loss: 0.0406 ||:  18%|#7        | 224/1250 [00:50<02:37,  6.50it/s]
2022-03-22 12:31:53,194 - INFO - tqdm - f1: 0.9866, accuracy: 0.9866, batch_loss: 0.0963, loss: 0.0439 ||:  22%|##1       | 271/1250 [01:01<03:08,  5.20it/s]
2022-03-22 12:32:03,255 - INFO - tqdm - f1: 0.9873, accuracy: 0.9873, batch_loss: 0.3416, loss: 0.0430 ||:  25%|##5       | 315/1250 [01:11<03:18,  4.71it/s]
2022-03-22 12:32:13,378 - INFO - tqdm - f1: 0.9867, accuracy: 0.9867, batch_loss: 0.0071, loss: 0.0426 ||:  29%|##9       | 363/1250 [01:21<03:02,  4.86it/s]
2022-03-22 12:32:23,508 - INFO - tqdm - f1: 0.9871, accuracy: 0.9871, batch_loss: 0.0127, loss: 0.0420 ||:  32%|###2      | 406/1250 [01:31<04:08,  3.40it/s]
2022-03-22 12:32:33,740 - INFO - tqdm - f1: 0.9865, accuracy: 0.9865, batch_loss: 0.0120, loss: 0.0434 ||:  35%|###5      | 441/1250 [01:41<04:02,  3.34it/s]
2022-03-22 12:32:43,740 - INFO - tqdm - f1: 0.9861, accuracy: 0.9861, batch_loss: 0.0061, loss: 0.0446 ||:  38%|###8      | 476/1250 [01:51<03:49,  3.37it/s]
2022-03-22 12:32:53,874 - INFO - tqdm - f1: 0.9863, accuracy: 0.9863, batch_loss: 0.0352, loss: 0.0444 ||:  41%|####1     | 516/1250 [02:01<02:54,  4.20it/s]
2022-03-22 12:33:04,079 - INFO - tqdm - f1: 0.9864, accuracy: 0.9864, batch_loss: 0.0117, loss: 0.0444 ||:  44%|####4     | 553/1250 [02:11<03:14,  3.59it/s]
2022-03-22 12:33:14,225 - INFO - tqdm - f1: 0.9863, accuracy: 0.9863, batch_loss: 0.0109, loss: 0.0451 ||:  47%|####7     | 588/1250 [02:22<03:21,  3.29it/s]
2022-03-22 12:33:24,465 - INFO - tqdm - f1: 0.9868, accuracy: 0.9868, batch_loss: 0.0074, loss: 0.0442 ||:  50%|####9     | 622/1250 [02:32<03:29,  3.00it/s]
2022-03-22 12:33:34,507 - INFO - tqdm - f1: 0.9867, accuracy: 0.9867, batch_loss: 0.0096, loss: 0.0450 ||:  53%|#####2    | 659/1250 [02:42<03:04,  3.21it/s]
2022-03-22 12:33:44,813 - INFO - tqdm - f1: 0.9866, accuracy: 0.9866, batch_loss: 0.0520, loss: 0.0458 ||:  55%|#####5    | 693/1250 [02:52<03:00,  3.09it/s]
2022-03-22 12:33:54,882 - INFO - tqdm - f1: 0.9864, accuracy: 0.9864, batch_loss: 0.0023, loss: 0.0461 ||:  58%|#####8    | 729/1250 [03:02<02:33,  3.39it/s]
2022-03-22 12:34:04,931 - INFO - tqdm - f1: 0.9865, accuracy: 0.9865, batch_loss: 0.0059, loss: 0.0460 ||:  62%|######1   | 770/1250 [03:12<01:48,  4.42it/s]
2022-03-22 12:34:15,084 - INFO - tqdm - f1: 0.9864, accuracy: 0.9864, batch_loss: 0.0610, loss: 0.0465 ||:  64%|######4   | 806/1250 [03:22<01:50,  4.01it/s]
2022-03-22 12:34:25,201 - INFO - tqdm - f1: 0.9865, accuracy: 0.9865, batch_loss: 0.1221, loss: 0.0460 ||:  67%|######7   | 843/1250 [03:33<01:53,  3.58it/s]
2022-03-22 12:34:35,383 - INFO - tqdm - f1: 0.9864, accuracy: 0.9864, batch_loss: 0.0527, loss: 0.0471 ||:  70%|#######   | 877/1250 [03:43<02:08,  2.89it/s]
2022-03-22 12:34:45,620 - INFO - tqdm - f1: 0.9864, accuracy: 0.9864, batch_loss: 0.1953, loss: 0.0466 ||:  73%|#######2  | 912/1250 [03:53<01:26,  3.91it/s]
2022-03-22 12:34:55,668 - INFO - tqdm - f1: 0.9860, accuracy: 0.9860, batch_loss: 0.0027, loss: 0.0473 ||:  76%|#######5  | 948/1250 [04:03<01:29,  3.39it/s]
2022-03-22 12:35:05,834 - INFO - tqdm - f1: 0.9862, accuracy: 0.9862, batch_loss: 0.1733, loss: 0.0465 ||:  79%|#######8  | 985/1250 [04:13<01:18,  3.37it/s]
2022-03-22 12:35:15,900 - INFO - tqdm - f1: 0.9864, accuracy: 0.9864, batch_loss: 0.0048, loss: 0.0457 ||:  82%|########1 | 1024/1250 [04:23<01:03,  3.53it/s]
2022-03-22 12:35:26,164 - INFO - tqdm - f1: 0.9863, accuracy: 0.9863, batch_loss: 0.0725, loss: 0.0461 ||:  85%|########4 | 1059/1250 [04:33<00:58,  3.27it/s]
2022-03-22 12:35:36,270 - INFO - tqdm - f1: 0.9861, accuracy: 0.9861, batch_loss: 0.0176, loss: 0.0471 ||:  88%|########7 | 1096/1250 [04:44<00:44,  3.49it/s]
2022-03-22 12:35:46,470 - INFO - tqdm - f1: 0.9857, accuracy: 0.9857, batch_loss: 0.0491, loss: 0.0478 ||:  91%|######### | 1135/1250 [04:54<00:30,  3.78it/s]
2022-03-22 12:35:56,832 - INFO - tqdm - f1: 0.9853, accuracy: 0.9853, batch_loss: 0.6456, loss: 0.0488 ||:  94%|#########3| 1171/1250 [05:04<00:21,  3.64it/s]
2022-03-22 12:36:07,077 - INFO - tqdm - f1: 0.9854, accuracy: 0.9854, batch_loss: 0.0032, loss: 0.0487 ||:  96%|#########6| 1205/1250 [05:14<00:14,  3.14it/s]
2022-03-22 12:36:17,044 - INFO - tqdm - f1: 0.9854, accuracy: 0.9854, batch_loss: 0.2881, loss: 0.0489 ||: 100%|#########9| 1244/1250 [05:24<00:01,  3.74it/s]
2022-03-22 12:36:17,282 - INFO - tqdm - f1: 0.9854, accuracy: 0.9854, batch_loss: 0.0034, loss: 0.0488 ||: 100%|#########9| 1245/1250 [05:25<00:01,  3.87it/s]
2022-03-22 12:36:17,595 - INFO - tqdm - f1: 0.9854, accuracy: 0.9854, batch_loss: 0.0754, loss: 0.0489 ||: 100%|#########9| 1246/1250 [05:25<00:01,  3.64it/s]
2022-03-22 12:36:17,956 - INFO - tqdm - f1: 0.9853, accuracy: 0.9853, batch_loss: 0.1212, loss: 0.0489 ||: 100%|#########9| 1247/1250 [05:25<00:00,  3.33it/s]
2022-03-22 12:36:18,405 - INFO - tqdm - f1: 0.9853, accuracy: 0.9853, batch_loss: 0.0154, loss: 0.0489 ||: 100%|#########9| 1248/1250 [05:26<00:00,  2.90it/s]
2022-03-22 12:36:18,572 - INFO - tqdm - f1: 0.9853, accuracy: 0.9853, batch_loss: 0.0204, loss: 0.0489 ||: 100%|#########9| 1249/1250 [05:26<00:00,  3.43it/s]
2022-03-22 12:36:18,808 - INFO - tqdm - f1: 0.9854, accuracy: 0.9853, batch_loss: 0.0262, loss: 0.0489 ||: 100%|##########| 1250/1250 [05:26<00:00,  3.64it/s]
2022-03-22 12:36:18,818 - INFO - tqdm - f1: 0.9854, accuracy: 0.9853, batch_loss: 0.0262, loss: 0.0489 ||: 100%|##########| 1250/1250 [05:26<00:00,  3.83it/s]
2022-03-22 12:36:18,852 - INFO - allennlp.training.trainer - Validating
2022-03-22 12:36:18,868 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-22 12:36:29,019 - INFO - tqdm - f1: 0.9405, accuracy: 0.9405, batch_loss: 0.3736, loss: 0.1853 ||:  27%|##6       | 83/313 [00:10<00:29,  7.81it/s]
2022-03-22 12:36:39,072 - INFO - tqdm - f1: 0.9469, accuracy: 0.9469, batch_loss: 0.3468, loss: 0.1639 ||:  52%|#####2    | 163/313 [00:20<00:17,  8.41it/s]
2022-03-22 12:36:49,096 - INFO - tqdm - f1: 0.9474, accuracy: 0.9474, batch_loss: 0.4264, loss: 0.1601 ||:  78%|#######7  | 244/313 [00:30<00:07,  9.38it/s]
2022-03-22 12:36:57,412 - INFO - tqdm - f1: 0.9474, accuracy: 0.9474, batch_loss: 0.2677, loss: 0.1597 ||: 100%|#########9| 312/313 [00:38<00:00,  7.68it/s]
2022-03-22 12:36:57,577 - INFO - tqdm - f1: 0.9474, accuracy: 0.9474, batch_loss: 0.2234, loss: 0.1599 ||: 100%|##########| 313/313 [00:38<00:00,  7.16it/s]
2022-03-22 12:36:57,586 - INFO - tqdm - f1: 0.9474, accuracy: 0.9474, batch_loss: 0.2234, loss: 0.1599 ||: 100%|##########| 313/313 [00:38<00:00,  8.08it/s]
2022-03-22 12:36:57,606 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/imdb_base_hyper_small_seed_47/best.th'.
2022-03-22 12:36:58,611 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-22 12:36:58,619 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.985  |     0.947
2022-03-22 12:36:58,619 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.985  |     0.947
2022-03-22 12:36:58,619 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-22 12:36:58,619 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.049  |     0.160
2022-03-22 12:36:58,619 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  10048.805  |       N/A
2022-03-22 12:36:58,619 - INFO - allennlp.training.trainer - Epoch duration: 0:06:06.432851
2022-03-22 12:36:58,619 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:37:45
2022-03-22 12:36:58,626 - INFO - allennlp.training.trainer - Epoch 4/9
2022-03-22 12:36:58,627 - INFO - allennlp.training.trainer - Worker 0 memory usage: 9.8G
2022-03-22 12:36:58,627 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-22 12:36:58,635 - INFO - allennlp.training.trainer - Training
2022-03-22 12:36:58,642 - INFO - tqdm - 0%|          | 0/1250 [00:00<?, ?it/s]
2022-03-22 12:37:08,653 - INFO - tqdm - f1: 0.9961, accuracy: 0.9961, batch_loss: 0.0009, loss: 0.0210 ||:   3%|2         | 32/1250 [00:10<06:20,  3.20it/s]
2022-03-22 12:37:18,682 - INFO - tqdm - f1: 0.9953, accuracy: 0.9953, batch_loss: 0.4506, loss: 0.0276 ||:   5%|5         | 66/1250 [00:20<06:15,  3.15it/s]
2022-03-22 12:37:28,720 - INFO - tqdm - f1: 0.9933, accuracy: 0.9933, batch_loss: 0.4054, loss: 0.0315 ||:   8%|8         | 103/1250 [00:30<05:09,  3.71it/s]
2022-03-22 12:37:38,792 - INFO - tqdm - f1: 0.9937, accuracy: 0.9937, batch_loss: 0.0120, loss: 0.0265 ||:  11%|#1        | 139/1250 [00:40<05:24,  3.43it/s]
2022-03-22 12:37:48,795 - INFO - tqdm - f1: 0.9918, accuracy: 0.9918, batch_loss: 0.0039, loss: 0.0318 ||:  14%|#4        | 175/1250 [00:50<04:21,  4.12it/s]
2022-03-22 12:37:58,942 - INFO - tqdm - f1: 0.9911, accuracy: 0.9911, batch_loss: 0.0102, loss: 0.0337 ||:  17%|#6        | 211/1250 [01:00<04:07,  4.19it/s]
2022-03-22 12:38:08,980 - INFO - tqdm - f1: 0.9906, accuracy: 0.9906, batch_loss: 0.0026, loss: 0.0343 ||:  20%|#9        | 247/1250 [01:10<05:01,  3.33it/s]
2022-03-22 12:38:19,116 - INFO - tqdm - f1: 0.9898, accuracy: 0.9898, batch_loss: 0.0030, loss: 0.0363 ||:  23%|##2       | 283/1250 [01:20<04:27,  3.61it/s]
2022-03-22 12:38:29,372 - INFO - tqdm - f1: 0.9897, accuracy: 0.9897, batch_loss: 0.0668, loss: 0.0343 ||:  26%|##5       | 321/1250 [01:30<04:19,  3.58it/s]
2022-03-22 12:38:39,774 - INFO - tqdm - f1: 0.9900, accuracy: 0.9900, batch_loss: 0.0029, loss: 0.0329 ||:  28%|##8       | 355/1250 [01:41<05:19,  2.80it/s]
2022-03-22 12:38:49,916 - INFO - tqdm - f1: 0.9900, accuracy: 0.9900, batch_loss: 0.0015, loss: 0.0329 ||:  31%|###1      | 392/1250 [01:51<03:34,  4.00it/s]
2022-03-22 12:38:59,921 - INFO - tqdm - f1: 0.9899, accuracy: 0.9899, batch_loss: 0.0607, loss: 0.0333 ||:  34%|###4      | 426/1250 [02:01<03:36,  3.81it/s]
2022-03-22 12:39:10,083 - INFO - tqdm - f1: 0.9895, accuracy: 0.9895, batch_loss: 0.0033, loss: 0.0338 ||:  37%|###7      | 464/1250 [02:11<03:28,  3.77it/s]
2022-03-22 12:39:20,219 - INFO - tqdm - f1: 0.9886, accuracy: 0.9886, batch_loss: 0.0154, loss: 0.0363 ||:  40%|###9      | 499/1250 [02:21<04:24,  2.84it/s]
2022-03-22 12:39:30,313 - INFO - tqdm - f1: 0.9885, accuracy: 0.9885, batch_loss: 0.0187, loss: 0.0370 ||:  43%|####2     | 536/1250 [02:31<03:12,  3.71it/s]
2022-03-22 12:39:40,631 - INFO - tqdm - f1: 0.9881, accuracy: 0.9881, batch_loss: 0.1392, loss: 0.0380 ||:  46%|####5     | 574/1250 [02:41<03:05,  3.65it/s]
2022-03-22 12:39:50,885 - INFO - tqdm - f1: 0.9883, accuracy: 0.9883, batch_loss: 0.0026, loss: 0.0378 ||:  49%|####9     | 613/1250 [02:52<02:45,  3.86it/s]
2022-03-22 12:40:01,029 - INFO - tqdm - f1: 0.9883, accuracy: 0.9883, batch_loss: 0.0251, loss: 0.0378 ||:  52%|#####1    | 649/1250 [03:02<02:09,  4.65it/s]
2022-03-22 12:40:11,286 - INFO - tqdm - f1: 0.9879, accuracy: 0.9879, batch_loss: 0.0419, loss: 0.0387 ||:  55%|#####4    | 685/1250 [03:12<02:46,  3.40it/s]
2022-03-22 12:40:21,549 - INFO - tqdm - f1: 0.9880, accuracy: 0.9880, batch_loss: 0.0338, loss: 0.0386 ||:  58%|#####7    | 723/1250 [03:22<02:36,  3.36it/s]
2022-03-22 12:40:31,714 - INFO - tqdm - f1: 0.9879, accuracy: 0.9879, batch_loss: 0.0075, loss: 0.0386 ||:  61%|######    | 759/1250 [03:33<01:58,  4.16it/s]
2022-03-22 12:40:41,810 - INFO - tqdm - f1: 0.9880, accuracy: 0.9880, batch_loss: 0.1392, loss: 0.0380 ||:  64%|######3   | 794/1250 [03:43<02:17,  3.31it/s]
2022-03-22 12:40:52,164 - INFO - tqdm - f1: 0.9880, accuracy: 0.9880, batch_loss: 0.0009, loss: 0.0372 ||:  66%|######6   | 831/1250 [03:53<02:01,  3.44it/s]
2022-03-22 12:41:02,390 - INFO - tqdm - f1: 0.9883, accuracy: 0.9883, batch_loss: 0.0171, loss: 0.0364 ||:  69%|######9   | 868/1250 [04:03<01:56,  3.29it/s]
2022-03-22 12:41:12,616 - INFO - tqdm - f1: 0.9883, accuracy: 0.9883, batch_loss: 0.0011, loss: 0.0370 ||:  72%|#######2  | 904/1250 [04:13<01:44,  3.31it/s]
2022-03-22 12:41:22,630 - INFO - tqdm - f1: 0.9880, accuracy: 0.9880, batch_loss: 0.0138, loss: 0.0377 ||:  75%|#######5  | 941/1250 [04:23<01:12,  4.26it/s]
2022-03-22 12:41:32,776 - INFO - tqdm - f1: 0.9880, accuracy: 0.9880, batch_loss: 0.0014, loss: 0.0382 ||:  78%|#######8  | 977/1250 [04:34<01:10,  3.85it/s]
2022-03-22 12:41:42,933 - INFO - tqdm - f1: 0.9881, accuracy: 0.9881, batch_loss: 0.0100, loss: 0.0374 ||:  81%|########1 | 1015/1250 [04:44<01:05,  3.57it/s]
2022-03-22 12:41:53,151 - INFO - tqdm - f1: 0.9880, accuracy: 0.9880, batch_loss: 0.1998, loss: 0.0383 ||:  84%|########4 | 1053/1250 [04:54<01:04,  3.03it/s]
2022-03-22 12:42:03,299 - INFO - tqdm - f1: 0.9882, accuracy: 0.9882, batch_loss: 0.0171, loss: 0.0379 ||:  87%|########7 | 1089/1250 [05:04<00:38,  4.18it/s]
2022-03-22 12:42:13,321 - INFO - tqdm - f1: 0.9880, accuracy: 0.9880, batch_loss: 0.0047, loss: 0.0384 ||:  90%|######### | 1127/1250 [05:14<00:33,  3.64it/s]
2022-03-22 12:42:23,467 - INFO - tqdm - f1: 0.9879, accuracy: 0.9879, batch_loss: 0.0076, loss: 0.0385 ||:  93%|#########3| 1165/1250 [05:24<00:18,  4.72it/s]
2022-03-22 12:42:33,636 - INFO - tqdm - f1: 0.9881, accuracy: 0.9881, batch_loss: 0.0522, loss: 0.0377 ||:  96%|#########6| 1203/1250 [05:34<00:14,  3.35it/s]
2022-03-22 12:42:43,695 - INFO - tqdm - f1: 0.9881, accuracy: 0.9881, batch_loss: 0.0029, loss: 0.0377 ||:  99%|#########9| 1241/1250 [05:45<00:02,  4.34it/s]
2022-03-22 12:42:44,504 - INFO - tqdm - f1: 0.9880, accuracy: 0.9880, batch_loss: 0.0016, loss: 0.0378 ||: 100%|#########9| 1244/1250 [05:45<00:01,  4.22it/s]
2022-03-22 12:42:44,762 - INFO - tqdm - f1: 0.9881, accuracy: 0.9881, batch_loss: 0.0045, loss: 0.0378 ||: 100%|#########9| 1245/1250 [05:46<00:01,  4.11it/s]
2022-03-22 12:42:45,054 - INFO - tqdm - f1: 0.9881, accuracy: 0.9881, batch_loss: 0.0023, loss: 0.0377 ||: 100%|#########9| 1246/1250 [05:46<00:01,  3.88it/s]
2022-03-22 12:42:45,459 - INFO - tqdm - f1: 0.9880, accuracy: 0.9880, batch_loss: 0.1539, loss: 0.0378 ||: 100%|#########9| 1247/1250 [05:46<00:00,  3.31it/s]
2022-03-22 12:42:45,692 - INFO - tqdm - f1: 0.9880, accuracy: 0.9880, batch_loss: 0.0029, loss: 0.0378 ||: 100%|#########9| 1248/1250 [05:47<00:00,  3.55it/s]
2022-03-22 12:42:45,884 - INFO - tqdm - f1: 0.9880, accuracy: 0.9880, batch_loss: 0.0020, loss: 0.0378 ||: 100%|#########9| 1249/1250 [05:47<00:00,  3.93it/s]
2022-03-22 12:42:46,188 - INFO - tqdm - f1: 0.9879, accuracy: 0.9879, batch_loss: 0.2179, loss: 0.0379 ||: 100%|##########| 1250/1250 [05:47<00:00,  3.71it/s]
2022-03-22 12:42:46,198 - INFO - tqdm - f1: 0.9879, accuracy: 0.9879, batch_loss: 0.2179, loss: 0.0379 ||: 100%|##########| 1250/1250 [05:47<00:00,  3.60it/s]
2022-03-22 12:42:46,234 - INFO - allennlp.training.trainer - Validating
2022-03-22 12:42:46,235 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-22 12:42:56,335 - INFO - tqdm - f1: 0.9449, accuracy: 0.9449, batch_loss: 0.0012, loss: 0.2618 ||:  27%|##6       | 84/313 [00:10<00:30,  7.41it/s]
2022-03-22 12:43:06,364 - INFO - tqdm - f1: 0.9473, accuracy: 0.9473, batch_loss: 0.2831, loss: 0.2479 ||:  54%|#####3    | 169/313 [00:20<00:17,  8.07it/s]
2022-03-22 12:43:16,397 - INFO - tqdm - f1: 0.9464, accuracy: 0.9464, batch_loss: 0.4498, loss: 0.2682 ||:  80%|########  | 251/313 [00:30<00:07,  7.94it/s]
2022-03-22 12:43:24,001 - INFO - tqdm - f1: 0.9482, accuracy: 0.9482, batch_loss: 0.7965, loss: 0.2604 ||: 100%|##########| 313/313 [00:37<00:00, 10.32it/s]
2022-03-22 12:43:24,003 - INFO - tqdm - f1: 0.9482, accuracy: 0.9482, batch_loss: 0.7965, loss: 0.2604 ||: 100%|##########| 313/313 [00:37<00:00,  8.29it/s]
2022-03-22 12:43:24,041 - INFO - dont_stop_pretraining.training.roberta_checkpointer - Best validation performance so far. Copying weights to 'model_logs/imdb_base_hyper_small_seed_47/best.th'.
2022-03-22 12:43:24,683 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-22 12:43:24,685 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.988  |     0.948
2022-03-22 12:43:24,685 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.988  |     0.948
2022-03-22 12:43:24,685 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-22 12:43:24,685 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.038  |     0.260
2022-03-22 12:43:24,685 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  10048.805  |       N/A
2022-03-22 12:43:24,685 - INFO - allennlp.training.trainer - Epoch duration: 0:06:26.058633
2022-03-22 12:43:24,685 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:31:36
2022-03-22 12:43:24,685 - INFO - allennlp.training.trainer - Epoch 5/9
2022-03-22 12:43:24,685 - INFO - allennlp.training.trainer - Worker 0 memory usage: 9.8G
2022-03-22 12:43:24,686 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-22 12:43:24,686 - INFO - allennlp.training.trainer - Training
2022-03-22 12:43:24,687 - INFO - tqdm - 0%|          | 0/1250 [00:00<?, ?it/s]
2022-03-22 12:43:34,847 - INFO - tqdm - f1: 0.9965, accuracy: 0.9965, batch_loss: 0.0010, loss: 0.0174 ||:   3%|2         | 36/1250 [00:10<05:46,  3.50it/s]
2022-03-22 12:43:44,946 - INFO - tqdm - f1: 0.9939, accuracy: 0.9939, batch_loss: 0.0007, loss: 0.0183 ||:   6%|5         | 72/1250 [00:20<05:21,  3.67it/s]
2022-03-22 12:43:55,150 - INFO - tqdm - f1: 0.9911, accuracy: 0.9911, batch_loss: 0.0880, loss: 0.0212 ||:   9%|8         | 112/1250 [00:30<03:46,  5.03it/s]
2022-03-22 12:44:05,404 - INFO - tqdm - f1: 0.9916, accuracy: 0.9916, batch_loss: 0.0030, loss: 0.0222 ||:  12%|#1        | 148/1250 [00:40<05:31,  3.33it/s]
2022-03-22 12:44:15,473 - INFO - tqdm - f1: 0.9919, accuracy: 0.9919, batch_loss: 0.0014, loss: 0.0214 ||:  15%|#4        | 186/1250 [00:50<03:51,  4.60it/s]
2022-03-22 12:44:25,547 - INFO - tqdm - f1: 0.9917, accuracy: 0.9917, batch_loss: 0.0019, loss: 0.0227 ||:  18%|#7        | 219/1250 [01:00<04:44,  3.63it/s]
2022-03-22 12:44:35,874 - INFO - tqdm - f1: 0.9919, accuracy: 0.9919, batch_loss: 0.0234, loss: 0.0241 ||:  20%|##        | 256/1250 [01:11<05:54,  2.81it/s]
2022-03-22 12:44:46,074 - INFO - tqdm - f1: 0.9925, accuracy: 0.9925, batch_loss: 0.0013, loss: 0.0245 ||:  23%|##3       | 293/1250 [01:21<03:53,  4.10it/s]
2022-03-22 12:44:56,114 - INFO - tqdm - f1: 0.9920, accuracy: 0.9920, batch_loss: 0.0011, loss: 0.0256 ||:  26%|##6       | 328/1250 [01:31<03:57,  3.88it/s]
2022-03-22 12:45:06,579 - INFO - tqdm - f1: 0.9920, accuracy: 0.9920, batch_loss: 0.0008, loss: 0.0249 ||:  29%|##9       | 365/1250 [01:41<05:41,  2.59it/s]
2022-03-22 12:45:16,668 - INFO - tqdm - f1: 0.9920, accuracy: 0.9920, batch_loss: 0.0043, loss: 0.0256 ||:  32%|###2      | 404/1250 [01:51<02:56,  4.78it/s]
2022-03-22 12:45:26,911 - INFO - tqdm - f1: 0.9920, accuracy: 0.9920, batch_loss: 0.0036, loss: 0.0264 ||:  35%|###5      | 438/1250 [02:02<04:39,  2.91it/s]
2022-03-22 12:45:37,011 - INFO - tqdm - f1: 0.9921, accuracy: 0.9921, batch_loss: 0.3704, loss: 0.0266 ||:  38%|###8      | 475/1250 [02:12<02:58,  4.35it/s]
2022-03-22 12:45:47,090 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0557, loss: 0.0258 ||:  41%|####1     | 513/1250 [02:22<03:24,  3.60it/s]
2022-03-22 12:45:57,166 - INFO - tqdm - f1: 0.9925, accuracy: 0.9925, batch_loss: 0.0004, loss: 0.0247 ||:  44%|####4     | 551/1250 [02:32<02:32,  4.59it/s]
2022-03-22 12:46:07,310 - INFO - tqdm - f1: 0.9921, accuracy: 0.9921, batch_loss: 0.0008, loss: 0.0258 ||:  47%|####7     | 589/1250 [02:42<03:03,  3.59it/s]
2022-03-22 12:46:17,325 - INFO - tqdm - f1: 0.9921, accuracy: 0.9921, batch_loss: 0.1297, loss: 0.0264 ||:  50%|####9     | 624/1250 [02:52<03:07,  3.34it/s]
2022-03-22 12:46:27,398 - INFO - tqdm - f1: 0.9920, accuracy: 0.9920, batch_loss: 0.0042, loss: 0.0267 ||:  53%|#####2    | 662/1250 [03:02<02:19,  4.23it/s]
2022-03-22 12:46:37,406 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0016, loss: 0.0259 ||:  56%|#####5    | 696/1250 [03:12<02:47,  3.32it/s]
2022-03-22 12:46:47,500 - INFO - tqdm - f1: 0.9919, accuracy: 0.9919, batch_loss: 0.0120, loss: 0.0263 ||:  59%|#####8    | 732/1250 [03:22<02:15,  3.84it/s]
2022-03-22 12:46:57,553 - INFO - tqdm - f1: 0.9921, accuracy: 0.9921, batch_loss: 0.0036, loss: 0.0255 ||:  62%|######1   | 769/1250 [03:32<02:08,  3.74it/s]
2022-03-22 12:47:07,752 - INFO - tqdm - f1: 0.9919, accuracy: 0.9919, batch_loss: 0.0004, loss: 0.0257 ||:  64%|######4   | 806/1250 [03:43<02:06,  3.52it/s]
2022-03-22 12:47:17,879 - INFO - tqdm - f1: 0.9918, accuracy: 0.9918, batch_loss: 0.2031, loss: 0.0260 ||:  67%|######7   | 842/1250 [03:53<01:55,  3.54it/s]
2022-03-22 12:47:28,057 - INFO - tqdm - f1: 0.9917, accuracy: 0.9917, batch_loss: 0.0035, loss: 0.0264 ||:  70%|#######   | 879/1250 [04:03<01:58,  3.14it/s]
2022-03-22 12:47:38,086 - INFO - tqdm - f1: 0.9918, accuracy: 0.9918, batch_loss: 0.0020, loss: 0.0265 ||:  74%|#######3  | 919/1250 [04:13<01:14,  4.42it/s]
2022-03-22 12:47:48,373 - INFO - tqdm - f1: 0.9919, accuracy: 0.9919, batch_loss: 0.0006, loss: 0.0262 ||:  76%|#######6  | 954/1250 [04:23<01:20,  3.66it/s]
2022-03-22 12:47:58,443 - INFO - tqdm - f1: 0.9920, accuracy: 0.9920, batch_loss: 0.0514, loss: 0.0260 ||:  79%|#######9  | 992/1250 [04:33<00:59,  4.30it/s]
2022-03-22 12:48:08,835 - INFO - tqdm - f1: 0.9919, accuracy: 0.9919, batch_loss: 0.0067, loss: 0.0268 ||:  82%|########2 | 1030/1250 [04:44<01:16,  2.89it/s]
2022-03-22 12:48:19,061 - INFO - tqdm - f1: 0.9915, accuracy: 0.9915, batch_loss: 0.0187, loss: 0.0282 ||:  85%|########5 | 1064/1250 [04:54<00:53,  3.49it/s]
2022-03-22 12:48:29,341 - INFO - tqdm - f1: 0.9915, accuracy: 0.9915, batch_loss: 0.0015, loss: 0.0281 ||:  88%|########8 | 1101/1250 [05:04<00:41,  3.60it/s]
2022-03-22 12:48:39,419 - INFO - tqdm - f1: 0.9915, accuracy: 0.9915, batch_loss: 0.0012, loss: 0.0281 ||:  91%|######### | 1135/1250 [05:14<00:30,  3.77it/s]
2022-03-22 12:48:49,740 - INFO - tqdm - f1: 0.9912, accuracy: 0.9912, batch_loss: 0.0106, loss: 0.0293 ||:  94%|#########3| 1173/1250 [05:25<00:20,  3.69it/s]
2022-03-22 12:48:59,904 - INFO - tqdm - f1: 0.9913, accuracy: 0.9913, batch_loss: 0.0092, loss: 0.0290 ||:  97%|#########6| 1211/1250 [05:35<00:08,  4.40it/s]
2022-03-22 12:49:09,349 - INFO - tqdm - f1: 0.9911, accuracy: 0.9911, batch_loss: 0.0012, loss: 0.0290 ||: 100%|#########9| 1244/1250 [05:44<00:01,  3.25it/s]
2022-03-22 12:49:09,658 - INFO - tqdm - f1: 0.9911, accuracy: 0.9911, batch_loss: 0.0036, loss: 0.0289 ||: 100%|#########9| 1245/1250 [05:44<00:01,  3.24it/s]
2022-03-22 12:49:10,044 - INFO - tqdm - f1: 0.9911, accuracy: 0.9911, batch_loss: 0.0013, loss: 0.0289 ||: 100%|#########9| 1246/1250 [05:45<00:01,  3.02it/s]
2022-03-22 12:49:10,254 - INFO - tqdm - f1: 0.9911, accuracy: 0.9911, batch_loss: 0.0011, loss: 0.0289 ||: 100%|#########9| 1247/1250 [05:45<00:00,  3.39it/s]
2022-03-22 12:49:10,490 - INFO - tqdm - f1: 0.9911, accuracy: 0.9911, batch_loss: 0.0010, loss: 0.0289 ||: 100%|#########9| 1248/1250 [05:45<00:00,  3.60it/s]
2022-03-22 12:49:10,868 - INFO - tqdm - f1: 0.9911, accuracy: 0.9911, batch_loss: 0.0015, loss: 0.0289 ||: 100%|#########9| 1249/1250 [05:46<00:00,  3.25it/s]
2022-03-22 12:49:11,269 - INFO - tqdm - f1: 0.9912, accuracy: 0.9911, batch_loss: 0.0011, loss: 0.0288 ||: 100%|##########| 1250/1250 [05:46<00:00,  2.98it/s]
2022-03-22 12:49:11,286 - INFO - tqdm - f1: 0.9912, accuracy: 0.9911, batch_loss: 0.0011, loss: 0.0288 ||: 100%|##########| 1250/1250 [05:46<00:00,  3.61it/s]
2022-03-22 12:49:11,306 - INFO - allennlp.training.trainer - Validating
2022-03-22 12:49:11,317 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-22 12:49:21,341 - INFO - tqdm - f1: 0.9446, accuracy: 0.9446, batch_loss: 0.4359, loss: 0.2497 ||:  25%|##5       | 79/313 [00:10<00:26,  8.78it/s]
2022-03-22 12:49:31,409 - INFO - tqdm - f1: 0.9419, accuracy: 0.9419, batch_loss: 0.0571, loss: 0.2729 ||:  52%|#####2    | 163/313 [00:20<00:15,  9.42it/s]
2022-03-22 12:49:41,425 - INFO - tqdm - f1: 0.9436, accuracy: 0.9436, batch_loss: 0.0217, loss: 0.2663 ||:  77%|#######7  | 242/313 [00:30<00:10,  6.80it/s]
2022-03-22 12:49:49,756 - INFO - tqdm - f1: 0.9426, accuracy: 0.9426, batch_loss: 0.4491, loss: 0.2670 ||: 100%|#########9| 312/313 [00:38<00:00,  7.61it/s]
2022-03-22 12:49:49,836 - INFO - tqdm - f1: 0.9428, accuracy: 0.9428, batch_loss: 0.0010, loss: 0.2661 ||: 100%|##########| 313/313 [00:38<00:00,  8.13it/s]
2022-03-22 12:49:49,850 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-22 12:49:49,850 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.991  |     0.943
2022-03-22 12:49:49,850 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.991  |     0.943
2022-03-22 12:49:49,850 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-22 12:49:49,850 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.029  |     0.266
2022-03-22 12:49:49,850 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  10049.004  |       N/A
2022-03-22 12:49:49,850 - INFO - allennlp.training.trainer - Epoch duration: 0:06:25.165280
2022-03-22 12:49:49,851 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:25:21
2022-03-22 12:49:49,851 - INFO - allennlp.training.trainer - Epoch 6/9
2022-03-22 12:49:49,851 - INFO - allennlp.training.trainer - Worker 0 memory usage: 9.8G
2022-03-22 12:49:49,851 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-22 12:49:49,852 - INFO - allennlp.training.trainer - Training
2022-03-22 12:49:49,860 - INFO - tqdm - 0%|          | 0/1250 [00:00<?, ?it/s]
2022-03-22 12:50:00,132 - INFO - tqdm - f1: 0.9982, accuracy: 0.9982, batch_loss: 0.0005, loss: 0.0033 ||:   3%|2         | 35/1250 [00:10<06:13,  3.26it/s]
2022-03-22 12:50:10,216 - INFO - tqdm - f1: 0.9974, accuracy: 0.9974, batch_loss: 0.0004, loss: 0.0081 ||:   6%|5         | 72/1250 [00:20<05:46,  3.40it/s]
2022-03-22 12:50:20,267 - INFO - tqdm - f1: 0.9971, accuracy: 0.9971, batch_loss: 0.0005, loss: 0.0088 ||:   9%|8         | 109/1250 [00:30<05:17,  3.59it/s]
2022-03-22 12:50:30,332 - INFO - tqdm - f1: 0.9936, accuracy: 0.9936, batch_loss: 0.0032, loss: 0.0262 ||:  13%|#3        | 165/1250 [00:40<02:14,  8.04it/s]
2022-03-22 12:50:40,585 - INFO - tqdm - f1: 0.9938, accuracy: 0.9938, batch_loss: 0.0016, loss: 0.0258 ||:  18%|#7        | 223/1250 [00:50<04:06,  4.16it/s]
2022-03-22 12:50:50,648 - INFO - tqdm - f1: 0.9930, accuracy: 0.9930, batch_loss: 0.0765, loss: 0.0285 ||:  21%|##1       | 268/1250 [01:00<04:13,  3.87it/s]
2022-03-22 12:51:00,902 - INFO - tqdm - f1: 0.9921, accuracy: 0.9921, batch_loss: 0.0024, loss: 0.0287 ||:  25%|##4       | 310/1250 [01:11<04:38,  3.38it/s]
2022-03-22 12:51:11,046 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0154, loss: 0.0282 ||:  28%|##8       | 354/1250 [01:21<03:35,  4.15it/s]
2022-03-22 12:51:21,117 - INFO - tqdm - f1: 0.9915, accuracy: 0.9915, batch_loss: 0.0027, loss: 0.0305 ||:  31%|###1      | 390/1250 [01:31<04:13,  3.39it/s]
2022-03-22 12:51:31,357 - INFO - tqdm - f1: 0.9917, accuracy: 0.9917, batch_loss: 0.0595, loss: 0.0310 ||:  34%|###4      | 428/1250 [01:41<03:51,  3.56it/s]
2022-03-22 12:51:41,469 - INFO - tqdm - f1: 0.9918, accuracy: 0.9918, batch_loss: 0.0021, loss: 0.0299 ||:  37%|###7      | 466/1250 [01:51<03:36,  3.62it/s]
2022-03-22 12:51:51,699 - INFO - tqdm - f1: 0.9919, accuracy: 0.9919, batch_loss: 0.0051, loss: 0.0297 ||:  40%|####      | 502/1250 [02:01<04:12,  2.96it/s]
2022-03-22 12:52:01,742 - INFO - tqdm - f1: 0.9920, accuracy: 0.9920, batch_loss: 0.0362, loss: 0.0288 ||:  43%|####2     | 536/1250 [02:11<03:47,  3.13it/s]
2022-03-22 12:52:11,758 - INFO - tqdm - f1: 0.9923, accuracy: 0.9923, batch_loss: 0.0009, loss: 0.0277 ||:  46%|####5     | 573/1250 [02:21<02:32,  4.44it/s]
2022-03-22 12:52:21,826 - INFO - tqdm - f1: 0.9920, accuracy: 0.9920, batch_loss: 0.0012, loss: 0.0282 ||:  49%|####8     | 611/1250 [02:31<02:51,  3.72it/s]
2022-03-22 12:52:32,079 - INFO - tqdm - f1: 0.9919, accuracy: 0.9919, batch_loss: 0.0021, loss: 0.0276 ||:  52%|#####2    | 651/1250 [02:42<02:51,  3.49it/s]
2022-03-22 12:52:42,275 - INFO - tqdm - f1: 0.9916, accuracy: 0.9916, batch_loss: 0.0521, loss: 0.0289 ||:  55%|#####4    | 686/1250 [02:52<03:12,  2.93it/s]
2022-03-22 12:52:52,282 - INFO - tqdm - f1: 0.9916, accuracy: 0.9916, batch_loss: 0.0029, loss: 0.0285 ||:  58%|#####7    | 722/1250 [03:02<02:41,  3.27it/s]
2022-03-22 12:53:02,366 - INFO - tqdm - f1: 0.9919, accuracy: 0.9919, batch_loss: 0.0007, loss: 0.0274 ||:  61%|######    | 759/1250 [03:12<02:04,  3.94it/s]
2022-03-22 12:53:12,528 - INFO - tqdm - f1: 0.9920, accuracy: 0.9920, batch_loss: 0.0014, loss: 0.0268 ||:  64%|######3   | 797/1250 [03:22<02:06,  3.58it/s]
2022-03-22 12:53:22,646 - INFO - tqdm - f1: 0.9920, accuracy: 0.9920, batch_loss: 0.0912, loss: 0.0265 ||:  67%|######6   | 835/1250 [03:32<02:01,  3.40it/s]
2022-03-22 12:53:32,985 - INFO - tqdm - f1: 0.9921, accuracy: 0.9921, batch_loss: 0.0010, loss: 0.0266 ||:  70%|######9   | 870/1250 [03:43<01:57,  3.22it/s]
2022-03-22 12:53:43,046 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0093, loss: 0.0265 ||:  73%|#######2  | 908/1250 [03:53<01:24,  4.05it/s]
2022-03-22 12:53:53,236 - INFO - tqdm - f1: 0.9924, accuracy: 0.9924, batch_loss: 0.0022, loss: 0.0259 ||:  76%|#######5  | 946/1250 [04:03<01:29,  3.39it/s]
2022-03-22 12:54:03,474 - INFO - tqdm - f1: 0.9923, accuracy: 0.9923, batch_loss: 0.0031, loss: 0.0266 ||:  79%|#######8  | 983/1250 [04:13<01:11,  3.74it/s]
2022-03-22 12:54:13,580 - INFO - tqdm - f1: 0.9924, accuracy: 0.9924, batch_loss: 0.0008, loss: 0.0262 ||:  81%|########1 | 1017/1250 [04:23<01:09,  3.38it/s]
2022-03-22 12:54:23,882 - INFO - tqdm - f1: 0.9925, accuracy: 0.9925, batch_loss: 0.0004, loss: 0.0257 ||:  84%|########4 | 1054/1250 [04:34<01:07,  2.90it/s]
2022-03-22 12:54:34,189 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0018, loss: 0.0263 ||:  87%|########7 | 1093/1250 [04:44<00:42,  3.69it/s]
2022-03-22 12:54:44,344 - INFO - tqdm - f1: 0.9923, accuracy: 0.9923, batch_loss: 0.0007, loss: 0.0259 ||:  90%|######### | 1128/1250 [04:54<00:29,  4.07it/s]
2022-03-22 12:54:54,485 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0024, loss: 0.0269 ||:  93%|#########3| 1163/1250 [05:04<00:27,  3.16it/s]
2022-03-22 12:55:04,790 - INFO - tqdm - f1: 0.9921, accuracy: 0.9921, batch_loss: 0.0020, loss: 0.0268 ||:  96%|#########6| 1202/1250 [05:14<00:15,  3.16it/s]
2022-03-22 12:55:15,075 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0114, loss: 0.0270 ||:  99%|#########9| 1240/1250 [05:25<00:02,  3.66it/s]
2022-03-22 12:55:16,155 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0026, loss: 0.0270 ||: 100%|#########9| 1244/1250 [05:26<00:01,  3.60it/s]
2022-03-22 12:55:16,413 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0031, loss: 0.0269 ||: 100%|#########9| 1245/1250 [05:26<00:01,  3.68it/s]
2022-03-22 12:55:16,858 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0033, loss: 0.0269 ||: 100%|#########9| 1246/1250 [05:26<00:01,  3.09it/s]
2022-03-22 12:55:17,038 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0012, loss: 0.0269 ||: 100%|#########9| 1247/1250 [05:27<00:00,  3.56it/s]
2022-03-22 12:55:17,293 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0039, loss: 0.0269 ||: 100%|#########9| 1248/1250 [05:27<00:00,  3.66it/s]
2022-03-22 12:55:17,509 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0022, loss: 0.0269 ||: 100%|#########9| 1249/1250 [05:27<00:00,  3.91it/s]
2022-03-22 12:55:17,728 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0027, loss: 0.0268 ||: 100%|##########| 1250/1250 [05:27<00:00,  4.09it/s]
2022-03-22 12:55:17,738 - INFO - tqdm - f1: 0.9922, accuracy: 0.9922, batch_loss: 0.0027, loss: 0.0268 ||: 100%|##########| 1250/1250 [05:27<00:00,  3.81it/s]
2022-03-22 12:55:17,773 - INFO - allennlp.training.trainer - Validating
2022-03-22 12:55:17,774 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-22 12:55:28,012 - INFO - tqdm - f1: 0.9345, accuracy: 0.9346, batch_loss: 0.2608, loss: 0.2884 ||:  27%|##7       | 85/313 [00:10<00:28,  8.04it/s]
2022-03-22 12:55:38,051 - INFO - tqdm - f1: 0.9415, accuracy: 0.9415, batch_loss: 0.0275, loss: 0.2714 ||:  53%|#####3    | 166/313 [00:20<00:18,  7.90it/s]
2022-03-22 12:55:48,210 - INFO - tqdm - f1: 0.9412, accuracy: 0.9412, batch_loss: 0.0018, loss: 0.2626 ||:  78%|#######8  | 245/313 [00:30<00:08,  7.85it/s]
2022-03-22 12:55:56,337 - INFO - tqdm - f1: 0.9434, accuracy: 0.9434, batch_loss: 0.0013, loss: 0.2550 ||: 100%|#########9| 312/313 [00:38<00:00,  9.12it/s]
2022-03-22 12:55:56,482 - INFO - tqdm - f1: 0.9432, accuracy: 0.9432, batch_loss: 0.5368, loss: 0.2559 ||: 100%|##########| 313/313 [00:38<00:00,  8.43it/s]
2022-03-22 12:55:56,489 - INFO - tqdm - f1: 0.9432, accuracy: 0.9432, batch_loss: 0.5368, loss: 0.2559 ||: 100%|##########| 313/313 [00:38<00:00,  8.08it/s]
2022-03-22 12:55:56,534 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-03-22 12:55:56,534 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.992  |     0.943
2022-03-22 12:55:56,535 - INFO - allennlp.training.callbacks.console_logger - f1                 |     0.992  |     0.943
2022-03-22 12:55:56,535 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |     0.000  |       N/A
2022-03-22 12:55:56,535 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.027  |     0.256
2022-03-22 12:55:56,535 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  10049.004  |       N/A
2022-03-22 12:55:56,535 - INFO - allennlp.training.trainer - Epoch duration: 0:06:06.684064
2022-03-22 12:55:56,535 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:18:54
2022-03-22 12:55:56,535 - INFO - allennlp.training.trainer - Epoch 7/9
2022-03-22 12:55:56,535 - INFO - allennlp.training.trainer - Worker 0 memory usage: 9.8G
2022-03-22 12:55:56,535 - INFO - allennlp.training.trainer - GPU 0 memory usage: 0B
2022-03-22 12:55:56,536 - INFO - allennlp.training.trainer - Training
2022-03-22 12:55:56,536 - INFO - tqdm - 0%|          | 0/1250 [00:00<?, ?it/s]
2022-03-22 12:56:06,717 - INFO - tqdm - f1: 0.9882, accuracy: 0.9882, batch_loss: 0.0010, loss: 0.0342 ||:   3%|2         | 37/1250 [00:10<05:01,  4.02it/s]
2022-03-22 12:56:16,898 - INFO - tqdm - f1: 0.9914, accuracy: 0.9914, batch_loss: 0.0100, loss: 0.0271 ||:   6%|5         | 73/1250 [00:20<05:36,  3.50it/s]
2022-03-22 12:56:27,186 - INFO - tqdm - f1: 0.9928, accuracy: 0.9928, batch_loss: 0.0007, loss: 0.0249 ||:   9%|9         | 113/1250 [00:30<04:51,  3.90it/s]
2022-03-22 12:56:37,430 - INFO - tqdm - f1: 0.9942, accuracy: 0.9942, batch_loss: 0.0009, loss: 0.0209 ||:  12%|#2        | 151/1250 [00:40<06:10,  2.97it/s]
2022-03-22 12:56:47,621 - INFO - tqdm - f1: 0.9947, accuracy: 0.9947, batch_loss: 0.3474, loss: 0.0215 ||:  15%|#5        | 188/1250 [00:51<06:18,  2.80it/s]
2022-03-22 12:56:57,750 - INFO - tqdm - f1: 0.9936, accuracy: 0.9936, batch_loss: 0.0074, loss: 0.0240 ||:  18%|#8        | 225/1250 [01:01<04:40,  3.65it/s]
2022-03-22 12:57:08,008 - INFO - tqdm - f1: 0.9938, accuracy: 0.9938, batch_loss: 0.0019, loss: 0.0246 ||:  21%|##1       | 263/1250 [01:11<03:50,  4.28it/s]
2022-03-22 12:57:18,298 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0032, loss: 0.0248 ||:  24%|##4       | 300/1250 [01:21<04:45,  3.33it/s]
2022-03-22 12:57:28,309 - INFO - tqdm - f1: 0.9937, accuracy: 0.9937, batch_loss: 0.0013, loss: 0.0246 ||:  27%|##6       | 336/1250 [01:31<05:27,  2.79it/s]
2022-03-22 12:57:38,453 - INFO - tqdm - f1: 0.9941, accuracy: 0.9941, batch_loss: 0.0014, loss: 0.0228 ||:  30%|##9       | 371/1250 [01:41<03:38,  4.01it/s]
2022-03-22 12:57:48,521 - INFO - tqdm - f1: 0.9940, accuracy: 0.9940, batch_loss: 0.0004, loss: 0.0234 ||:  33%|###2      | 408/1250 [01:51<03:25,  4.10it/s]
2022-03-22 12:57:58,851 - INFO - tqdm - f1: 0.9938, accuracy: 0.9938, batch_loss: 0.0009, loss: 0.0232 ||:  36%|###5      | 446/1250 [02:02<03:55,  3.42it/s]
2022-03-22 12:58:08,896 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0013, loss: 0.0237 ||:  39%|###8      | 482/1250 [02:12<03:36,  3.54it/s]
2022-03-22 12:58:19,010 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0039, loss: 0.0245 ||:  41%|####1     | 517/1250 [02:22<03:26,  3.54it/s]
2022-03-22 12:58:29,155 - INFO - tqdm - f1: 0.9933, accuracy: 0.9933, batch_loss: 0.0017, loss: 0.0252 ||:  44%|####4     | 551/1250 [02:32<02:59,  3.90it/s]
2022-03-22 12:58:39,245 - INFO - tqdm - f1: 0.9934, accuracy: 0.9934, batch_loss: 0.0018, loss: 0.0248 ||:  47%|####7     | 588/1250 [02:42<02:43,  4.05it/s]
2022-03-22 12:58:49,302 - INFO - tqdm - f1: 0.9934, accuracy: 0.9934, batch_loss: 0.0026, loss: 0.0247 ||:  50%|####9     | 622/1250 [02:52<03:05,  3.38it/s]
2022-03-22 12:58:59,341 - INFO - tqdm - f1: 0.9936, accuracy: 0.9936, batch_loss: 0.0008, loss: 0.0239 ||:  53%|#####2    | 658/1250 [03:02<02:21,  4.19it/s]
2022-03-22 12:59:09,414 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0500, loss: 0.0238 ||:  56%|#####5    | 695/1250 [03:12<02:07,  4.34it/s]
2022-03-22 12:59:19,517 - INFO - tqdm - f1: 0.9933, accuracy: 0.9933, batch_loss: 0.0087, loss: 0.0242 ||:  58%|#####8    | 730/1250 [03:22<02:04,  4.17it/s]
2022-03-22 12:59:29,710 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.1885, loss: 0.0240 ||:  61%|######1   | 766/1250 [03:33<02:18,  3.50it/s]
2022-03-22 12:59:39,965 - INFO - tqdm - f1: 0.9936, accuracy: 0.9936, batch_loss: 0.0198, loss: 0.0239 ||:  64%|######4   | 802/1250 [03:43<02:07,  3.52it/s]
2022-03-22 12:59:50,023 - INFO - tqdm - f1: 0.9934, accuracy: 0.9934, batch_loss: 0.0087, loss: 0.0247 ||:  67%|######6   | 833/1250 [03:53<01:59,  3.49it/s]
2022-03-22 13:00:00,354 - INFO - tqdm - f1: 0.9933, accuracy: 0.9933, batch_loss: 0.0792, loss: 0.0245 ||:  70%|######9   | 873/1250 [04:03<01:48,  3.48it/s]
2022-03-22 13:00:10,503 - INFO - tqdm - f1: 0.9934, accuracy: 0.9934, batch_loss: 0.0056, loss: 0.0245 ||:  73%|#######2  | 910/1250 [04:13<01:42,  3.31it/s]
2022-03-22 13:00:20,656 - INFO - tqdm - f1: 0.9933, accuracy: 0.9933, batch_loss: 0.0114, loss: 0.0248 ||:  76%|#######5  | 945/1250 [04:24<01:24,  3.59it/s]
2022-03-22 13:00:30,713 - INFO - tqdm - f1: 0.9934, accuracy: 0.9934, batch_loss: 0.0011, loss: 0.0247 ||:  79%|#######8  | 982/1250 [04:34<01:21,  3.29it/s]
2022-03-22 13:00:40,951 - INFO - tqdm - f1: 0.9934, accuracy: 0.9934, batch_loss: 0.0015, loss: 0.0243 ||:  82%|########1 | 1019/1250 [04:44<01:08,  3.37it/s]
2022-03-22 13:00:51,252 - INFO - tqdm - f1: 0.9934, accuracy: 0.9934, batch_loss: 0.0016, loss: 0.0245 ||:  84%|########4 | 1056/1250 [04:54<00:54,  3.57it/s]
2022-03-22 13:01:01,537 - INFO - tqdm - f1: 0.9934, accuracy: 0.9934, batch_loss: 0.3729, loss: 0.0250 ||:  88%|########7 | 1095/1250 [05:05<00:48,  3.17it/s]
2022-03-22 13:01:11,586 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0028, loss: 0.0247 ||:  90%|######### | 1131/1250 [05:15<00:38,  3.12it/s]
2022-03-22 13:01:21,710 - INFO - tqdm - f1: 0.9936, accuracy: 0.9936, batch_loss: 0.0038, loss: 0.0244 ||:  94%|#########3| 1169/1250 [05:25<00:20,  3.89it/s]
2022-03-22 13:01:31,805 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0018, loss: 0.0242 ||:  96%|#########6| 1204/1250 [05:35<00:14,  3.13it/s]
2022-03-22 13:01:42,037 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0018, loss: 0.0242 ||:  99%|#########9| 1241/1250 [05:45<00:02,  3.90it/s]
2022-03-22 13:01:42,842 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0177, loss: 0.0242 ||: 100%|#########9| 1244/1250 [05:46<00:01,  3.64it/s]
2022-03-22 13:01:43,043 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0059, loss: 0.0242 ||: 100%|#########9| 1245/1250 [05:46<00:01,  3.96it/s]
2022-03-22 13:01:43,481 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0009, loss: 0.0242 ||: 100%|#########9| 1246/1250 [05:46<00:01,  3.24it/s]
2022-03-22 13:01:43,647 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0060, loss: 0.0242 ||: 100%|#########9| 1247/1250 [05:47<00:00,  3.77it/s]
2022-03-22 13:01:43,927 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0007, loss: 0.0242 ||: 100%|#########9| 1248/1250 [05:47<00:00,  3.70it/s]
2022-03-22 13:01:44,319 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0167, loss: 0.0242 ||: 100%|#########9| 1249/1250 [05:47<00:00,  3.26it/s]
2022-03-22 13:01:44,524 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0012, loss: 0.0241 ||: 100%|##########| 1250/1250 [05:47<00:00,  3.62it/s]
2022-03-22 13:01:44,536 - INFO - tqdm - f1: 0.9935, accuracy: 0.9935, batch_loss: 0.0012, loss: 0.0241 ||: 100%|##########| 1250/1250 [05:47<00:00,  3.59it/s]
2022-03-22 13:01:44,595 - INFO - allennlp.training.trainer - Validating
2022-03-22 13:01:44,620 - INFO - tqdm - 0%|          | 0/313 [00:00<?, ?it/s]
2022-03-22 13:01:54,751 - INFO - tqdm - f1: 0.9477, accuracy: 0.9477, batch_loss: 0.0216, loss: 0.2833 ||:  27%|##6       | 83/313 [00:10<00:29,  7.78it/s]
2022-03-22 13:02:04,818 - INFO - tqdm - f1: 0.9480, accuracy: 0.9480, batch_loss: 0.0006, loss: 0.2576 ||:  52%|#####2    | 164/313 [00:20<00:20,  7.28it/s]
2022-03-22 13:02:14,867 - INFO - tqdm - f1: 0.9454, accuracy: 0.9455, batch_loss: 0.7821, loss: 0.2594 ||:  79%|#######9  | 248/313 [00:30<00:06,  9.53it/s]
2022-03-22 13:02:23,080 - INFO - tqdm - f1: 0.9470, accuracy: 0.9470, batch_loss: 0.0033, loss: 0.2497 ||: 100%|##########| 313/313 [00:38<00:00,  8.08it/s]
2022-03-22 13:02:23,085 - INFO - tqdm - f1: 0.9470, accuracy: 0.9470, batch_loss: 0.0033, loss: 0.2497 ||: 100%|##########| 313/313 [00:38<00:00,  8.14it/s]
2022-03-22 13:02:23,127 - INFO - allennlp.training.trainer - Ran out of patience.  Stopping training.
2022-03-22 13:02:23,132 - INFO - dont_stop_pretraining.training.roberta_checkpointer - loading best weights
2022-03-22 13:02:23,768 - INFO - allennlp.commands.train - The model will be evaluated using the best epoch weights.
2022-03-22 13:02:23,781 - INFO - allennlp.training.util - Iterating over dataset
2022-03-22 13:02:23,792 - INFO - tqdm - 0it [00:00, ?it/s]
2022-03-22 13:02:23,849 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-22 13:02:23,856 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-22 13:02:33,989 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.25 ||: : 83it [00:10,  9.71it/s]
2022-03-22 13:02:44,098 - INFO - tqdm - f1: 0.94, accuracy: 0.94, loss: 0.25 ||: : 164it [00:20,  7.55it/s]
2022-03-22 13:02:54,187 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.24 ||: : 250it [00:30,  7.27it/s]
2022-03-22 13:03:04,297 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.23 ||: : 337it [00:40,  9.04it/s]
2022-03-22 13:03:14,327 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.24 ||: : 423it [00:50,  9.48it/s]
2022-03-22 13:03:24,340 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.24 ||: : 504it [01:00,  8.42it/s]
2022-03-22 13:03:34,358 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.24 ||: : 586it [01:10,  8.70it/s]
2022-03-22 13:03:44,425 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.24 ||: : 668it [01:20,  6.89it/s]
2022-03-22 13:03:54,439 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.24 ||: : 749it [01:30,  9.20it/s]
2022-03-22 13:04:04,471 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.24 ||: : 834it [01:40,  7.88it/s]
2022-03-22 13:04:14,602 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.23 ||: : 916it [01:50,  6.88it/s]
2022-03-22 13:04:24,679 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.23 ||: : 997it [02:00,  7.30it/s]
2022-03-22 13:04:34,809 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.23 ||: : 1078it [02:11,  8.47it/s]
2022-03-22 13:04:44,892 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.24 ||: : 1160it [02:21,  8.36it/s]
2022-03-22 13:04:55,026 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.24 ||: : 1243it [02:31,  7.04it/s]
2022-03-22 13:05:05,147 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.24 ||: : 1325it [02:41,  9.17it/s]
2022-03-22 13:05:15,210 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.24 ||: : 1408it [02:51,  7.32it/s]
2022-03-22 13:05:25,302 - INFO - tqdm - f1: 0.95, accuracy: 0.95, loss: 0.24 ||: : 1487it [03:01,  7.36it/s]
2022-03-22 13:05:34,914 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 4,
  "peak_worker_0_memory_MB": 10049.00390625,
  "peak_gpu_0_memory_MB": 0,
  "training_duration": "0:44:08.278371",
  "training_start_epoch": 0,
  "training_epochs": 6,
  "epoch": 6,
  "training_f1": 0.9922499656677246,
  "training_accuracy": 0.99225,
  "training_loss": 0.02684432153145317,
  "training_worker_0_memory_MB": 10049.00390625,
  "training_gpu_0_memory_MB": 0.0,
  "validation_f1": 0.9431996643543243,
  "validation_accuracy": 0.9432,
  "validation_loss": 0.25586377342842287,
  "best_validation_f1": 0.9481804966926575,
  "best_validation_accuracy": 0.9482,
  "best_validation_loss": 0.26036561747991327,
  "test_f1": 0.946905642747879,
  "test_accuracy": 0.94692,
  "test_loss": 0.23696454666296177
}
2022-03-22 13:05:35,973 - INFO - allennlp.models.archival - archiving weights and vocabulary to model_logs/imdb_base_hyper_small_seed_47/model.tar.gz
